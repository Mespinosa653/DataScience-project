{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Predicting the number of hours worked: a neural network vs a bayesian approach\"\n",
    "COLLABORATORS = \"Matteo and Manel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Datascience-for-Economics-Final-Assignment\" data-toc-modified-id=\"Datascience-for-Economics-Final-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Datascience for Economics Final Assignment</a></span></li><li><span><a href=\"#Research-question-(0.5-points)\" data-toc-modified-id=\"Research-question-(0.5-points)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Research question (0.5 points)</a></span></li><li><span><a href=\"#Motivation-(0.5-points)\" data-toc-modified-id=\"Motivation-(0.5-points)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Motivation (0.5 points)</a></span></li><li><span><a href=\"#Method-and-data-(1.0-point)\" data-toc-modified-id=\"Method-and-data-(1.0-point)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Method and data (1.0 point)</a></span></li><li><span><a href=\"#Preview-of-the-answers-(0.5-points)\" data-toc-modified-id=\"Preview-of-the-answers-(0.5-points)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preview of the answers (0.5 points)</a></span></li><li><span><a href=\"#Main-assumptions-(0.5-points)\" data-toc-modified-id=\"Main-assumptions-(0.5-points)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Main assumptions (0.5 points)</a></span></li><li><span><a href=\"#Python/R-code-(6-points)\" data-toc-modified-id=\"Python/R-code-(6-points)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Python/R code (6 points)</a></span></li><li><span><a href=\"#Robustness-analysis-(0.5-points)\" data-toc-modified-id=\"Robustness-analysis-(0.5-points)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Robustness analysis (0.5 points)</a></span></li><li><span><a href=\"#Discussion-and-conclusion-(0.5-points)\" data-toc-modified-id=\"Discussion-and-conclusion-(0.5-points)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Discussion and conclusion (0.5 points)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535c21960d4663d5edac398cb445d087",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascience for Economics Final Assignment\n",
    "\n",
    "\n",
    "\n",
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Matteo|12345|u6786|\n",
    "|Manuel Espinosa|2067814|u112479|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question (0.5 points)\n",
    "\n",
    "Formulate the research question: what question do you want to answer in this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a9d08ea38644e3ac0b85955731a7e3",
     "grade": true,
     "grade_id": "cell-44d71caea99ce92b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "How does personal characteristics help predicting the level of hours worked per week in the UK. We will predict if active people work or not depending on the personal characeristics and also the number of hours that they do. We only need data from people over 65 and that is active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation (0.5 points)\n",
    "\n",
    "Motivate why this question is interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74e1c61d4c263e5dcedc9c4f7565432",
     "grade": true,
     "grade_id": "cell-a00025e68181b6f6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Find a reason of why predicting the hours worked per week may be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and data (1.0 point)\n",
    "\n",
    "Which data do you have to analyze the question and what methods are you using to answer the question?\n",
    "\n",
    "* explain where your data come from (give a link or the code to download the data if you use an API like [wbdata](https://wbdata.readthedocs.io/en/stable/));\n",
    "* explain the data cleaning and data normalization steps that you use;\n",
    "* make sure that the data is in your github repository as well (or explain why this is not possible) so that we can replicate your analysis.\n",
    "* Motivate why you have decided to use these methods (e.g., neural network) over other methods (e.g., Lasso) in this specific context of your study.\n",
    "* How is the data representative to the population of interest?\n",
    "\n",
    "The data come from the English Office for National Statistics, here's the link to the website: https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/datasets/2011censusteachingfile This is a sample of 2011 providing microdata with 569741 observations. People analysed in this sample are classified under 16 variables:the region, the residence type, family composition, population base, sex, age, marital status, student, country of birth, health, ethnic group, religion, economic activity, occupation, industry, hours worked per week, and approximated social grade. For a more in depth explanation of each varaibles and the different values it can take you can acces this [file](Microdatateachingvariablels.pdf). PUT THE CORRESPONDENT LINK and attach file in repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b91e9ed0432af5f7bd6e33a6a51515",
     "grade": true,
     "grade_id": "cell-a2a13b6c938ec4c8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers (0.5 points)\n",
    "\n",
    "Summarize the results that you find and the answer to your research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a675684538158f19ff2a70fa98e7fff2",
     "grade": true,
     "grade_id": "cell-c2854f0b8b034fae",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions (0.5 points)\n",
    "\n",
    "What are the main assumptions that you need to answer the question with your data? For example, assumptions about missing data, random sampling, stability of the data generating process. If the aim is causal inference, then also make clear your assumptions about the random assignment of treatment (e.g. plot a DAG to illustrate your view on the causal relations between the variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43d4f76461858e4d6140ccbf5347ab6",
     "grade": true,
     "grade_id": "cell-03cb6a4826b945c9",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/R code (6 points)\n",
    "\n",
    "Give the python/R code in code cells and use markdown cells to explain why you code things in this way and what the outcomes are of the code cells.\n",
    "\n",
    "Create as many python/R and markdown cells as you need to explain things well.\n",
    "\n",
    "If you program mainly in R, you can also use R-studio if you prefer. On jupyterlab, you can switch to an R kernel and combine python and R code in one notebook.\n",
    "\n",
    "## Package importation\n",
    "\n",
    "First of all we need to import the libraries which functions we are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0736d3fd2fd323ad705ff2d7f5f0b8",
     "grade": true,
     "grade_id": "cell-c8076f3091bfd885",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#we import the numpy library  for many uses such as generate new data or manipulate arrays\n",
    "import numpy as np\n",
    "#the pandas library to manipulate and transfrom teh data\n",
    "import pandas as pd\n",
    "#the seaborn library which is based on matplotlib and makes the graphs more attractive\n",
    "import seaborn as sn\n",
    "#scipy provides more scientific fucntions \n",
    "import scipy as sc\n",
    "\n",
    "#import pymc3 as pm\n",
    "#matplotlib and concretely pyplot will be useful to plot the diferent graphs that we need to sow our results\n",
    "import matplotlib.pyplot as plt\n",
    "#is a library designed for machine learning and it contains keras which we will use for the neural network\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import models\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from tensorflow.keras import datasets, layers, models\n",
    "#from tensorflow import keras\n",
    "#import arviz as az\n",
    "\n",
    "#we will use it to supress warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "#we import scikit learn to acces different statistical tools and function such as method to split the data in training and testing set, \n",
    "#to standarize the data if needed and to compute the accuracy between predicted and true values\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#We finally import the ipywidgets package to make our functions interactive and display them\n",
    "from ipywidgets import interact, interactive, fixed, Layout\n",
    "import ipywidgets as widgets \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "We will need to give a look at the data and see how to transform the data ini a way that we can manipulate it given that all of the variables are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation\n",
    "\n",
    "We first import the data from a csv that you will find in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569736</th>\n",
       "      <td>7946020</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569737</th>\n",
       "      <td>7944310</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569738</th>\n",
       "      <td>7945374</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569739</th>\n",
       "      <td>7944768</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569740</th>\n",
       "      <td>7944959</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569741 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Person ID     Region Residence Type Family Composition Population Base  \\\n",
       "0        7394816  E12000001              H                  2               1   \n",
       "1        7394745  E12000001              H                  5               1   \n",
       "2        7395066  E12000001              H                  3               1   \n",
       "3        7395329  E12000001              H                  3               1   \n",
       "4        7394712  E12000001              H                  3               1   \n",
       "...          ...        ...            ...                ...             ...   \n",
       "569736   7946020  W92000004              H                  1               1   \n",
       "569737   7944310  W92000004              H                  3               1   \n",
       "569738   7945374  W92000004              H                  3               1   \n",
       "569739   7944768  W92000004              H                  1               1   \n",
       "569740   7944959  W92000004              H                  2               1   \n",
       "\n",
       "       Sex Age Marital Status Student Country of Birth Health Ethnic Group  \\\n",
       "0        2   6              2       2                1      2            1   \n",
       "1        1   4              1       2                1      1            1   \n",
       "2        2   4              1       2                1      1            1   \n",
       "3        2   2              1       2                1      2            1   \n",
       "4        1   5              4       2                1      1            1   \n",
       "...     ..  ..            ...     ...              ...    ...          ...   \n",
       "569736   1   5              1       2                1      4            1   \n",
       "569737   1   3              1       2                1      2            1   \n",
       "569738   1   1              1       1                1      1            1   \n",
       "569739   2   8              5       2                1      3            1   \n",
       "569740   2   2              2       2                1      2            1   \n",
       "\n",
       "       Religion Economic Activity Occupation Industry Hours worked per week  \\\n",
       "0             2                 5          8        2                    -9   \n",
       "1             2                 1          8        6                     4   \n",
       "2             1                 1          6       11                     3   \n",
       "3             2                 1          7        7                     3   \n",
       "4             2                 1          1        4                     3   \n",
       "...         ...               ...        ...      ...                   ...   \n",
       "569736        9                 1          8        8                     3   \n",
       "569737        1                 1          7        4                     3   \n",
       "569738        2                -9         -9       -9                    -9   \n",
       "569739        9                 5          9        2                    -9   \n",
       "569740        1                 1          7        4                     1   \n",
       "\n",
       "       Approximated Social Grade  \n",
       "0                              4  \n",
       "1                              3  \n",
       "2                              4  \n",
       "3                              2  \n",
       "4                              2  \n",
       "...                          ...  \n",
       "569736                         3  \n",
       "569737                         4  \n",
       "569738                        -9  \n",
       "569739                         4  \n",
       "569740                         4  \n",
       "\n",
       "[569741 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data importation\n",
    "#set the file directory path\n",
    "file=('2011teachingfile.csv')\n",
    "#use the pandas methos read_csv to create a dataframe from the csv\n",
    "df= pd.read_csv(file, header=None)\n",
    "#we set the colun names\n",
    "df.columns = df.iloc[1,:].values\n",
    "#we drop the first two raws of the file as there is no interesting information in these.\n",
    "df.drop([0,1], axis=0, inplace=True)\n",
    "#after manipulating the dataframe we reset the index of it\n",
    "df_small=df.reset_index(drop=True)\n",
    "df_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "\n",
    "Given that all of the variables are integer encoded except from region and Residence type, we have prefered to change these two to integer encoded as well. Moreover, all the columns where of the type object (Strings) and we prefered them to be integers so we also changed the type of varaibles of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    object\n",
       "Region                        int64\n",
       "Residence Type                int64\n",
       "Family Composition           object\n",
       "Population Base              object\n",
       "Sex                          object\n",
       "Age                          object\n",
       "Marital Status               object\n",
       "Student                      object\n",
       "Country of Birth             object\n",
       "Health                       object\n",
       "Ethnic Group                 object\n",
       "Religion                     object\n",
       "Economic Activity            object\n",
       "Occupation                   object\n",
       "Industry                     object\n",
       "Hours worked per week        object\n",
       "Approximated Social Grade    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Region and Residence Type to integers:\n",
    "dictionary_region={\"E12000001\":1,\"E12000002\":2,\"E12000003\":3, \"E12000004\":4,\"E12000005\":5,\"E12000006\":6,\"E12000007\":7,\"E12000008\":8,\"E12000009\":9,\"W92000004\":10}\n",
    "dictionary_residence_type={\"C\":1,\"H\":2}\n",
    "#we use the two previous dictionaries created to replace the keys for the values in each column\n",
    "df_small=df_small.replace({\"Region\": dictionary_region,\"Residence Type\":dictionary_residence_type})\n",
    "\n",
    "#We show the types of data of the columns which is mostly objects\n",
    "df_small.dtypes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    int32\n",
       "Region                       int32\n",
       "Residence Type               int32\n",
       "Family Composition           int32\n",
       "Population Base              int32\n",
       "Sex                          int32\n",
       "Age                          int32\n",
       "Marital Status               int32\n",
       "Student                      int32\n",
       "Country of Birth             int32\n",
       "Health                       int32\n",
       "Ethnic Group                 int32\n",
       "Religion                     int32\n",
       "Economic Activity            int32\n",
       "Occupation                   int32\n",
       "Industry                     int32\n",
       "Hours worked per week        int32\n",
       "Approximated Social Grade    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create a list with the column names\n",
    "column_names=list(df_small.columns)\n",
    "#create a dictionary\n",
    "dictionary = dict()\n",
    "#this for will set the column naes as the key of the dictionary and the int class as the value\n",
    "for item in column_names:\n",
    "    key = item\n",
    "    value=int\n",
    "    \n",
    "    dictionary[key] = value\n",
    "#we change the data type of each column using the dictionary created\n",
    "df_Small = df_small.astype(dictionary)\n",
    "\n",
    "#show the new data types for the columns\n",
    "df_Small.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clenaing\n",
    "\n",
    "Given the escope of our project we will not be needing data for people under 16 neither students living away during term-time. Therefore, we delete the rows corresponding tp these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457118</th>\n",
       "      <td>7944868</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457119</th>\n",
       "      <td>7946020</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457120</th>\n",
       "      <td>7944310</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457121</th>\n",
       "      <td>7944768</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457122</th>\n",
       "      <td>7944959</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457123 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Person ID  Region  Residence Type  Family Composition  \\\n",
       "0         7394816       1               2                   2   \n",
       "1         7394745       1               2                   5   \n",
       "2         7395066       1               2                   3   \n",
       "3         7395329       1               2                   3   \n",
       "4         7394712       1               2                   3   \n",
       "...           ...     ...             ...                 ...   \n",
       "457118    7944868      10               2                   2   \n",
       "457119    7946020      10               2                   1   \n",
       "457120    7944310      10               2                   3   \n",
       "457121    7944768      10               2                   1   \n",
       "457122    7944959      10               2                   2   \n",
       "\n",
       "        Population Base  Sex  Age  Marital Status  Student  Country of Birth  \\\n",
       "0                     1    2    6               2        2                 1   \n",
       "1                     1    1    4               1        2                 1   \n",
       "2                     1    2    4               1        2                 1   \n",
       "3                     1    2    2               1        2                 1   \n",
       "4                     1    1    5               4        2                 1   \n",
       "...                 ...  ...  ...             ...      ...               ...   \n",
       "457118                1    1    2               1        2                 1   \n",
       "457119                1    1    5               1        2                 1   \n",
       "457120                1    1    3               1        2                 1   \n",
       "457121                1    2    8               5        2                 1   \n",
       "457122                1    2    2               2        2                 1   \n",
       "\n",
       "        Health  Ethnic Group  Religion  Economic Activity  Occupation  \\\n",
       "0            2             1         2                  5           8   \n",
       "1            1             1         2                  1           8   \n",
       "2            1             1         1                  1           6   \n",
       "3            2             1         2                  1           7   \n",
       "4            1             1         2                  1           1   \n",
       "...        ...           ...       ...                ...         ...   \n",
       "457118       1             1         2                  1           4   \n",
       "457119       4             1         9                  1           8   \n",
       "457120       2             1         1                  1           7   \n",
       "457121       3             1         9                  5           9   \n",
       "457122       2             1         1                  1           7   \n",
       "\n",
       "        Industry  Hours worked per week  Approximated Social Grade  \n",
       "0              2                      0                          4  \n",
       "1              6                      4                          3  \n",
       "2             11                      3                          4  \n",
       "3              7                      3                          2  \n",
       "4              4                      3                          2  \n",
       "...          ...                    ...                        ...  \n",
       "457118         9                      3                          2  \n",
       "457119         8                      3                          3  \n",
       "457120         4                      3                          4  \n",
       "457121         2                      0                          4  \n",
       "457122         4                      1                          4  \n",
       "\n",
       "[457123 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We replace the -9 values that refer to no code required to the 0 as it will make it easier to plot the graphs\n",
    "df_Small=df_Small.replace(-9,0)\n",
    "#deelte the rows in which age is under 16, which corresponds to value 1\n",
    "df_Small = df_Small[~(df_Small[\"Age\"] == 1)]\n",
    "#delete the rows in wich population base is students living away during term-time, which corresponds to value 2\n",
    "df_Small = df_Small[~(df_Small[\"Population Base\"] == 2)]\n",
    "\n",
    "\n",
    "#we define the list variables with all the column names of the dataframe\n",
    "variables=column_names[1:]\n",
    "#we reset the index of the dataframe after all these changes\n",
    "df_Small.reset_index(drop=True,inplace=True)\n",
    "df_Small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels\n",
    "\n",
    "Given that the data is integer encoded we will need dictionaries that associate the label to the value to set the labels in the plots or other implmentations in the different functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "#Dictionary for every variable label and its corresponding value\n",
    "dict_family_composition={'No code required':0,'Not in a family':1, 'Married':2, 'Cohabiting couple':3, 'Lone parent family (male)':4, 'Lone parent family (female)':5, 'Others':6}\n",
    "dict_residence_type={'Resident in a communal establishment':1,'Not resident in a communal establishment':2}\n",
    "dict_population_base={'Usual resident':1, 'Short-term resident':3}\n",
    "dict_sex={'Male':1, 'Female':2}\n",
    "dict_age={'16 to 24':2, '25 to 34':3, '35 to 44':4, '45 to 54':5, '55 to 64':6, '65 to 74':7, '75 and over':8}\n",
    "dict_marital_status={'Single':1, 'Married':2, 'Separated':3, 'Divorced':4, 'Widowed':5}\n",
    "dict_student={'Yes':1, 'No':2}\n",
    "dict_country_of_birth={'UK':1, 'Non UK':2}\n",
    "dict_health={'Very good health':1, 'Good health':2, 'Fair health':3, 'Bad health':4, 'Very bad health':5}\n",
    "dict_ethnic_group={'White':1, 'Mixed':2, 'Asian and Asian British':3, 'Black or Black British':4, 'Chinese or Other ethnic group':5}\n",
    "dict_religion={'No religion':1, 'Christian':2, 'Buddhist':3, 'Hindu':4, 'Jewish':5, 'Muslim':6, 'Sikh':7, 'Other religion':8, 'Not stated':9}\n",
    "dict_economic_activity={'Employee':1, 'Self-employed':2, 'Unemployed':3, 'Full-time student':4, 'Retired':5, 'Student':6, 'Looking after home or family':7, 'Long-term sick or disabled':8, 'Others':9}\n",
    "dict_occupation={'No code required':0,'Managers, Directors and Senior Officials':1, 'Professional Occupations':2, 'Associate Professional and Technical Occupations':3, 'Administrative and Secretarial Occupations':4, 'Skilled Trades Occupations':5, 'Caring, Leisure and Other Service Occupations':6, 'Sales and Customer Service Occupations':7, 'Process, Plant and Machine Operatives':8, 'Elementary Occupations':9}\n",
    "dict_industry={'No code required':0,'Agriculture, forestry and fishing':1, 'Mining and quarrying; Manufacturing; Electricity, gas, steam and air conditioning system; Water supply':2, 'Construction':3, 'Wholesale and retail trade; Repair of motor vehicles and motorcycles':4, 'Accommodation and food service activities':5, 'Transport and storage; Information and communication':6, 'Financial and insurance activities; Intermediation':7, 'Real estate activities; Professional, scientific and technical activities; Administrative and support service activities':8, 'Public administration and defence; compulsory social security':9, 'Education':10, 'Human health and social work activities':11, 'Other community, social and personal service activities; Private households employing domestic staff; Extra-territorial organisations and bodies':12}\n",
    "dict_hours_worked_per_week={ 'No code required':0,'Part-time: 15 or less hours worked':1, 'Part-time: 16 to 30 hours worked':2, 'Full-time: 31 to 48 hours worked':3, 'Full-time: 49 or more hours worked':4}\n",
    "dict_approximated_social_grade={'No code required':0,'AB':1, 'C1':2, 'C2':3, 'DE':4}\n",
    "dict_region={\"North East\":1,\"North West\":2,\"Yorkshire and the Humber\":3,\"East Midlands\":4,\"West Midlands\":5,\"East of England\":6,\"London\":7,\"South East\":8,\"South West\":9,\"Wales\":10}\n",
    "#list with all the dictionaries for each variable\n",
    "dict_variables=[dict_region,dict_residence_type,dict_family_composition, dict_population_base,dict_sex,dict_age,dict_marital_status,dict_student, dict_country_of_birth,dict_health,dict_ethnic_group,dict_religion, dict_economic_activity,dict_occupation,dict_industry,dict_hours_worked_per_week,dict_approximated_social_grade]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data visualization and descrpitive analysis\n",
    "\n",
    "<p style='text-align: justify;'>Finally, we have defined four different functions that will be used to plot the main descriptive plots of each variable interactively.\n",
    "    \n",
    "First, the **generate_pie_chart**, will take in the name of a variable and return the pie chart showing the distribution of this variable.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pie_chart(Variable):\n",
    "    \n",
    "        #we plot the pie chart using the value_count which computes the frequency of each value. We also use sort_index \n",
    "        #to sort them in order that it will be easier to match the labels in the legend\n",
    "        plt.pie(df_Small[Variable].value_counts().sort_index(), labels=None,autopct='%1.1f%%')\n",
    "        #the title of the plot in bold\n",
    "        plt.title(Variable,weight='bold', size=14)\n",
    "        #the elgend of the plot using the function match_varaible_lables to have the labels of the variable\n",
    "        plt.legend( match_variable_labels(Variable),loc='center left', bbox_to_anchor=(1.5, 0.5));\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Then, the **generate_relation_plot**, creates a barplot that relates the variable passed as argument and the Hours worked per week variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_plot(Variable):\n",
    "        #a stores a crosstable of two factors which are the variable passed as an argument and Hours worked per week\n",
    "        a=pd.crosstab(df_Small[\"Hours worked per week\"],df_Small[Variable])\n",
    "        #we plot the crosstable\n",
    "        a.plot.bar()\n",
    "        #plot the legend using the same technique as in generate_pie function \n",
    "        plt.legend(match_variable_labels(Variable).keys(),loc='center left', bbox_to_anchor=(1.5, 0.5))\n",
    "        #we configure the values of the x axis to see the different labels of the variable Hours worked per week\n",
    "        plt.xticks(list(dict_hours_worked_per_week.values()), list(dict_hours_worked_per_week.keys()),rotation=\"vertical\")\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "The **match_variable_labels**, is an auxiliar function that will be useful in different parts of the project as it matches the variable with the labels corresponding to the values of the variable. Specifically, it return the list of the labels .</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_variable_labels(Variable):\n",
    "    #we implemnt a for loop that will have a boolean condition to see wether the variable passed is equal to the variable of \n",
    "    #the variable list passed in the for loop. If it is we will have the position of the dictionary of the current variable in \n",
    "    #the list of dictionaries. This dictionary will be returned as label. In the other function we will use the method .keys() \n",
    "    #to get the keys of the dictionaries as lables.\n",
    "    for i in range(len(variables)):\n",
    "        if Variable == variables[i]:\n",
    "            label=dict_variables[i]\n",
    "            break\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Finally, the **descriptive** function anidates the other three functions to plot the pie chart of the variable and the bar plot relating the variable with Hours worked per week.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive(Variable,Execute):\n",
    "      \n",
    "    #if the execute variable is true it will execute the function(enter the if)        \n",
    "    if Execute==True:\n",
    "        #print the title        \n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Pie Chart          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "        #show the plot created in the function generate_pie_chart\n",
    "        generate_pie_chart(Variable).show()\n",
    "        #print title\n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Relation with Hours worked per week          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "        #show the plot created in the function generate_relation_plot\n",
    "        generate_relation_plot(Variable).show()\n",
    "    else: \n",
    "        #if the execute varaible is not pressed this message will be displayed\n",
    "        print(\"In order to display the plots you need to press the execute button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a41b7b335343d5965b0815f7c1445d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('Region', 'Residence Type', 'Family Compositio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#in this cell we use the interactive function from ipywidgets to make the descrptive function interactives. We can choose \n",
    "#wether to execute or not the function by clicking the number and the variable to be ploted from all the variables\n",
    "w=interactive(descriptive,Variable=variables,Execute=False)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the pie charts, people in the sample are almost equally distributed to what regards regions of origin, gender, age, industry sector, occupation, and approximated social grade. \n",
    "\n",
    "London, South East and North West are the regions with most of the full-time job workers: this is probably because of the high cost of life in London and South East, while a lot of steel industries, mines and factories were built in the North-West, and that's why plenty of workers went living there.\n",
    "\n",
    "Quite logically, given that feeding and supporting a family requires more money and, most of the time, more hours worked, married people, which corresponds to more than half of the sample, are mostly involved in full time jobs.\n",
    "\n",
    "To what regards gender differences, we can see that males and females are equally distributed, but at the same time males are more involved in full time jobs while females are more involved with part-time jobs: this could be related to the fact that  women are generally seen as responsible for houseworks and other chores not necessarily related with their jobs.\n",
    "\n",
    "People in the sample are also equally split along all the age ranges present in the analysis, with people from 25 to 54 more involved with full-time jobs and people under 25 involved with part-time jobs, given that most people under 25 are probably student, and therefore they don't have time for a full-time job.\n",
    "\n",
    "87.5% of the sample are white people, with the biggest minority being asian people.\n",
    "\n",
    "It's interesting to note that \"professional occupations\" and \"associate professional and technical occupations\" are the two industry sectors mostly present in full-time jobs in the 31 to 48 hours range, while \"Managers, directors and seniors officials\" are the majority in 49 or more hours range.\n",
    "\n",
    "At the same time, industry types with more hours worked are \"Real Estate activities; Professional, Scientific and Technical activities; Administrative and support service activities\", \"Wholesale and Retail Trade; Repair of motor vehicles and motorcycles\", and \"Transport and Storage; Information and Communication\".\n",
    "\n",
    "As a final remark, AB approximated social grade, which corresponds to \"higher and intermediate managerial, administrative and professional occupations\", is the one with most hours worked; C1 grade follows, corresponding to \"supervisory, clerical and junior managerial, administrative, professional occupations\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>As a brief introduction, a Neural Network is a trainable tool that is formed by connecting the different\n",
    "layers containing neural units, which are the processing elements, that enable the Neural\n",
    "Network to learn and recognise patterns. Data propagates from\n",
    "the input layer to the output layer by connecting neurons in that single direction. Within\n",
    "the process, each neuron receives inputs from previous neurons and transfers outputs to\n",
    "next neurons. Depending on the strength of each connection, a weight is assigned to it,\n",
    "turning each neuron into a function of weighted inputs that in turn generate an output\n",
    "that is used as input to the next neuron. Once we have the\n",
    "output layer arising from forward feeding, the error of this is calculated by comparing\n",
    "the outputs generated with the true values of the dependent variable. Then, the error\n",
    "propagates backwards through the hidden layers until it reaches the input layer to readjust\n",
    "the weights seeking to minimise the error (”learning”) and that is why it is called back\n",
    "propagation. The aforementioned adjustments are produced by iteration with the objective of converging to an optimum. Therefore we have to set all the parameters mentioned (number of hidden layers and nuerons, loss-function, optimizer, the activation function and the learning rate) and train our model to get the best results.\n",
    "\n",
    "First of all we will prepare our data to train and evaluate our model. In order to do so we will one-hot encode all the nominal categorical variables in order to make each of the values of the variables binary which makes it easier for the model to detect paterns as it helps reduce the bias that the different variable scales may produce.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the list of the varaibles we are going to one-hot encode\n",
    "dummy_variables=list(column_names[1:])\n",
    "#We remove the ordinal categorical variables from the list\n",
    "dummy_variables.remove('Age')\n",
    "dummy_variables.remove('Health')\n",
    "#We remove the variable that will be the target\n",
    "dummy_variables.remove('Hours worked per week')\n",
    "#We create the dataset that will be used to feed the neural network which will contain the ordinal vairales and the one-hot encoded\n",
    "#nominal variables together with the target\n",
    "df_nn=df_Small[['Age','Health','Hours worked per week']]\n",
    "\n",
    "#We implement a for loop that will one-hot encode each of the nominal variables and put them in th enew database\n",
    "for column in dummy_variables:\n",
    "    #it will store the arrays of 1s and 0s for each row of the column and add them in the new dataset. TH eprefix argument is the\n",
    "    #first part of the new column name.\n",
    "    dummies = pd.get_dummies(df_Small[column],prefix=column)\n",
    "    #create the new columns (as many as values could take the variable when was integer encoded)in the new dataset\n",
    "    df_nn[dummies.columns] = dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Now that we have the dataset prepared, we have to separate the data in the features that will be used to predict the hours worked per week and also in the target variable itself. After this we are going to split the features and the target varaibles between the trainning set and the test set. Therefore, we are going to have the train set of features (x_train), the train set  of variables(y_train), the test set of features(x_test) and the test set of targets (y_test). Finally we will standarize the variables using the StandardScaler function from the sklearn library.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the list of features excluding the target variable\n",
    "features=list(df_nn.columns)\n",
    "features.remove('Hours worked per week')\n",
    "\n",
    "#We set the features data to x and the target variable to y\n",
    "x=df_nn[features]\n",
    "y=df_nn[[\"Hours worked per week\"]]\n",
    "\n",
    "#We randomly split the data putting 20% of the observations into the test set and 80% in the trainning set \n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=50,test_size=0.20)\n",
    "#We standarize the features variables by  deducting the mean and dividing by the standard deviation \n",
    "scaler=StandardScaler().fit(x_train)\n",
    "x_train=scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Once the data is prepared we have to configure the neural network model. To do so we are goig to use the keras package from tensorflow. Concretely, we have decided to anidate three different for loops that will produce 8 different neural networks. This is a way of doing a little bit of parameter tunning. We could have choosen more options in each for and implement more for loops in order to improve our tunning but it will have taken to much time. Nonetheless, we have followed a trial and error method before doing this parameter tunning in order to choose the other parameters. Each of these neural networks will be stored in an array. Concretely, we will store the accruacy metric for the test set of each neural network, the list of the validation loss, the trainning loss, the validation accuracy and the trainning accuracy of each iteration when training the different neural networks to be able to plot the curves and analyse if there is over or underfitting. BE AWARE THAT NEXT CELL IS GOING TO TRAIN 8 NEURAl NETWORKS WHICH CAN TAKE UP TO 40 MINUTES.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/2429146698.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#we set the optimizer as 'Adam',which is a stochastic gradient descent method that is based on adaptive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#estimation of first-order and second-order moments. We also set the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;31m#initialize the model as Sequential which groups different layyers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#we will set to options of the activation functions which will be the relu and the sigmoid functions\n",
    "activation_function = ['relu', 'sigmoid']\n",
    "#the first hidden layer will have 100 or 80 neurons\n",
    "neurons_1 = [100, 80]\n",
    "#the second hidden layer will have 50 or 40 neurons\n",
    "neurons_2 = [50,40]\n",
    "#we initialize the array that will store the data of each neural network\n",
    "results = []\n",
    "\n",
    "#We will have 8 neural networks given that we have two options per loop (2^3)\n",
    "#Activations will take values 'relu' and generate 4 neural networks and then 'sigmoid'and generate 4 more neural networks\n",
    "for activations in activation_function:\n",
    "    #neuron_1 will take 100 and 80 values\n",
    "    for neuron_1 in neurons_1:\n",
    "        #neuron_2 will take 50 and 40 values\n",
    "        for neuron_2 in neurons_2:\n",
    "            # Train the model\n",
    "            #we set the optimizer as 'Adam',which is a stochastic gradient descent method that is based on adaptive \n",
    "            #estimation of first-order and second-order moments. We also set the learning rate \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "            #initialize the model as Sequential which groups different layyers \n",
    "            model=models.Sequential()\n",
    "            #we add the input layer and 1 hidden layer with the neurons (100 or 80) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_1,activation=activations,input_shape=(x_train.shape[1],)))\n",
    "            #we add the second hidden layer with the neurons (50 or 40) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_2, activation=activations))\n",
    "            #we add the output layer with 5 neurons given that we have 5 cateogires with the softmax function which gives a probabilitie\n",
    "            #for every category or neuron\n",
    "            model.add(layers.Dense(5,activation='softmax'))\n",
    "            #we compile the model and set the loss function to 'sparse_categorical_corssentropy', that suits our case as our target variable\n",
    "            #is integer encoded. We set the optimizer as explained before and we choose the accuracy as the metric to evaluate the performance.\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "            #now we  fit the model with the data and train the model with 100 epochs and 90 as batch size. Epochs are the number of times that \n",
    "            #the model will go trough the entire training set in the training process whereas batches are the number of training samples to work through \n",
    "            #before updating the model. Finally, we set to 20% the proportion of data from the training set that will be part of the validation set used to\n",
    "            #control for over or under fitting and for the parameter tuning. Moreover, we store the whole trainning process in the varaible history\n",
    "            history=model.fit(x_train,y_train,epochs=100,batch_size=90,validation_split=0.20)\n",
    "            \n",
    "            #we also store the predictions that the model does with the features of the test set\n",
    "            predictions=model.predict(x_test)\n",
    "            y_predictions=[]\n",
    "            #this for will select the maxium from the array of 5 probabilities for each category that has been predicted and append it to the list of predictions\n",
    "            for i in range(predictions.shape[0]):\n",
    "                maximum = np.argmax(predictions[i])\n",
    "                y_predictions.append(maximum)\n",
    "            \n",
    "            \n",
    "            # Append the current results trough a dictionary\n",
    "            results.append({\n",
    "                #we round the accouracy and multiply it by 100\n",
    "                'Accuracy': round(model.evaluate(x_test, y_test)[1]*100, 2),\n",
    "                'activation_function': activations,\n",
    "                'neuron_1': neuron_1,\n",
    "                'neuron_2': neuron_2,\n",
    "                'loss':history.history['loss'],\n",
    "                'val_loss':history.history['val_loss'],\n",
    "                'accuracy':history.history['accuracy'],\n",
    "                'val_accuracy':history.history['val_accuracy'],\n",
    "                'predictions':y_predictions\n",
    "            })\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data sotred for each neural network, we can do a tbale to compare them and see their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Accuracy', 'activation_function', 'neuron_1', 'neuron_2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/685334751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcomparison\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model 1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Model 2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model 3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Model 4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Model 5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model 6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Model 7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Model 8'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#keep only the important variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mresult1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomparison\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'activation_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'neuron_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'neuron_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mresult1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Accuracy', 'activation_function', 'neuron_1', 'neuron_2'] not in index\""
     ]
    }
   ],
   "source": [
    "# Convert the array to Pandas DataFrame \n",
    "comparison = pd.DataFrame(results)\n",
    "#add the model column\n",
    "comparison['Model']=['Model 1','Model 2', 'Model 3','Model 4','Model 5', 'Model 6','Model 7','Model 8']\n",
    "#keep only the important variables\n",
    "result1=comparison[['Model','Accuracy','activation_function','neuron_1','neuron_2']]\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the characteristics of each model but we have to analyse each of them more in depth to see if there is over or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate the dictionary to know the index of each model\n",
    "dict_models={'Model 1':0,'Model 2':1, 'Model 3':2,'Model 4':3,'Model 5':4, 'Model 6':5,'Model 7':6,'Model 8':7}\n",
    "#generate a list with the name of each model\n",
    "model_num=list(dict_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>We are going to create three different functions to plot the results of each model iteractively.\n",
    "\n",
    "First, the **overfitting_plots function**, creates a figure with two subbplots. In the first sublot we can see the curves of the training loss and the validation-loss. This function takes the index of the model as argument and returns the figure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_plots(option):\n",
    "    #we create two subplots of a certain dimention each    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "    #we define the variable loss as the array generated by the loss function when iterating in the training process extracted form the comparison data frame.\n",
    "    loss =comparison.iloc[option]['loss']\n",
    "    #the same as before but with the validation set\n",
    "    val_loss = comparison.iloc[option]['val_loss']\n",
    "    #we set de epochs as the length of the list\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    #we plot the epochs in the x axis and the loss in the y axis in yellow\n",
    "    ax1.plot(epochs, loss, 'y', label='Training loss')\n",
    "    #same as before with the validation loss in red\n",
    "    ax1.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    #put the title of the subplot\n",
    "    ax1.title.set_text('Training and validation loss')\n",
    "    #set the x and the y label\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    #plot the legend\n",
    "    ax1.legend()\n",
    "    #we do the second plot as the first but changing the loss for the accuracy.\n",
    "    accuracy =comparison.iloc[option]['accuracy']\n",
    "    val_accuracy =comparison.iloc[option]['val_accuracy']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    ax2.plot(epochs, accuracy, 'y', label='Accuracy')\n",
    "    ax2.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "    ax2.title.set_text('Training and validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Second, the **observed_against_predicted ** function, takes the index of the model in the comparison table as argument and returns a barplot with the observed value against the predicted value for the first 20 observations of the test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed_against_predicted(option):\n",
    "    #we first construct a dataframe with the column of the observd hours worked per week and the predicted ones(two columns)\n",
    "    y_test['Predicted']=comparison.iloc[option]['predictions']\n",
    "    y_test. rename(columns = {'Hours worked per week':'Observed'},inplace = True)\n",
    "    #we reset the index to go from 0 onwards \n",
    "    y_test.reset_index(drop=True,inplace=True)\n",
    "    #we sum one to all the values of the dataframe to be able to plot the vaalue 0\n",
    "    y_test2=y_test+1\n",
    "    #we plot only the 20 first values of the dataframe\n",
    "    table=y_test2.head(20)\n",
    "    table.plot.bar()\n",
    "    #we switch the values of the y axis to make them the cateogries of the hours worked per week variable\n",
    "    plt.yticks([0,1,2,3,4,5],[\"\",\"Not working\",\"Part-time: 15 or less hours worked\",\"Part-time: 16 to 30 hours worked\",\"Full-time: 31 to 48 hours worked\",\"Full-time: 49 or more hours worked\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Last, the **plot_model_results**, takes the name of th model as argument and has the previous two functions anidated so that it plots both the ocer or underfitting  plots and the comaprison of predicted against oserved values of the model choosed. Moreover, it prints a string that indicates the accuracy and other characteristics of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model):\n",
    "    #using the previous dictionary  we find the index with the name of the model\n",
    "    option=dict_models[model]\n",
    "    #print the title\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+\"                                                           \" +model+ ' Results:'+'\\033[0m')\n",
    "    #plot the curves of over and underfitting\n",
    "    overfitting_plots(option).show()\n",
    "    #prints a title to separate both plots\n",
    "    print('--------------------------------------------------------')\n",
    "    print('  Hours worked per week: Observed against Predicted  ')\n",
    "    print('--------------------------------------------------------')\n",
    "    #plots the arplot of the values predicted and observed\n",
    "    observed_against_predicted(option)\n",
    "    #prints the string with the data from the model\n",
    "    print('For this model we have used '+ str(comparison.iloc[option]['activation_function']) +' as activation function and the first hidden layer had '+str(comparison.iloc[option]['neuron_1']) \n",
    "          + ' neurons while the second hidden layer had '+str(comparison.iloc[option]['neuron_2']) + ' neurons. The accuracy of this model measured with the test set has been: '+ str(comparison.iloc[option]['Accuracy']) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e9e6f72989467dbef03e57a53f0787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('Model 1', 'Model 2', 'Model 3', 'Model 4', 'Mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we finally make the last function described interactively and we set all the model names as the options\n",
    "#to choose in the parameter of the function\n",
    "res=interactive(plot_model_results,model=model_num)\n",
    "display(res)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTSS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "\n",
    "#model=models.Sequential()\n",
    "#model.add(layers.Dense(80,activation='relu',input_shape=(x_train.shape[1],)))\n",
    "#model.add(layers.Dense(80, activation='relu'))\n",
    "#model.add(layers.Dense(25, activation='relu'))\n",
    "#model.add(layers.Dense(5,activation='softmax'))\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "#history=model.fit(x_train,y_train,epochs=20,batch_size=100,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posterior2(Variable_1,value_1,Variable_2,value_2,Execute):\n",
    "    if Execute:\n",
    "        dictionary_1=match_variable_labels(Variable_1)\n",
    "        dictionary_2=match_variable_labels(Variable_2)\n",
    "        num_value_1=dictionary_1[value_1]\n",
    "        num_value_2=dictionary_2[value_2]\n",
    "        freq_1=(df_Small[Variable_1]== num_value_1).sum()\n",
    "        freq_2=(df_Small[Variable_2]== num_value_2).sum()\n",
    "        range_p=np.linspace(0,1,50)\n",
    "        prior=np.ones_like(range_p)\n",
    "        likelihood_1=sc.stats.binom.pmf(freq_1,len(df_Small),range_p)\n",
    "        likelihood_2=sc.stats.binom.pmf(freq_2,len(df_Small),range_p)\n",
    "        posterior_1=likelihood_1*prior/np.sum(likelihood_1*prior)\n",
    "        posterior_2=likelihood_2*prior/np.sum(likelihood_2*prior)\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "        \n",
    "        ax1.plot(range_p,posterior_1)\n",
    "        ax1.title.set_text(\"Posterior when \"+Variable_1+ \" is equal to \"+value_1)\n",
    "        \n",
    "            \n",
    "        \n",
    "        ax2.plot(range_p,posterior_2)\n",
    "        \n",
    "        ax2.title.set_text(\"Posterior when \"+Variable_2+ \" is equal to \"+value_2)\n",
    "    \n",
    "        return plt\n",
    "    else: \n",
    "        print(\"The execute button has to be pressed in order to plot see the plots\")\n",
    "    \n",
    "def generate_posterior_variable2(Variable_1,Variable_2):\n",
    "    \n",
    "    dictionary_1=match_variable_labels(Variable_1)\n",
    "    dictionary_2=match_variable_labels(Variable_2)\n",
    "    list_1=list(dictionary_1.keys())\n",
    "    list_2=list(dictionary_2.keys())\n",
    "    \n",
    "    Variable_1 = widgets.Text(value=Variable_1,placeholder=Variable_1,description='Variable:',disabled=False)\n",
    "    value_1 = widgets.Dropdown(options=list_1, value=list_1[0],description='Value:')\n",
    "    Variable_2 =  widgets.Text(value=Variable_2,placeholder=Variable_2,description='Variable:',disabled=False)\n",
    "    value_2 = widgets.Dropdown(options=list_2, value=list_2[0],description='Value:')\n",
    "    Execute=widgets.ToggleButton(options=\"Execute\",description='Execute',style={\"button_width\": \"10px\"})\n",
    "    ui = widgets.HBox([Execute,Variable_1,value_1,Variable_2,value_2])\n",
    "    \n",
    "    \n",
    "    out=widgets.interactive_output(generate_posterior2, {'Execute':Execute,'Variable_1': Variable_1,'value_1':value_1, 'Variable_2' : Variable_2,'value_2' : value_2})\n",
    "    display(ui,out)\n",
    "\n",
    "def generate_posterior(Variable_1,value_1,Execute):\n",
    "    if Execute:\n",
    "        dictionary_1=match_variable_labels(Variable_1)\n",
    "        \n",
    "        num_value_1=dictionary_1[value_1]\n",
    "        \n",
    "        freq_1=(df_Small[Variable_1]== num_value_1).sum()\n",
    "        \n",
    "        range_p=np.linspace(0,1,50)\n",
    "        prior=np.ones_like(range_p)\n",
    "        likelihood_1=sc.stats.binom.pmf(freq_1,len(df_Small),range_p)\n",
    "        \n",
    "        posterior_1=likelihood_1*prior/np.sum(likelihood_1*prior)\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.plot(range_p,posterior_1)\n",
    "        plt.title(\"Posterior when \"+Variable_1+ \" is equal to \"+value_1)\n",
    "        \n",
    "    \n",
    "        return plt\n",
    "    else: \n",
    "        print(\"The execute button has to be pressed in order to plot see the plots\")\n",
    "    \n",
    "def generate_posterior_variable(Variable_1):\n",
    "    \n",
    "    dictionary_1=match_variable_labels(Variable_1)\n",
    "    \n",
    "    list_1=list(dictionary_1.keys())\n",
    "    \n",
    "    \n",
    "    Variable_1 = widgets.Text(value=Variable_1,placeholder=Variable_1,description='Variable:',disabled=False)\n",
    "    value_1 = widgets.Dropdown(options=list_1, value=list_1[0],description='Value:')\n",
    "    \n",
    "    Execute=widgets.ToggleButton(options=\"Execute\",description='Execute',style={\"button_width\": \"10px\"})\n",
    "    ui = widgets.HBox([Execute,Variable_1,value_1])\n",
    "    \n",
    "    \n",
    "    out=widgets.interactive_output(generate_posterior, {'Execute':Execute,'Variable_1': Variable_1,'value_1':value_1})\n",
    "    display(ui,out)\n",
    "    \n",
    "    \n",
    "\n",
    "                  \n",
    "#generate_posterior_variable2(\"Region\",\"Residence Type\")\n",
    "#generate_posterior_variable2(\"Family Composition\",\"Population Base\")\n",
    "#generate_posterior_variable2(\"Sex\",\"Age\")\n",
    "#generate_posterior_variable2(\"Marital Status\",\"Student\")\n",
    "#generate_posterior_variable2(\"Country of Birth\",\"Health\")\n",
    "#generate_posterior_variable2(\"Ethnic Group\",\"Religion\")\n",
    "#generate_posterior_variable2(\"Economic Activity\",\"Occupation\")\n",
    "#generate_posterior_variable2(\"Industry\",\"Hours worked per week\")\n",
    "#generate_posterior_variable(\"Approximated Social Grade\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c277af4719b5488c849c4328d92b76bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(ToggleButton(value=False, description='Execute'), Text(value='Approximated Social Grade', descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6930a6ac99b74576ad00ae95266abdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_posterior_variable(\"Approximated Social Grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness analysis (0.5 points)\n",
    "\n",
    "How robust are your results? Think of things like:\n",
    "\n",
    "* outliers in the data\n",
    "* different specification of regression, neural network etc.\n",
    "* split of data in train, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c76eccb6035c07aa27eec1492ed7ed3",
     "grade": true,
     "grade_id": "cell-365f4bd48b20ea77",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion (0.5 points)\n",
    "\n",
    "What did you find in the analysis above; what is the answer to the question you started out with.\n",
    "\n",
    "What are weaknesses of your approach that can be improved upon in future research (e.g. in your thesis).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e908b508af5c2244449588615f83a4d",
     "grade": true,
     "grade_id": "cell-f8f3ab0fc4655b00",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
