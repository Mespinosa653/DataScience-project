{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Predicting the number of hours worked: a neural network vs logistic model approach\"\n",
    "COLLABORATORS = \"Matteo Turrini and Manel Espinosa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Datascience-for-Economics-Final-Assignment\" data-toc-modified-id=\"Datascience-for-Economics-Final-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Datascience for Economics Final Assignment</a></span></li><li><span><a href=\"#Research-question-(0.5-points)\" data-toc-modified-id=\"Research-question-(0.5-points)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Research question (0.5 points)</a></span></li><li><span><a href=\"#Motivation-(0.5-points)\" data-toc-modified-id=\"Motivation-(0.5-points)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Motivation (0.5 points)</a></span></li><li><span><a href=\"#Method-and-data-(1.0-point)\" data-toc-modified-id=\"Method-and-data-(1.0-point)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Method and data (1.0 point)</a></span></li><li><span><a href=\"#Preview-of-the-answers-(0.5-points)\" data-toc-modified-id=\"Preview-of-the-answers-(0.5-points)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preview of the answers (0.5 points)</a></span></li><li><span><a href=\"#Main-assumptions-(0.5-points)\" data-toc-modified-id=\"Main-assumptions-(0.5-points)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Main assumptions (0.5 points)</a></span></li><li><span><a href=\"#Python/R-code-(6-points)\" data-toc-modified-id=\"Python/R-code-(6-points)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Python/R code (6 points)</a></span></li><li><span><a href=\"#Robustness-analysis-(0.5-points)\" data-toc-modified-id=\"Robustness-analysis-(0.5-points)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Robustness analysis (0.5 points)</a></span></li><li><span><a href=\"#Discussion-and-conclusion-(0.5-points)\" data-toc-modified-id=\"Discussion-and-conclusion-(0.5-points)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Discussion and conclusion (0.5 points)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535c21960d4663d5edac398cb445d087",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascience for Economics Final Assignment\n",
    "\n",
    "\n",
    "\n",
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Matteo Turrini|2084906|u207794|\n",
    "|Manuel Espinosa|2067814|u112479|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question (0.5 points)\n",
    "\n",
    "Formulate the research question: what question do you want to answer in this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a9d08ea38644e3ac0b85955731a7e3",
     "grade": true,
     "grade_id": "cell-44d71caea99ce92b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "How does personal characteristics help predicting the level of hours worked per week in the UK. We will predict if active people work or not depending on the personal characteristics and also the number of hours they work. We only need data from people over 65 and that is active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation (0.5 points)\n",
    "\n",
    "Motivate why this question is interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74e1c61d4c263e5dcedc9c4f7565432",
     "grade": true,
     "grade_id": "cell-a00025e68181b6f6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Predicting the number of hours worked per week could be a helpful tool in order to classify workers on the basis of this parameter, and then use this information to promote and implement Active Labor Market Programmes (ALMPs). These are government programmes aimed at increasing the rate of employment by bringing unemployed people into employment, preventing already employed people to lose their jobs, and increasing productivity and earnings: this way, governments are able to improve the functioning of labor markets. Therefore, predicting the hours worked per week per type of worker can provide some \"profiles\" of workers that ALMPs can use to improve the efficiency of a working sector or to maintain an already efficient one.\n",
    "\n",
    "Moreover, we think that it would be interesting to compare two methods widely used in classification problems like ours: on one hand, we have  Neural networks, which are capable of disentangling different patterns in the data by using non-linear functions like relu and sigmoid [(Dreiseitl  et al., 2012)](https://doi.org/10.1016/S1532-0464(03)00034-0); on the other hand, we have Logistic Multinomial Regression, which is another classification method used to analyse the relationship between the outcome and multiple independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and data (1.0 point)\n",
    "\n",
    "Which data do you have to analyze the question and what methods are you using to answer the question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b91e9ed0432af5f7bd6e33a6a51515",
     "grade": true,
     "grade_id": "cell-a2a13b6c938ec4c8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The data we have used to conduct the study comes from the English Office for National Statistics, here's the link to the [website](https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/datasets/2011censusteachingfile). \n",
    "    This is a sample of 2011 providing microdata with 569741 observations. People analysed in this sample are classified under 16 cateorical variables:the region, the residence type, family composition, population base, sex, age, marital status, student, country of birth, health, ethnic group, religion, economic activity, occupation, industry, hours worked per week, and approximated social grade. For a more in depth explanation of each variable and the different values it can take you can access this [file](Microdatateachingvariablels.pdf). This dataset is going to be [cleaned](#cleaning_data) and [transformed](#data_transformation) using pandas library. Once the data is ready there will be a brief descriptive analysis of our variables to see any relevant distribution or relationship.\n",
    "    \n",
    "   Regarding the methodology of our study it will consist in predicting, following a classification problem, the hours worked per week of our individuals. In order to do so, we first have to prepare our features and our target variable and have them separate. Having done this, we will first apply a [Neural network approach](#neural_network), in which we will compare several neural networks by implementing a simple parameter tunning technique; Then, we will implement a [logistic multinomial regression](#logistic_regression) comparing a few of them by introducing the penalty term. Finally, we will choose the best model for each approach based on its performance and compare them by implementing a [bootstrapping](#bootstrapping) tool that will serve as a robustness analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers (0.5 points)\n",
    "\n",
    "Summarize the results that you find and the answer to your research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a675684538158f19ff2a70fa98e7fff2",
     "grade": true,
     "grade_id": "cell-c2854f0b8b034fae",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions (0.5 points)\n",
    "\n",
    "What are the main assumptions that you need to answer the question with your data? For example, assumptions about missing data, random sampling, stability of the data generating process. If the aim is causal inference, then also make clear your assumptions about the random assignment of treatment (e.g. plot a DAG to illustrate your view on the causal relations between the variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43d4f76461858e4d6140ccbf5347ab6",
     "grade": true,
     "grade_id": "cell-03cb6a4826b945c9",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In order to answer to the research question with our analysis, we need to make two main assumptions: first of all, we assume the survey to be randomized in order to minimize data corruption, avoiding bias, and improve data collection. Our other main assumption is that we assume, in order for this model to be valid, that there are no significant changes in the economic environment, the industry or the markets: for this reason, we can't guarantee that this model will hold in the future if there will be significant changes in the aforementioned fields. \n",
    "Given that we don't establish any causal relationship in the model, there's no need to make additional assumptions regarding the distribution of variables or multicollinearity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/R code (6 points)\n",
    "\n",
    "Give the python/R code in code cells and use markdown cells to explain why you code things in this way and what the outcomes are of the code cells.\n",
    "\n",
    "Create as many python/R and markdown cells as you need to explain things well.\n",
    "\n",
    "If you program mainly in R, you can also use R-studio if you prefer. On jupyterlab, you can switch to an R kernel and combine python and R code in one notebook.\n",
    "\n",
    "## Package importation\n",
    "\n",
    "First of all we need to import the libraries for the functions we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0736d3fd2fd323ad705ff2d7f5f0b8",
     "grade": true,
     "grade_id": "cell-c8076f3091bfd885",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5280/151923514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#is a library designed for machine learning and it contains keras which we will use for the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "#we import the numpy library  for many uses such as generate new data or manipulate arrays\n",
    "import numpy as np\n",
    "#the pandas library to manipulate and transform the data\n",
    "import pandas as pd\n",
    "#the seaborn library which is based on matplotlib and makes the graphs more attractive\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "\n",
    "#matplotlib and concretely pyplot will be useful to plot the different graphs that we need to show our results\n",
    "import matplotlib.pyplot as plt\n",
    "#is a library designed for machine learning and it contains keras which we will use for the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#we will use it to suppress warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "#we import scikit learn to access different statistical tools and functions such as methods to split the data in training and testing set, \n",
    "#to standardize the data if needed and to compute the accuracy between predicted and true values\n",
    "#from sklearn.preprocessing import scale\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#We finally import the ipywidgets package to make our functions interactive and display them\n",
    "from ipywidgets import interact, interactive, fixed, Layout\n",
    "import ipywidgets as widgets \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "We will have to check thoroughly the structure of the data as well as the column types to see how to transform the data in a way that can be used afterwards, taking into account that all of the variables are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation\n",
    "\n",
    "We first import the data from a csv that you will find in the [repository](/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569736</th>\n",
       "      <td>7946020</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569737</th>\n",
       "      <td>7944310</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569738</th>\n",
       "      <td>7945374</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569739</th>\n",
       "      <td>7944768</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569740</th>\n",
       "      <td>7944959</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569741 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Person ID     Region Residence Type Family Composition Population Base  \\\n",
       "0        7394816  E12000001              H                  2               1   \n",
       "1        7394745  E12000001              H                  5               1   \n",
       "2        7395066  E12000001              H                  3               1   \n",
       "3        7395329  E12000001              H                  3               1   \n",
       "4        7394712  E12000001              H                  3               1   \n",
       "...          ...        ...            ...                ...             ...   \n",
       "569736   7946020  W92000004              H                  1               1   \n",
       "569737   7944310  W92000004              H                  3               1   \n",
       "569738   7945374  W92000004              H                  3               1   \n",
       "569739   7944768  W92000004              H                  1               1   \n",
       "569740   7944959  W92000004              H                  2               1   \n",
       "\n",
       "       Sex Age Marital Status Student Country of Birth Health Ethnic Group  \\\n",
       "0        2   6              2       2                1      2            1   \n",
       "1        1   4              1       2                1      1            1   \n",
       "2        2   4              1       2                1      1            1   \n",
       "3        2   2              1       2                1      2            1   \n",
       "4        1   5              4       2                1      1            1   \n",
       "...     ..  ..            ...     ...              ...    ...          ...   \n",
       "569736   1   5              1       2                1      4            1   \n",
       "569737   1   3              1       2                1      2            1   \n",
       "569738   1   1              1       1                1      1            1   \n",
       "569739   2   8              5       2                1      3            1   \n",
       "569740   2   2              2       2                1      2            1   \n",
       "\n",
       "       Religion Economic Activity Occupation Industry Hours worked per week  \\\n",
       "0             2                 5          8        2                    -9   \n",
       "1             2                 1          8        6                     4   \n",
       "2             1                 1          6       11                     3   \n",
       "3             2                 1          7        7                     3   \n",
       "4             2                 1          1        4                     3   \n",
       "...         ...               ...        ...      ...                   ...   \n",
       "569736        9                 1          8        8                     3   \n",
       "569737        1                 1          7        4                     3   \n",
       "569738        2                -9         -9       -9                    -9   \n",
       "569739        9                 5          9        2                    -9   \n",
       "569740        1                 1          7        4                     1   \n",
       "\n",
       "       Approximated Social Grade  \n",
       "0                              4  \n",
       "1                              3  \n",
       "2                              4  \n",
       "3                              2  \n",
       "4                              2  \n",
       "...                          ...  \n",
       "569736                         3  \n",
       "569737                         4  \n",
       "569738                        -9  \n",
       "569739                         4  \n",
       "569740                         4  \n",
       "\n",
       "[569741 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data importation\n",
    "#set the file directory path\n",
    "file=('2011 Census Microdata Teaching File.csv')\n",
    "#use the pandas methos read_csv to create a dataframe from the csv\n",
    "df= pd.read_csv(file, header=None)\n",
    "#we set the column names\n",
    "df.columns = df.iloc[1,:].values\n",
    "#we drop the first two raws of the file as there is no interesting information in these.\n",
    "df.drop([0,1], axis=0, inplace=True)\n",
    "#after manipulating the dataframe, we reset the index of it\n",
    "df_small=df.reset_index(drop=True)\n",
    "df_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation  <a id='data_transformation'></a>\n",
    "\n",
    "Given that all of the variables are integer encoded except from region and Residence type, we preferred to change these two to integer encoded as well. Moreover, all the columns were of the type object (Strings) and we would like them to be integers so we also changed the type of variables of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    object\n",
       "Region                        int64\n",
       "Residence Type                int64\n",
       "Family Composition           object\n",
       "Population Base              object\n",
       "Sex                          object\n",
       "Age                          object\n",
       "Marital Status               object\n",
       "Student                      object\n",
       "Country of Birth             object\n",
       "Health                       object\n",
       "Ethnic Group                 object\n",
       "Religion                     object\n",
       "Economic Activity            object\n",
       "Occupation                   object\n",
       "Industry                     object\n",
       "Hours worked per week        object\n",
       "Approximated Social Grade    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Region and Residence Type to integers:\n",
    "dictionary_region={\"E12000001\":1,\"E12000002\":2,\"E12000003\":3, \"E12000004\":4,\"E12000005\":5,\"E12000006\":6,\"E12000007\":7,\"E12000008\":8,\"E12000009\":9,\"W92000004\":10}\n",
    "dictionary_residence_type={\"C\":1,\"H\":2}\n",
    "#we use the two previous dictionaries created to replace the keys for the values in each column\n",
    "df_small=df_small.replace({\"Region\": dictionary_region,\"Residence Type\":dictionary_residence_type})\n",
    "\n",
    "#We show the types of data of the columns which is mostly objects\n",
    "df_small.dtypes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    int32\n",
       "Region                       int32\n",
       "Residence Type               int32\n",
       "Family Composition           int32\n",
       "Population Base              int32\n",
       "Sex                          int32\n",
       "Age                          int32\n",
       "Marital Status               int32\n",
       "Student                      int32\n",
       "Country of Birth             int32\n",
       "Health                       int32\n",
       "Ethnic Group                 int32\n",
       "Religion                     int32\n",
       "Economic Activity            int32\n",
       "Occupation                   int32\n",
       "Industry                     int32\n",
       "Hours worked per week        int32\n",
       "Approximated Social Grade    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create a list with the column names\n",
    "column_names=list(df_small.columns)\n",
    "#create a dictionary\n",
    "dictionary = dict()\n",
    "#this for will set the column names as the key of the dictionary and the int class as the value\n",
    "for item in column_names:\n",
    "    key = item\n",
    "    value=int\n",
    "    \n",
    "    dictionary[key] = value\n",
    "#we change the data type of each column using the dictionary created\n",
    "df_Small = df_small.astype(dictionary)\n",
    "\n",
    "#show the new data types for the columns\n",
    "df_Small.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clenaing <a id='data_cleaning'></a>\n",
    "\n",
    "Given the scope of our project we don't need data for people under 16 neither students living away during term-time. Therefore, we delete the rows corresponding to these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457118</th>\n",
       "      <td>7944868</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457119</th>\n",
       "      <td>7946020</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457120</th>\n",
       "      <td>7944310</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457121</th>\n",
       "      <td>7944768</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457122</th>\n",
       "      <td>7944959</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457123 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Person ID  Region  Residence Type  Family Composition  \\\n",
       "0         7394816       1               2                   2   \n",
       "1         7394745       1               2                   5   \n",
       "2         7395066       1               2                   3   \n",
       "3         7395329       1               2                   3   \n",
       "4         7394712       1               2                   3   \n",
       "...           ...     ...             ...                 ...   \n",
       "457118    7944868      10               2                   2   \n",
       "457119    7946020      10               2                   1   \n",
       "457120    7944310      10               2                   3   \n",
       "457121    7944768      10               2                   1   \n",
       "457122    7944959      10               2                   2   \n",
       "\n",
       "        Population Base  Sex  Age  Marital Status  Student  Country of Birth  \\\n",
       "0                     1    2    6               2        2                 1   \n",
       "1                     1    1    4               1        2                 1   \n",
       "2                     1    2    4               1        2                 1   \n",
       "3                     1    2    2               1        2                 1   \n",
       "4                     1    1    5               4        2                 1   \n",
       "...                 ...  ...  ...             ...      ...               ...   \n",
       "457118                1    1    2               1        2                 1   \n",
       "457119                1    1    5               1        2                 1   \n",
       "457120                1    1    3               1        2                 1   \n",
       "457121                1    2    8               5        2                 1   \n",
       "457122                1    2    2               2        2                 1   \n",
       "\n",
       "        Health  Ethnic Group  Religion  Economic Activity  Occupation  \\\n",
       "0            2             1         2                  5           8   \n",
       "1            1             1         2                  1           8   \n",
       "2            1             1         1                  1           6   \n",
       "3            2             1         2                  1           7   \n",
       "4            1             1         2                  1           1   \n",
       "...        ...           ...       ...                ...         ...   \n",
       "457118       1             1         2                  1           4   \n",
       "457119       4             1         9                  1           8   \n",
       "457120       2             1         1                  1           7   \n",
       "457121       3             1         9                  5           9   \n",
       "457122       2             1         1                  1           7   \n",
       "\n",
       "        Industry  Hours worked per week  Approximated Social Grade  \n",
       "0              2                      0                          4  \n",
       "1              6                      4                          3  \n",
       "2             11                      3                          4  \n",
       "3              7                      3                          2  \n",
       "4              4                      3                          2  \n",
       "...          ...                    ...                        ...  \n",
       "457118         9                      3                          2  \n",
       "457119         8                      3                          3  \n",
       "457120         4                      3                          4  \n",
       "457121         2                      0                          4  \n",
       "457122         4                      1                          4  \n",
       "\n",
       "[457123 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We replace the -9 values that refer to no code required to the 0 as it will make it easier to plot the graphs\n",
    "df_Small=df_Small.replace(-9,0)\n",
    "#delete the rows in which age is under 16, which corresponds to value 1\n",
    "df_Small = df_Small[~(df_Small[\"Age\"] == 1)]\n",
    "#delete the rows in wich population base is students living away during term-time, which corresponds to value 2\n",
    "df_Small = df_Small[~(df_Small[\"Population Base\"] == 2)]\n",
    "\n",
    "\n",
    "#we define the list variables with all the column names of the dataframe\n",
    "variables=column_names[1:]\n",
    "#we reset the index of the dataframe after all these changes\n",
    "df_Small.reset_index(drop=True,inplace=True)\n",
    "df_Small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels\n",
    "\n",
    "Given that the data is integer encoded we will need dictionaries that associate the label to the value to set the labels in the plots or other implementations in the different functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "#Dictionary for every variable label and its corresponding value\n",
    "dict_family_composition={'Resident of a comunal establishment or short-term resident':0,'Not in a family':1, 'Married':2, 'Cohabiting couple':3, 'Lone parent family (male)':4, 'Lone parent family (female)':5, 'Others':6}\n",
    "dict_residence_type={'Resident in a communal establishment':1,'Not resident in a communal establishment':2}\n",
    "dict_population_base={'Usual resident':1, 'Short-term resident':3}\n",
    "dict_sex={'Male':1, 'Female':2}\n",
    "dict_age={'16 to 24':2, '25 to 34':3, '35 to 44':4, '45 to 54':5, '55 to 64':6, '65 to 74':7, '75 and over':8}\n",
    "dict_marital_status={'Single':1, 'Married':2, 'Separated':3, 'Divorced':4, 'Widowed':5}\n",
    "dict_student={'Yes':1, 'No':2}\n",
    "dict_country_of_birth={'UK':1, 'Non UK':2}\n",
    "dict_health={'Very good health':1, 'Good health':2, 'Fair health':3, 'Bad health':4, 'Very bad health':5}\n",
    "dict_ethnic_group={'White':1, 'Mixed':2, 'Asian and Asian British':3, 'Black or Black British':4, 'Chinese or Other ethnic group':5}\n",
    "dict_religion={'No religion':1, 'Christian':2, 'Buddhist':3, 'Hindu':4, 'Jewish':5, 'Muslim':6, 'Sikh':7, 'Other religion':8, 'Not stated':9}\n",
    "dict_economic_activity={'Employee':1, 'Self-employed':2, 'Unemployed':3, 'Full-time student':4, 'Retired':5, 'Student':6, 'Looking after home or family':7, 'Long-term sick or disabled':8, 'Others':9}\n",
    "dict_occupation={'People that never worked':0,'Managers, Directors and Senior Officials':1, 'Professional Occupations':2, 'Associate Professional and Technical Occupations':3, 'Administrative and Secretarial Occupations':4, 'Skilled Trades Occupations':5, 'Caring, Leisure and Other Service Occupations':6, 'Sales and Customer Service Occupations':7, 'Process, Plant and Machine Operatives':8, 'Elementary Occupations':9}\n",
    "dict_industry={'People never worked':0,'Agriculture, forestry and fishing':1, 'Mining and quarrying; Manufacturing; Electricity, gas, steam and air conditioning system; Water supply':2, 'Construction':3, 'Wholesale and retail trade; Repair of motor vehicles and motorcycles':4, 'Accommodation and food service activities':5, 'Transport and storage; Information and communication':6, 'Financial and insurance activities; Intermediation':7, 'Real estate activities; Professional, scientific and technical activities; Administrative and support service activities':8, 'Public administration and defence; compulsory social security':9, 'Education':10, 'Human health and social work activities':11, 'Other community, social and personal service activities; Private households employing domestic staff; Extra-territorial organisations and bodies':12}\n",
    "dict_hours_worked_per_week={ 'Not working':0,'Part-time: 15 or less hours worked':1, 'Part-time: 16 to 30 hours worked':2, 'Full-time: 31 to 48 hours worked':3, 'Full-time: 49 or more hours worked':4}\n",
    "dict_approximated_social_grade={'Resident in a comunal establishment':0,'AB':1, 'C1':2, 'C2':3, 'DE':4}\n",
    "dict_region={\"North East\":1,\"North West\":2,\"Yorkshire and the Humber\":3,\"East Midlands\":4,\"West Midlands\":5,\"East of England\":6,\"London\":7,\"South East\":8,\"South West\":9,\"Wales\":10}\n",
    "#list with all the dictionaries for each variable\n",
    "dict_variables=[dict_region,dict_residence_type,dict_family_composition, dict_population_base,dict_sex,dict_age,dict_marital_status,dict_student, dict_country_of_birth,dict_health,dict_ethnic_group,dict_religion, dict_economic_activity,dict_occupation,dict_industry,dict_hours_worked_per_week,dict_approximated_social_grade]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Data visualization and descrpitive analysis <a id='descriptive_analysis'></a>\n",
    "\n",
    "<p style='text-align: justify;'>Finally, we have defined four different functions that will be used to plot the main descriptive plots of each variable interactively.\n",
    "    \n",
    "First, the **generate_pie_chart**, will take the name of a variable and return the pie chart showing the distribution of this variable.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pie_chart(Variable):\n",
    "    \n",
    "        #we plot the pie chart using the value_count which computes the frequency of each value. We also use sort_index \n",
    "        #to sort them in order that it will be easier to match the labels in the legend\n",
    "        plt.pie(df_Small[Variable].value_counts().sort_index(), labels=None,autopct='%1.1f%%')\n",
    "        #the title of the plot in bold\n",
    "        plt.title(Variable,weight='bold', size=14)\n",
    "        #the legend of the plot using the function match_varaible_lables to have the labels of the variable\n",
    "        plt.legend( match_variable_labels(Variable),loc='center left', bbox_to_anchor=(1.5, 0.5));\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Then, the <b>generate_relation_plot</b>, creates a barplot that relates the variable passed as argument and the Hours worked per week variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_plot(Variable):\n",
    "        #a stores a crosstable of two factors which are the variable passed as an argument and Hours worked per week\n",
    "        a=pd.crosstab(df_Small[\"Hours worked per week\"],df_Small[Variable])\n",
    "        #we plot the crosstable\n",
    "        a.plot.bar()\n",
    "        #plot the legend using the same technique as in generate_pie function \n",
    "        plt.legend(match_variable_labels(Variable).keys(),loc='center left', bbox_to_anchor=(1.5, 0.5))\n",
    "        #we configure the values of the x axis to see the different labels of the variable Hours worked per week\n",
    "        plt.xticks(list(dict_hours_worked_per_week.values()), list(dict_hours_worked_per_week.keys()),rotation=\"vertical\")\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    The <b>match_variable_labels</b>, is an auxiliar function that will be useful in different parts of the project as it matches the variable with the labels corresponding to the values of the variable. Specifically, it return the list of the labels .</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_variable_labels(Variable):\n",
    "    #we implement a for loop that will have a boolean condition to see whether the variable passed is equal to the variable of \n",
    "    #the variable list passed in the for loop. If that's the case, we will have the position of the dictionary of the current variable in \n",
    "    #the list of dictionaries. This dictionary will be returned as label. In the other function we will use the method .keys() \n",
    "    #to get the keys of the dictionaries as lables.\n",
    "    for i in range(len(variables)):\n",
    "        if Variable == variables[i]:\n",
    "            label=dict_variables[i]\n",
    "            break\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    Finally, the <b>descriptive</b> function groups together the other three functions to plot the pie chart of the variable and the bar plot relating the variable with Hours worked per week.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive(Variable,Execute):\n",
    "      \n",
    "    #if the execute variable is true it will execute the function(enter the if)        \n",
    "    if Execute==True:\n",
    "        #print the title        \n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Pie Chart          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "        #show the plot created in the function generate_pie_chart\n",
    "        generate_pie_chart(Variable).show()\n",
    "        #print title\n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Relation with Hours worked per week          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "        #show the plot created in the function generate_relation_plot\n",
    "        generate_relation_plot(Variable).show()\n",
    "    else: \n",
    "        #if the execute variable is not pressed this message will be displayed\n",
    "        print(\"In order to display the plots you need to press the execute button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32215d67006d46ffa3a39ea2ec83b349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('Region', 'Residence Type', 'Family Compositio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#in this cell we use the interactive function from ipywidgets to make the descriptive functions interactive. We can choose \n",
    "#whether to execute or not the function by clicking the number and the variable to be plotted from all the variables\n",
    "w=interactive(descriptive,Variable=variables,Execute=False)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>As we can see from the pie charts, people in the sample are almost equally distributed to what regards regions of origin, gender, age, industry sector, occupation, and approximated social grade.\n",
    "\n",
    "London, South East and North West are the regions with most of the full-time job workers: this is probably because of the high cost of life in London and South East, while a lot of steel industries, mines and factories were built in the North-West, and that's why plenty of workers went living there.\n",
    "\n",
    "Quite logically, given that feeding and supporting a family requires more money and, most of the time, more hours worked, married people, which corresponds to more than half of the sample, are mostly involved in full time jobs.\n",
    "\n",
    "To what regards gender differences, we can see that males and females are equally distributed, but at the same time males are more involved in full time jobs while females are more involved with part-time jobs: this could be related to the fact that women are generally seen as responsible for houseworks and other chores not necessarily related with their jobs.\n",
    "\n",
    "People in the sample are also equally split along all the age ranges present in the analysis, with people from 25 to 54 more involved with full-time jobs and people under 25 involved with part-time jobs, given that most people under 25 are probably student, and therefore they don't have time for a full-time job.\n",
    "\n",
    "87.5% of the sample are white people, with the biggest minority being asian people.\n",
    "\n",
    "It's interesting to note that \"professional occupations\" and \"associate professional and technical occupations\" are the two industry sectors mostly present in full-time jobs in the 31 to 48 hours range, while \"Managers, directors and seniors officials\" are the majority in 49 or more hours range.\n",
    "\n",
    "At the same time, industry types with more hours worked are \"Real Estate activities; Professional, Scientific and Technical activities; Administrative and support service activities\", \"Wholesale and Retail Trade; Repair of motor vehicles and motorcycles\", and \"Transport and Storage; Information and Communication\".\n",
    "\n",
    "As a final remark, AB approximated social grade, which corresponds to \"higher and intermediate managerial, administrative and professional occupations\", is the one with most hours worked; C1 grade follows, corresponding to \"supervisory, clerical and junior managerial, administrative, professional occupations\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting hours worked : Neural Network approach <a id='neural_network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>As a brief introduction, a Neural Network is a trainable tool that is formed by connecting the different\n",
    "layers containing neural units, which are the processing elements, that enable the Neural\n",
    "Network to learn and recognise patterns. Data propagates from\n",
    "the input layer to the output layer by connecting neurons in that single direction. Within\n",
    "the process, each neuron receives inputs from previous neurons and transfers outputs to\n",
    "next neurons. Depending on the strength of each connection, a weight is assigned to it,\n",
    "turning each neuron into a function of weighted inputs that in turn generate an output\n",
    "that is used as input to the next neuron. Once we have the\n",
    "output layer arising from forward feeding, the error of this is calculated by comparing\n",
    "the outputs generated with the true values of the dependent variable. Then, the error\n",
    "propagates backwards through the hidden layers until it reaches the input layer to readjust\n",
    "the weights seeking to minimise the error (”learning”) and that is why it is called back\n",
    "propagation. The aforementioned adjustments are produced by iteration with the objective of converging to an optimum. Therefore we have to set all the parameters mentioned (number of hidden layers and nuerons, loss-function, optimizer, the activation function and the learning rate) and train our model to get the best results.\n",
    "\n",
    "First of all we will prepare our data to train and evaluate our model. In order to do so we will one-hot encode all the nominal categorical variables in order to make each of the values of the variables binary which makes it easier for the model to detect paterns as it helps reduce the bias that the different variable scales may produce.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the list of variables we are going to one-hot encode\n",
    "dummy_variables=list(column_names[1:])\n",
    "#We remove the ordinal categorical variables from the list\n",
    "dummy_variables.remove('Age')\n",
    "dummy_variables.remove('Health')\n",
    "#We remove the variable that will be the target\n",
    "dummy_variables.remove('Hours worked per week')\n",
    "#We create the dataset that will be used to feed the neural network which will contain the ordinal variables and the one-hot encoded\n",
    "#nominal variables together with the target\n",
    "df_nn=df_Small[['Age','Health','Hours worked per week']]\n",
    "\n",
    "#We implement a for loop that will one-hot encode each of the nominal variables and put them in the new database\n",
    "for column in dummy_variables:\n",
    "    #it will store the arrays of 1s and 0s for each row of the column and add them in the new dataset. The prefix argument is the\n",
    "    #first part of the new column name.\n",
    "    dummies = pd.get_dummies(df_Small[column],prefix=column)\n",
    "    #create the new columns (as many as values could take the variable when was integer encoded)in the new dataset\n",
    "    df_nn[dummies.columns] = dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Now that we have the dataset prepared, we have to separate the data in the features that will be used to predict the hours worked per week and also in the target variable itself. After this we are going to split the features and the target variables between the training set and the test set. Therefore, we are going to have the train set of features (x_train), the train set  of variables(y_train), the test set of features(x_test) and the test set of targets (y_test). Finally we will standardize the variables using the StandardScaler function from the sklearn library.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the list of features excluding the target variable\n",
    "features=list(df_nn.columns)\n",
    "features.remove('Hours worked per week')\n",
    "\n",
    "#We set the features data to x and the target variable to y\n",
    "x=df_nn[features]\n",
    "y=df_nn[[\"Hours worked per week\"]]\n",
    "\n",
    "#We randomly split the data putting 20% of the observations into the test set and 80% in the training set \n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=50,test_size=0.20)\n",
    "\n",
    "#After claning and transforming the data we think there is no need to standarize our cateogrical variables.??\n",
    "#????We standardize the features variables by  deducting the mean and dividing by the standard deviation \n",
    "#scaler=StandardScaler().fit(x_train)\n",
    "#x_train=scaler.transform(x_train)\n",
    "#x_test=scaler.transform(x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Once the data is prepared we have to configure the neural network model. To do so we are goig to use the keras package from tensorflow. Concretely, we have decided to nest three different for loops that will produce 8 different neural networks. This is a way of doing a little bit of parameter tunning. We could have choosen more options in each for and implement more for loops in order to improve our tuning but it will have taken to much time. Nonetheless, we have followed a trial and error method before doing this parameter tuning in order to choose the other parameters. Each of these neural networks will be stored in an array. Concretely, we will store the accruacy metric for the test set of each neural network, the list of the validation loss, the trainning loss, the validation accuracy and the trainning accuracy of each iteration when training the different neural networks to be able to plot the curves and analyse if there is over or underfitting. <b>BE AWARE THAT NEXT CELL IS GOING TO TRAIN 8 NEURAl NETWORKS WHICH CAN TAKE UP TO 40 MINUTES</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.1783 - accuracy: 0.5894 - val_loss: 0.9782 - val_accuracy: 0.6994\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.8344 - accuracy: 0.7265 - val_loss: 0.7167 - val_accuracy: 0.7457\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.6607 - accuracy: 0.7527 - val_loss: 0.6158 - val_accuracy: 0.7639\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5971 - accuracy: 0.7669 - val_loss: 0.5796 - val_accuracy: 0.7702\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5742 - accuracy: 0.7701 - val_loss: 0.5660 - val_accuracy: 0.7719\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5647 - accuracy: 0.7708 - val_loss: 0.5598 - val_accuracy: 0.7729\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5600 - accuracy: 0.7718 - val_loss: 0.5564 - val_accuracy: 0.7744\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5571 - accuracy: 0.7725 - val_loss: 0.5542 - val_accuracy: 0.7752\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5552 - accuracy: 0.7731 - val_loss: 0.5525 - val_accuracy: 0.7754\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5537 - accuracy: 0.7740 - val_loss: 0.5513 - val_accuracy: 0.7761\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5525 - accuracy: 0.7743 - val_loss: 0.5502 - val_accuracy: 0.7761\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5515 - accuracy: 0.7746 - val_loss: 0.5493 - val_accuracy: 0.7763\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5506 - accuracy: 0.7752 - val_loss: 0.5485 - val_accuracy: 0.7767\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5498 - accuracy: 0.7754 - val_loss: 0.5478 - val_accuracy: 0.7769\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5491 - accuracy: 0.7758 - val_loss: 0.5471 - val_accuracy: 0.7771\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5483 - accuracy: 0.7758 - val_loss: 0.5464 - val_accuracy: 0.7774\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5477 - accuracy: 0.7764 - val_loss: 0.5459 - val_accuracy: 0.7778\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5471 - accuracy: 0.7764 - val_loss: 0.5452 - val_accuracy: 0.7778\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5465 - accuracy: 0.7766 - val_loss: 0.5448 - val_accuracy: 0.7782\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5459 - accuracy: 0.7768 - val_loss: 0.5440 - val_accuracy: 0.7785\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5453 - accuracy: 0.7771 - val_loss: 0.5435 - val_accuracy: 0.7789\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5447 - accuracy: 0.7772 - val_loss: 0.5429 - val_accuracy: 0.7791\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5442 - accuracy: 0.7775 - val_loss: 0.5427 - val_accuracy: 0.7788\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5437 - accuracy: 0.7777 - val_loss: 0.5419 - val_accuracy: 0.7795\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5432 - accuracy: 0.7779 - val_loss: 0.5414 - val_accuracy: 0.7795\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5427 - accuracy: 0.7781 - val_loss: 0.5409 - val_accuracy: 0.7798\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5422 - accuracy: 0.7783 - val_loss: 0.5404 - val_accuracy: 0.7799\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5418 - accuracy: 0.7784 - val_loss: 0.5401 - val_accuracy: 0.7802\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5413 - accuracy: 0.7785 - val_loss: 0.5396 - val_accuracy: 0.7803\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5408 - accuracy: 0.7788 - val_loss: 0.5393 - val_accuracy: 0.7805\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5404 - accuracy: 0.7789 - val_loss: 0.5388 - val_accuracy: 0.7807\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5400 - accuracy: 0.7790 - val_loss: 0.5383 - val_accuracy: 0.7806\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5396 - accuracy: 0.7791 - val_loss: 0.5381 - val_accuracy: 0.7805\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5391 - accuracy: 0.7793 - val_loss: 0.5376 - val_accuracy: 0.7808\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5387 - accuracy: 0.7796 - val_loss: 0.5371 - val_accuracy: 0.7811\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5383 - accuracy: 0.7793 - val_loss: 0.5370 - val_accuracy: 0.7812\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5379 - accuracy: 0.7796 - val_loss: 0.5363 - val_accuracy: 0.7814\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5375 - accuracy: 0.7795 - val_loss: 0.5361 - val_accuracy: 0.7817\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5371 - accuracy: 0.7798 - val_loss: 0.5357 - val_accuracy: 0.7819\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5368 - accuracy: 0.7801 - val_loss: 0.5353 - val_accuracy: 0.7816\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5364 - accuracy: 0.7801 - val_loss: 0.5349 - val_accuracy: 0.7819\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5361 - accuracy: 0.7801 - val_loss: 0.5346 - val_accuracy: 0.7820\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5357 - accuracy: 0.7803 - val_loss: 0.5346 - val_accuracy: 0.7817\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5353 - accuracy: 0.7804 - val_loss: 0.5343 - val_accuracy: 0.7818\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5350 - accuracy: 0.7805 - val_loss: 0.5336 - val_accuracy: 0.7820\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5347 - accuracy: 0.7805 - val_loss: 0.5333 - val_accuracy: 0.7821\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5343 - accuracy: 0.7806 - val_loss: 0.5330 - val_accuracy: 0.7823\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5340 - accuracy: 0.7808 - val_loss: 0.5327 - val_accuracy: 0.7822\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5337 - accuracy: 0.7809 - val_loss: 0.5323 - val_accuracy: 0.7824\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5334 - accuracy: 0.7810 - val_loss: 0.5321 - val_accuracy: 0.7827\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5331 - accuracy: 0.7809 - val_loss: 0.5319 - val_accuracy: 0.7828\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5328 - accuracy: 0.7812 - val_loss: 0.5316 - val_accuracy: 0.7825\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5325 - accuracy: 0.7812 - val_loss: 0.5312 - val_accuracy: 0.7827\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5321 - accuracy: 0.7814 - val_loss: 0.5310 - val_accuracy: 0.7830\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5318 - accuracy: 0.7815 - val_loss: 0.5308 - val_accuracy: 0.7830\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5316 - accuracy: 0.7815 - val_loss: 0.5306 - val_accuracy: 0.7831\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5313 - accuracy: 0.7816 - val_loss: 0.5302 - val_accuracy: 0.7833\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5310 - accuracy: 0.7816 - val_loss: 0.5303 - val_accuracy: 0.7829\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5307 - accuracy: 0.7817 - val_loss: 0.5298 - val_accuracy: 0.7836\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5305 - accuracy: 0.7817 - val_loss: 0.5295 - val_accuracy: 0.7834\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5302 - accuracy: 0.7819 - val_loss: 0.5293 - val_accuracy: 0.7835\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5299 - accuracy: 0.7818 - val_loss: 0.5290 - val_accuracy: 0.7841\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5297 - accuracy: 0.7821 - val_loss: 0.5287 - val_accuracy: 0.7839\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5294 - accuracy: 0.7820 - val_loss: 0.5285 - val_accuracy: 0.7841\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5291 - accuracy: 0.7820 - val_loss: 0.5284 - val_accuracy: 0.7838\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5289 - accuracy: 0.7822 - val_loss: 0.5286 - val_accuracy: 0.7834\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5287 - accuracy: 0.7825 - val_loss: 0.5279 - val_accuracy: 0.7841\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5284 - accuracy: 0.7822 - val_loss: 0.5277 - val_accuracy: 0.7840\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5282 - accuracy: 0.7825 - val_loss: 0.5276 - val_accuracy: 0.7843\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5280 - accuracy: 0.7827 - val_loss: 0.5273 - val_accuracy: 0.7844\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5277 - accuracy: 0.7827 - val_loss: 0.5271 - val_accuracy: 0.7843\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5275 - accuracy: 0.7827 - val_loss: 0.5269 - val_accuracy: 0.7840\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5272 - accuracy: 0.7828 - val_loss: 0.5268 - val_accuracy: 0.7837\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5270 - accuracy: 0.7827 - val_loss: 0.5266 - val_accuracy: 0.7843\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5268 - accuracy: 0.7828 - val_loss: 0.5265 - val_accuracy: 0.7844\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5266 - accuracy: 0.7830 - val_loss: 0.5262 - val_accuracy: 0.7846\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5264 - accuracy: 0.7831 - val_loss: 0.5261 - val_accuracy: 0.7845\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5262 - accuracy: 0.7832 - val_loss: 0.5258 - val_accuracy: 0.7847\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5260 - accuracy: 0.7832 - val_loss: 0.5256 - val_accuracy: 0.7845\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5258 - accuracy: 0.7832 - val_loss: 0.5254 - val_accuracy: 0.7844\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5255 - accuracy: 0.7831 - val_loss: 0.5253 - val_accuracy: 0.7848\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5254 - accuracy: 0.7834 - val_loss: 0.5253 - val_accuracy: 0.7852\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5252 - accuracy: 0.7832 - val_loss: 0.5252 - val_accuracy: 0.7848\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5250 - accuracy: 0.7834 - val_loss: 0.5248 - val_accuracy: 0.7848\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5248 - accuracy: 0.7834 - val_loss: 0.5249 - val_accuracy: 0.7848\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5246 - accuracy: 0.7836 - val_loss: 0.5246 - val_accuracy: 0.7850\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5244 - accuracy: 0.7837 - val_loss: 0.5243 - val_accuracy: 0.7853\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5242 - accuracy: 0.7835 - val_loss: 0.5242 - val_accuracy: 0.7855\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5241 - accuracy: 0.7837 - val_loss: 0.5241 - val_accuracy: 0.7849\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5239 - accuracy: 0.7836 - val_loss: 0.5239 - val_accuracy: 0.7855\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5237 - accuracy: 0.7836 - val_loss: 0.5238 - val_accuracy: 0.7851\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5235 - accuracy: 0.7837 - val_loss: 0.5237 - val_accuracy: 0.7851\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5233 - accuracy: 0.7838 - val_loss: 0.5235 - val_accuracy: 0.7854\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5232 - accuracy: 0.7838 - val_loss: 0.5234 - val_accuracy: 0.7853\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5230 - accuracy: 0.7839 - val_loss: 0.5235 - val_accuracy: 0.7853\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5228 - accuracy: 0.7839 - val_loss: 0.5231 - val_accuracy: 0.7853\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5227 - accuracy: 0.7842 - val_loss: 0.5229 - val_accuracy: 0.7853\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5225 - accuracy: 0.7842 - val_loss: 0.5230 - val_accuracy: 0.7855\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5223 - accuracy: 0.7841 - val_loss: 0.5227 - val_accuracy: 0.7856\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5222 - accuracy: 0.7841 - val_loss: 0.5226 - val_accuracy: 0.7856\n",
      "2858/2858 [==============================] - 2s 701us/step\n",
      "2858/2858 [==============================] - 3s 882us/step - loss: 0.5263 - accuracy: 0.7826\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.2502 - accuracy: 0.5531 - val_loss: 1.0147 - val_accuracy: 0.6990\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.8718 - accuracy: 0.7251 - val_loss: 0.7512 - val_accuracy: 0.7469\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.6862 - accuracy: 0.7555 - val_loss: 0.6333 - val_accuracy: 0.7661\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.6092 - accuracy: 0.7673 - val_loss: 0.5874 - val_accuracy: 0.7713\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5794 - accuracy: 0.7703 - val_loss: 0.5695 - val_accuracy: 0.7725\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5671 - accuracy: 0.7713 - val_loss: 0.5614 - val_accuracy: 0.7729\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5610 - accuracy: 0.7721 - val_loss: 0.5570 - val_accuracy: 0.7734\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5575 - accuracy: 0.7727 - val_loss: 0.5543 - val_accuracy: 0.7744\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5551 - accuracy: 0.7733 - val_loss: 0.5524 - val_accuracy: 0.7750\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5533 - accuracy: 0.7736 - val_loss: 0.5510 - val_accuracy: 0.7752\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5519 - accuracy: 0.7743 - val_loss: 0.5497 - val_accuracy: 0.7759\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5507 - accuracy: 0.7745 - val_loss: 0.5487 - val_accuracy: 0.7760\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5496 - accuracy: 0.7750 - val_loss: 0.5478 - val_accuracy: 0.7757\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5486 - accuracy: 0.7752 - val_loss: 0.5468 - val_accuracy: 0.7766\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5477 - accuracy: 0.7755 - val_loss: 0.5460 - val_accuracy: 0.7770\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5469 - accuracy: 0.7758 - val_loss: 0.5452 - val_accuracy: 0.7770\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5461 - accuracy: 0.7760 - val_loss: 0.5445 - val_accuracy: 0.7776\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5453 - accuracy: 0.7762 - val_loss: 0.5437 - val_accuracy: 0.7775\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5446 - accuracy: 0.7766 - val_loss: 0.5431 - val_accuracy: 0.7777\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5439 - accuracy: 0.7767 - val_loss: 0.5424 - val_accuracy: 0.7783\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5432 - accuracy: 0.7769 - val_loss: 0.5418 - val_accuracy: 0.7787\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5426 - accuracy: 0.7773 - val_loss: 0.5413 - val_accuracy: 0.7788\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5420 - accuracy: 0.7776 - val_loss: 0.5405 - val_accuracy: 0.7791\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5413 - accuracy: 0.7779 - val_loss: 0.5401 - val_accuracy: 0.7792\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5408 - accuracy: 0.7781 - val_loss: 0.5395 - val_accuracy: 0.7797\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5401 - accuracy: 0.7783 - val_loss: 0.5390 - val_accuracy: 0.7798\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5396 - accuracy: 0.7785 - val_loss: 0.5384 - val_accuracy: 0.7799\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5391 - accuracy: 0.7788 - val_loss: 0.5380 - val_accuracy: 0.7801\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5386 - accuracy: 0.7788 - val_loss: 0.5376 - val_accuracy: 0.7803\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5381 - accuracy: 0.7790 - val_loss: 0.5371 - val_accuracy: 0.7803\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5376 - accuracy: 0.7793 - val_loss: 0.5366 - val_accuracy: 0.7806\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5372 - accuracy: 0.7796 - val_loss: 0.5363 - val_accuracy: 0.7808\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5367 - accuracy: 0.7794 - val_loss: 0.5357 - val_accuracy: 0.7809\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5363 - accuracy: 0.7799 - val_loss: 0.5354 - val_accuracy: 0.7813\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5358 - accuracy: 0.7799 - val_loss: 0.5349 - val_accuracy: 0.7813\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5354 - accuracy: 0.7800 - val_loss: 0.5346 - val_accuracy: 0.7813\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5350 - accuracy: 0.7802 - val_loss: 0.5342 - val_accuracy: 0.7814\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5346 - accuracy: 0.7804 - val_loss: 0.5339 - val_accuracy: 0.7818\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5343 - accuracy: 0.7804 - val_loss: 0.5335 - val_accuracy: 0.7818\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5339 - accuracy: 0.7807 - val_loss: 0.5333 - val_accuracy: 0.7816\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5335 - accuracy: 0.7809 - val_loss: 0.5328 - val_accuracy: 0.7819\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5332 - accuracy: 0.7810 - val_loss: 0.5325 - val_accuracy: 0.7818\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5329 - accuracy: 0.7808 - val_loss: 0.5322 - val_accuracy: 0.7821\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5325 - accuracy: 0.7812 - val_loss: 0.5319 - val_accuracy: 0.7821\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5322 - accuracy: 0.7812 - val_loss: 0.5316 - val_accuracy: 0.7821\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5319 - accuracy: 0.7813 - val_loss: 0.5314 - val_accuracy: 0.7822\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5316 - accuracy: 0.7812 - val_loss: 0.5312 - val_accuracy: 0.7815\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5312 - accuracy: 0.7813 - val_loss: 0.5308 - val_accuracy: 0.7817\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5310 - accuracy: 0.7816 - val_loss: 0.5305 - val_accuracy: 0.7821\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5307 - accuracy: 0.7816 - val_loss: 0.5303 - val_accuracy: 0.7825\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5304 - accuracy: 0.7817 - val_loss: 0.5300 - val_accuracy: 0.7826\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5301 - accuracy: 0.7818 - val_loss: 0.5298 - val_accuracy: 0.7824\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5299 - accuracy: 0.7818 - val_loss: 0.5295 - val_accuracy: 0.7826\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5296 - accuracy: 0.7819 - val_loss: 0.5293 - val_accuracy: 0.7825\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5293 - accuracy: 0.7819 - val_loss: 0.5291 - val_accuracy: 0.7830\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5290 - accuracy: 0.7821 - val_loss: 0.5288 - val_accuracy: 0.7831\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5288 - accuracy: 0.7820 - val_loss: 0.5287 - val_accuracy: 0.7831\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5286 - accuracy: 0.7822 - val_loss: 0.5284 - val_accuracy: 0.7832\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5283 - accuracy: 0.7824 - val_loss: 0.5282 - val_accuracy: 0.7830\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5280 - accuracy: 0.7823 - val_loss: 0.5280 - val_accuracy: 0.7833\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5279 - accuracy: 0.7824 - val_loss: 0.5278 - val_accuracy: 0.7835\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5276 - accuracy: 0.7823 - val_loss: 0.5276 - val_accuracy: 0.7833\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5273 - accuracy: 0.7824 - val_loss: 0.5273 - val_accuracy: 0.7839\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5272 - accuracy: 0.7825 - val_loss: 0.5272 - val_accuracy: 0.7834\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5269 - accuracy: 0.7825 - val_loss: 0.5269 - val_accuracy: 0.7837\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5267 - accuracy: 0.7827 - val_loss: 0.5268 - val_accuracy: 0.7836\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5265 - accuracy: 0.7827 - val_loss: 0.5266 - val_accuracy: 0.7837\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5263 - accuracy: 0.7827 - val_loss: 0.5265 - val_accuracy: 0.7839\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5261 - accuracy: 0.7829 - val_loss: 0.5262 - val_accuracy: 0.7840\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5259 - accuracy: 0.7828 - val_loss: 0.5261 - val_accuracy: 0.7841\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5257 - accuracy: 0.7830 - val_loss: 0.5259 - val_accuracy: 0.7839\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5255 - accuracy: 0.7829 - val_loss: 0.5257 - val_accuracy: 0.7838\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5253 - accuracy: 0.7830 - val_loss: 0.5256 - val_accuracy: 0.7840\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5251 - accuracy: 0.7830 - val_loss: 0.5254 - val_accuracy: 0.7841\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5249 - accuracy: 0.7831 - val_loss: 0.5253 - val_accuracy: 0.7843\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5247 - accuracy: 0.7830 - val_loss: 0.5251 - val_accuracy: 0.7842\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5245 - accuracy: 0.7832 - val_loss: 0.5251 - val_accuracy: 0.7840\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5243 - accuracy: 0.7831 - val_loss: 0.5248 - val_accuracy: 0.7844\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5242 - accuracy: 0.7831 - val_loss: 0.5248 - val_accuracy: 0.7841\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5240 - accuracy: 0.7833 - val_loss: 0.5248 - val_accuracy: 0.7843\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5238 - accuracy: 0.7833 - val_loss: 0.5244 - val_accuracy: 0.7849\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5237 - accuracy: 0.7835 - val_loss: 0.5243 - val_accuracy: 0.7847\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5235 - accuracy: 0.7833 - val_loss: 0.5241 - val_accuracy: 0.7844\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5233 - accuracy: 0.7834 - val_loss: 0.5241 - val_accuracy: 0.7845\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5231 - accuracy: 0.7835 - val_loss: 0.5239 - val_accuracy: 0.7845\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5230 - accuracy: 0.7836 - val_loss: 0.5238 - val_accuracy: 0.7847\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5228 - accuracy: 0.7836 - val_loss: 0.5236 - val_accuracy: 0.7848\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5227 - accuracy: 0.7836 - val_loss: 0.5234 - val_accuracy: 0.7848\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5225 - accuracy: 0.7837 - val_loss: 0.5233 - val_accuracy: 0.7848\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5223 - accuracy: 0.7837 - val_loss: 0.5232 - val_accuracy: 0.7848\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5222 - accuracy: 0.7839 - val_loss: 0.5232 - val_accuracy: 0.7847\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5221 - accuracy: 0.7838 - val_loss: 0.5230 - val_accuracy: 0.7842\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5219 - accuracy: 0.7839 - val_loss: 0.5229 - val_accuracy: 0.7848\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5218 - accuracy: 0.7839 - val_loss: 0.5229 - val_accuracy: 0.7852\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5216 - accuracy: 0.7838 - val_loss: 0.5227 - val_accuracy: 0.7848\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5215 - accuracy: 0.7838 - val_loss: 0.5225 - val_accuracy: 0.7847\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5213 - accuracy: 0.7839 - val_loss: 0.5224 - val_accuracy: 0.7849\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5212 - accuracy: 0.7839 - val_loss: 0.5223 - val_accuracy: 0.7849\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5210 - accuracy: 0.7840 - val_loss: 0.5222 - val_accuracy: 0.7848\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5209 - accuracy: 0.7840 - val_loss: 0.5221 - val_accuracy: 0.7847\n",
      "2858/2858 [==============================] - 2s 786us/step\n",
      "2858/2858 [==============================] - 3s 943us/step - loss: 0.5244 - accuracy: 0.7821\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.3633 - accuracy: 0.4522 - val_loss: 1.1182 - val_accuracy: 0.6641\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.9832 - accuracy: 0.7004 - val_loss: 0.8573 - val_accuracy: 0.7263\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.7718 - accuracy: 0.7366 - val_loss: 0.6982 - val_accuracy: 0.7481\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.6564 - accuracy: 0.7525 - val_loss: 0.6188 - val_accuracy: 0.7634\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.6011 - accuracy: 0.7656 - val_loss: 0.5837 - val_accuracy: 0.7700\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5776 - accuracy: 0.7692 - val_loss: 0.5689 - val_accuracy: 0.7713\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5673 - accuracy: 0.7702 - val_loss: 0.5620 - val_accuracy: 0.7717\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5621 - accuracy: 0.7706 - val_loss: 0.5583 - val_accuracy: 0.7721\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5591 - accuracy: 0.7711 - val_loss: 0.5559 - val_accuracy: 0.7722\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5571 - accuracy: 0.7716 - val_loss: 0.5543 - val_accuracy: 0.7729\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5556 - accuracy: 0.7721 - val_loss: 0.5530 - val_accuracy: 0.7734\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7726 - val_loss: 0.5520 - val_accuracy: 0.7739\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5534 - accuracy: 0.7729 - val_loss: 0.5512 - val_accuracy: 0.7751\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5526 - accuracy: 0.7734 - val_loss: 0.5503 - val_accuracy: 0.7752\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5518 - accuracy: 0.7736 - val_loss: 0.5498 - val_accuracy: 0.7760\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5511 - accuracy: 0.7740 - val_loss: 0.5491 - val_accuracy: 0.7762\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5505 - accuracy: 0.7743 - val_loss: 0.5487 - val_accuracy: 0.7766\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5499 - accuracy: 0.7745 - val_loss: 0.5480 - val_accuracy: 0.7770\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5494 - accuracy: 0.7748 - val_loss: 0.5473 - val_accuracy: 0.7772\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5488 - accuracy: 0.7753 - val_loss: 0.5469 - val_accuracy: 0.7770\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5483 - accuracy: 0.7755 - val_loss: 0.5464 - val_accuracy: 0.7776\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5478 - accuracy: 0.7758 - val_loss: 0.5459 - val_accuracy: 0.7778\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5473 - accuracy: 0.7762 - val_loss: 0.5455 - val_accuracy: 0.7779\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5468 - accuracy: 0.7762 - val_loss: 0.5450 - val_accuracy: 0.7781\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5464 - accuracy: 0.7764 - val_loss: 0.5446 - val_accuracy: 0.7783\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5459 - accuracy: 0.7768 - val_loss: 0.5442 - val_accuracy: 0.7785\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5455 - accuracy: 0.7768 - val_loss: 0.5437 - val_accuracy: 0.7784\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5450 - accuracy: 0.7769 - val_loss: 0.5434 - val_accuracy: 0.7785\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5446 - accuracy: 0.7772 - val_loss: 0.5428 - val_accuracy: 0.7787\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5441 - accuracy: 0.7774 - val_loss: 0.5425 - val_accuracy: 0.7787\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5437 - accuracy: 0.7776 - val_loss: 0.5421 - val_accuracy: 0.7792\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5433 - accuracy: 0.7777 - val_loss: 0.5417 - val_accuracy: 0.7792\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5429 - accuracy: 0.7776 - val_loss: 0.5413 - val_accuracy: 0.7794\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5425 - accuracy: 0.7780 - val_loss: 0.5410 - val_accuracy: 0.7793\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5421 - accuracy: 0.7780 - val_loss: 0.5405 - val_accuracy: 0.7796\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5417 - accuracy: 0.7782 - val_loss: 0.5402 - val_accuracy: 0.7799\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5413 - accuracy: 0.7784 - val_loss: 0.5398 - val_accuracy: 0.7799\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5410 - accuracy: 0.7784 - val_loss: 0.5394 - val_accuracy: 0.7801\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5406 - accuracy: 0.7788 - val_loss: 0.5391 - val_accuracy: 0.7803\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5402 - accuracy: 0.7787 - val_loss: 0.5388 - val_accuracy: 0.7804\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5399 - accuracy: 0.7789 - val_loss: 0.5385 - val_accuracy: 0.7805\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5395 - accuracy: 0.7788 - val_loss: 0.5381 - val_accuracy: 0.7808\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5391 - accuracy: 0.7790 - val_loss: 0.5377 - val_accuracy: 0.7806\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5388 - accuracy: 0.7793 - val_loss: 0.5376 - val_accuracy: 0.7809\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5385 - accuracy: 0.7794 - val_loss: 0.5372 - val_accuracy: 0.7804\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5381 - accuracy: 0.7794 - val_loss: 0.5367 - val_accuracy: 0.7809\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5378 - accuracy: 0.7795 - val_loss: 0.5366 - val_accuracy: 0.7810\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5375 - accuracy: 0.7796 - val_loss: 0.5362 - val_accuracy: 0.7813\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5371 - accuracy: 0.7797 - val_loss: 0.5359 - val_accuracy: 0.7816\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5368 - accuracy: 0.7798 - val_loss: 0.5356 - val_accuracy: 0.7815\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5365 - accuracy: 0.7800 - val_loss: 0.5353 - val_accuracy: 0.7815\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5362 - accuracy: 0.7802 - val_loss: 0.5350 - val_accuracy: 0.7815\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5359 - accuracy: 0.7801 - val_loss: 0.5347 - val_accuracy: 0.7817\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5356 - accuracy: 0.7801 - val_loss: 0.5344 - val_accuracy: 0.7820\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5353 - accuracy: 0.7805 - val_loss: 0.5344 - val_accuracy: 0.7816\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5350 - accuracy: 0.7804 - val_loss: 0.5339 - val_accuracy: 0.7824\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5347 - accuracy: 0.7807 - val_loss: 0.5337 - val_accuracy: 0.7823\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5344 - accuracy: 0.7805 - val_loss: 0.5335 - val_accuracy: 0.7825\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5341 - accuracy: 0.7807 - val_loss: 0.5332 - val_accuracy: 0.7823\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5339 - accuracy: 0.7808 - val_loss: 0.5329 - val_accuracy: 0.7825\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5336 - accuracy: 0.7809 - val_loss: 0.5327 - val_accuracy: 0.7826\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5333 - accuracy: 0.7811 - val_loss: 0.5324 - val_accuracy: 0.7828\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5331 - accuracy: 0.7809 - val_loss: 0.5322 - val_accuracy: 0.7828\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5328 - accuracy: 0.7811 - val_loss: 0.5320 - val_accuracy: 0.7828\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5325 - accuracy: 0.7813 - val_loss: 0.5318 - val_accuracy: 0.7828\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5323 - accuracy: 0.7815 - val_loss: 0.5316 - val_accuracy: 0.7831\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5320 - accuracy: 0.7816 - val_loss: 0.5313 - val_accuracy: 0.7828\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5318 - accuracy: 0.7817 - val_loss: 0.5311 - val_accuracy: 0.7831\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5316 - accuracy: 0.7817 - val_loss: 0.5309 - val_accuracy: 0.7833\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5313 - accuracy: 0.7819 - val_loss: 0.5307 - val_accuracy: 0.7832\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5311 - accuracy: 0.7818 - val_loss: 0.5304 - val_accuracy: 0.7835\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5308 - accuracy: 0.7820 - val_loss: 0.5304 - val_accuracy: 0.7834\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5306 - accuracy: 0.7819 - val_loss: 0.5302 - val_accuracy: 0.7834\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5304 - accuracy: 0.7820 - val_loss: 0.5298 - val_accuracy: 0.7837\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5302 - accuracy: 0.7821 - val_loss: 0.5296 - val_accuracy: 0.7837\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5299 - accuracy: 0.7823 - val_loss: 0.5295 - val_accuracy: 0.7836\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5297 - accuracy: 0.7822 - val_loss: 0.5294 - val_accuracy: 0.7841\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5295 - accuracy: 0.7824 - val_loss: 0.5290 - val_accuracy: 0.7841\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5293 - accuracy: 0.7824 - val_loss: 0.5289 - val_accuracy: 0.7840\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5291 - accuracy: 0.7825 - val_loss: 0.5287 - val_accuracy: 0.7843\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5289 - accuracy: 0.7825 - val_loss: 0.5286 - val_accuracy: 0.7844\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5287 - accuracy: 0.7828 - val_loss: 0.5283 - val_accuracy: 0.7843\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5285 - accuracy: 0.7826 - val_loss: 0.5282 - val_accuracy: 0.7844\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5283 - accuracy: 0.7825 - val_loss: 0.5280 - val_accuracy: 0.7844\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5281 - accuracy: 0.7827 - val_loss: 0.5278 - val_accuracy: 0.7844\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5279 - accuracy: 0.7827 - val_loss: 0.5278 - val_accuracy: 0.7839\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5277 - accuracy: 0.7828 - val_loss: 0.5277 - val_accuracy: 0.7846\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5275 - accuracy: 0.7830 - val_loss: 0.5276 - val_accuracy: 0.7844\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5273 - accuracy: 0.7829 - val_loss: 0.5272 - val_accuracy: 0.7846\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5271 - accuracy: 0.7829 - val_loss: 0.5273 - val_accuracy: 0.7846\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5270 - accuracy: 0.7829 - val_loss: 0.5270 - val_accuracy: 0.7850\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5268 - accuracy: 0.7830 - val_loss: 0.5268 - val_accuracy: 0.7849\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5266 - accuracy: 0.7830 - val_loss: 0.5266 - val_accuracy: 0.7850\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5264 - accuracy: 0.7833 - val_loss: 0.5264 - val_accuracy: 0.7850\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5262 - accuracy: 0.7832 - val_loss: 0.5263 - val_accuracy: 0.7849\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5261 - accuracy: 0.7831 - val_loss: 0.5261 - val_accuracy: 0.7848\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5259 - accuracy: 0.7832 - val_loss: 0.5260 - val_accuracy: 0.7849\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5257 - accuracy: 0.7832 - val_loss: 0.5258 - val_accuracy: 0.7850\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5256 - accuracy: 0.7832 - val_loss: 0.5257 - val_accuracy: 0.7853\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5254 - accuracy: 0.7832 - val_loss: 0.5256 - val_accuracy: 0.7852\n",
      "2858/2858 [==============================] - 2s 815us/step\n",
      "2858/2858 [==============================] - 3s 996us/step - loss: 0.5290 - accuracy: 0.7818\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.1998 - accuracy: 0.5860 - val_loss: 1.0097 - val_accuracy: 0.6955\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.8807 - accuracy: 0.7216 - val_loss: 0.7729 - val_accuracy: 0.7399\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7079 - accuracy: 0.7490 - val_loss: 0.6541 - val_accuracy: 0.7607\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6260 - accuracy: 0.7652 - val_loss: 0.6013 - val_accuracy: 0.7711\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5901 - accuracy: 0.7701 - val_loss: 0.5781 - val_accuracy: 0.7723\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5738 - accuracy: 0.7714 - val_loss: 0.5674 - val_accuracy: 0.7724\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5658 - accuracy: 0.7721 - val_loss: 0.5618 - val_accuracy: 0.7735\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5613 - accuracy: 0.7725 - val_loss: 0.5583 - val_accuracy: 0.7741\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5585 - accuracy: 0.7732 - val_loss: 0.5560 - val_accuracy: 0.7747\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.7736 - val_loss: 0.5545 - val_accuracy: 0.7753\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7740 - val_loss: 0.5531 - val_accuracy: 0.7757\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5538 - accuracy: 0.7743 - val_loss: 0.5521 - val_accuracy: 0.7756\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5528 - accuracy: 0.7746 - val_loss: 0.5513 - val_accuracy: 0.7761\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5519 - accuracy: 0.7749 - val_loss: 0.5504 - val_accuracy: 0.7762\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5512 - accuracy: 0.7750 - val_loss: 0.5497 - val_accuracy: 0.7766\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5505 - accuracy: 0.7753 - val_loss: 0.5491 - val_accuracy: 0.7769\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5498 - accuracy: 0.7755 - val_loss: 0.5485 - val_accuracy: 0.7774\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5492 - accuracy: 0.7757 - val_loss: 0.5479 - val_accuracy: 0.7772\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5487 - accuracy: 0.7761 - val_loss: 0.5474 - val_accuracy: 0.7777\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5482 - accuracy: 0.7761 - val_loss: 0.5469 - val_accuracy: 0.7779\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5476 - accuracy: 0.7764 - val_loss: 0.5463 - val_accuracy: 0.7783\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5472 - accuracy: 0.7765 - val_loss: 0.5458 - val_accuracy: 0.7785\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5467 - accuracy: 0.7766 - val_loss: 0.5454 - val_accuracy: 0.7783\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5462 - accuracy: 0.7768 - val_loss: 0.5449 - val_accuracy: 0.7786\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5457 - accuracy: 0.7770 - val_loss: 0.5446 - val_accuracy: 0.7788\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5453 - accuracy: 0.7773 - val_loss: 0.5440 - val_accuracy: 0.7789\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5449 - accuracy: 0.7774 - val_loss: 0.5436 - val_accuracy: 0.7792\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5445 - accuracy: 0.7773 - val_loss: 0.5434 - val_accuracy: 0.7794\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5441 - accuracy: 0.7776 - val_loss: 0.5428 - val_accuracy: 0.7795\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5437 - accuracy: 0.7777 - val_loss: 0.5426 - val_accuracy: 0.7794\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5433 - accuracy: 0.7781 - val_loss: 0.5422 - val_accuracy: 0.7793\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5429 - accuracy: 0.7781 - val_loss: 0.5418 - val_accuracy: 0.7797\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5425 - accuracy: 0.7781 - val_loss: 0.5414 - val_accuracy: 0.7796\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5421 - accuracy: 0.7783 - val_loss: 0.5412 - val_accuracy: 0.7799\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5417 - accuracy: 0.7785 - val_loss: 0.5407 - val_accuracy: 0.7799\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5414 - accuracy: 0.7786 - val_loss: 0.5403 - val_accuracy: 0.7801\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5410 - accuracy: 0.7787 - val_loss: 0.5401 - val_accuracy: 0.7802\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5407 - accuracy: 0.7787 - val_loss: 0.5398 - val_accuracy: 0.7805\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5403 - accuracy: 0.7788 - val_loss: 0.5393 - val_accuracy: 0.7802\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5400 - accuracy: 0.7791 - val_loss: 0.5390 - val_accuracy: 0.7803\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5397 - accuracy: 0.7791 - val_loss: 0.5387 - val_accuracy: 0.7806\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5393 - accuracy: 0.7793 - val_loss: 0.5383 - val_accuracy: 0.7807\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5390 - accuracy: 0.7795 - val_loss: 0.5381 - val_accuracy: 0.7810\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5387 - accuracy: 0.7795 - val_loss: 0.5382 - val_accuracy: 0.7806\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5384 - accuracy: 0.7798 - val_loss: 0.5374 - val_accuracy: 0.7811\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5380 - accuracy: 0.7798 - val_loss: 0.5372 - val_accuracy: 0.7810\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5377 - accuracy: 0.7799 - val_loss: 0.5369 - val_accuracy: 0.7814\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5374 - accuracy: 0.7800 - val_loss: 0.5365 - val_accuracy: 0.7813\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5371 - accuracy: 0.7800 - val_loss: 0.5362 - val_accuracy: 0.7813\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5368 - accuracy: 0.7803 - val_loss: 0.5360 - val_accuracy: 0.7814\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5365 - accuracy: 0.7805 - val_loss: 0.5358 - val_accuracy: 0.7813\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5362 - accuracy: 0.7804 - val_loss: 0.5355 - val_accuracy: 0.7819\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5359 - accuracy: 0.7806 - val_loss: 0.5351 - val_accuracy: 0.7820\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5356 - accuracy: 0.7807 - val_loss: 0.5349 - val_accuracy: 0.7820\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5353 - accuracy: 0.7807 - val_loss: 0.5346 - val_accuracy: 0.7820\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5350 - accuracy: 0.7809 - val_loss: 0.5345 - val_accuracy: 0.7819\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5348 - accuracy: 0.7811 - val_loss: 0.5341 - val_accuracy: 0.7820\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5345 - accuracy: 0.7811 - val_loss: 0.5340 - val_accuracy: 0.7823\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5342 - accuracy: 0.7811 - val_loss: 0.5336 - val_accuracy: 0.7825\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5340 - accuracy: 0.7813 - val_loss: 0.5334 - val_accuracy: 0.7824\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5337 - accuracy: 0.7814 - val_loss: 0.5331 - val_accuracy: 0.7826\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5334 - accuracy: 0.7813 - val_loss: 0.5328 - val_accuracy: 0.7830\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5332 - accuracy: 0.7814 - val_loss: 0.5326 - val_accuracy: 0.7823\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5329 - accuracy: 0.7814 - val_loss: 0.5326 - val_accuracy: 0.7820\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5327 - accuracy: 0.7816 - val_loss: 0.5322 - val_accuracy: 0.7828\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5324 - accuracy: 0.7815 - val_loss: 0.5319 - val_accuracy: 0.7830\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5322 - accuracy: 0.7817 - val_loss: 0.5317 - val_accuracy: 0.7827\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5319 - accuracy: 0.7818 - val_loss: 0.5315 - val_accuracy: 0.7829\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5317 - accuracy: 0.7819 - val_loss: 0.5313 - val_accuracy: 0.7831\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5315 - accuracy: 0.7819 - val_loss: 0.5311 - val_accuracy: 0.7829\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5312 - accuracy: 0.7819 - val_loss: 0.5308 - val_accuracy: 0.7831\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5310 - accuracy: 0.7821 - val_loss: 0.5308 - val_accuracy: 0.7826\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5308 - accuracy: 0.7818 - val_loss: 0.5305 - val_accuracy: 0.7830\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5306 - accuracy: 0.7821 - val_loss: 0.5305 - val_accuracy: 0.7829\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5303 - accuracy: 0.7822 - val_loss: 0.5301 - val_accuracy: 0.7830\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5302 - accuracy: 0.7821 - val_loss: 0.5298 - val_accuracy: 0.7832\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5299 - accuracy: 0.7822 - val_loss: 0.5296 - val_accuracy: 0.7831\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5297 - accuracy: 0.7822 - val_loss: 0.5296 - val_accuracy: 0.7833\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5295 - accuracy: 0.7823 - val_loss: 0.5295 - val_accuracy: 0.7831\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5293 - accuracy: 0.7826 - val_loss: 0.5290 - val_accuracy: 0.7833\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5291 - accuracy: 0.7825 - val_loss: 0.5289 - val_accuracy: 0.7835\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5289 - accuracy: 0.7825 - val_loss: 0.5288 - val_accuracy: 0.7833\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5287 - accuracy: 0.7828 - val_loss: 0.5285 - val_accuracy: 0.7837\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5285 - accuracy: 0.7827 - val_loss: 0.5284 - val_accuracy: 0.7835\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5283 - accuracy: 0.7829 - val_loss: 0.5283 - val_accuracy: 0.7835\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5281 - accuracy: 0.7829 - val_loss: 0.5281 - val_accuracy: 0.7835\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5279 - accuracy: 0.7831 - val_loss: 0.5280 - val_accuracy: 0.7836\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5278 - accuracy: 0.7831 - val_loss: 0.5277 - val_accuracy: 0.7838\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5276 - accuracy: 0.7831 - val_loss: 0.5276 - val_accuracy: 0.7840\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5274 - accuracy: 0.7832 - val_loss: 0.5274 - val_accuracy: 0.7840\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5272 - accuracy: 0.7832 - val_loss: 0.5272 - val_accuracy: 0.7839\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5271 - accuracy: 0.7832 - val_loss: 0.5271 - val_accuracy: 0.7837\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5269 - accuracy: 0.7833 - val_loss: 0.5270 - val_accuracy: 0.7837\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5267 - accuracy: 0.7834 - val_loss: 0.5268 - val_accuracy: 0.7841\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5266 - accuracy: 0.7834 - val_loss: 0.5267 - val_accuracy: 0.7842\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5264 - accuracy: 0.7834 - val_loss: 0.5265 - val_accuracy: 0.7841\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5262 - accuracy: 0.7835 - val_loss: 0.5265 - val_accuracy: 0.7841\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5261 - accuracy: 0.7835 - val_loss: 0.5262 - val_accuracy: 0.7841\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5259 - accuracy: 0.7836 - val_loss: 0.5261 - val_accuracy: 0.7843\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5257 - accuracy: 0.7839 - val_loss: 0.5260 - val_accuracy: 0.7844\n",
      "2858/2858 [==============================] - 3s 882us/step\n",
      "2858/2858 [==============================] - 3s 989us/step - loss: 0.5296 - accuracy: 0.7813\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 1.4736 - accuracy: 0.3539 - val_loss: 1.3179 - val_accuracy: 0.4161\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.2917 - accuracy: 0.4525 - val_loss: 1.2546 - val_accuracy: 0.5657\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.2075 - accuracy: 0.6340 - val_loss: 1.1473 - val_accuracy: 0.6726\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.0866 - accuracy: 0.6885 - val_loss: 1.0205 - val_accuracy: 0.6986\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.9659 - accuracy: 0.7060 - val_loss: 0.9105 - val_accuracy: 0.7142\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.8684 - accuracy: 0.7199 - val_loss: 0.8249 - val_accuracy: 0.7281\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7923 - accuracy: 0.7323 - val_loss: 0.7578 - val_accuracy: 0.7390\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7329 - accuracy: 0.7410 - val_loss: 0.7055 - val_accuracy: 0.7458\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6869 - accuracy: 0.7455 - val_loss: 0.6657 - val_accuracy: 0.7486\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6526 - accuracy: 0.7482 - val_loss: 0.6367 - val_accuracy: 0.7516\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6280 - accuracy: 0.7532 - val_loss: 0.6162 - val_accuracy: 0.7577\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6105 - accuracy: 0.7603 - val_loss: 0.6017 - val_accuracy: 0.7642\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5982 - accuracy: 0.7651 - val_loss: 0.5915 - val_accuracy: 0.7674\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5894 - accuracy: 0.7674 - val_loss: 0.5842 - val_accuracy: 0.7690\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5830 - accuracy: 0.7687 - val_loss: 0.5788 - val_accuracy: 0.7700\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5783 - accuracy: 0.7692 - val_loss: 0.5748 - val_accuracy: 0.7702\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5747 - accuracy: 0.7698 - val_loss: 0.5717 - val_accuracy: 0.7707\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5719 - accuracy: 0.7699 - val_loss: 0.5693 - val_accuracy: 0.7710\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5697 - accuracy: 0.7698 - val_loss: 0.5674 - val_accuracy: 0.7713\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5680 - accuracy: 0.7700 - val_loss: 0.5658 - val_accuracy: 0.7714\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5666 - accuracy: 0.7702 - val_loss: 0.5646 - val_accuracy: 0.7716\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5654 - accuracy: 0.7705 - val_loss: 0.5635 - val_accuracy: 0.7717\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5645 - accuracy: 0.7707 - val_loss: 0.5627 - val_accuracy: 0.7718\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5636 - accuracy: 0.7709 - val_loss: 0.5621 - val_accuracy: 0.7718\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5630 - accuracy: 0.7711 - val_loss: 0.5613 - val_accuracy: 0.7720\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5624 - accuracy: 0.7712 - val_loss: 0.5607 - val_accuracy: 0.7721\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5619 - accuracy: 0.7713 - val_loss: 0.5603 - val_accuracy: 0.7721\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5614 - accuracy: 0.7715 - val_loss: 0.5599 - val_accuracy: 0.7723\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5610 - accuracy: 0.7716 - val_loss: 0.5595 - val_accuracy: 0.7720\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5606 - accuracy: 0.7717 - val_loss: 0.5592 - val_accuracy: 0.7721\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5603 - accuracy: 0.7718 - val_loss: 0.5588 - val_accuracy: 0.7721\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5600 - accuracy: 0.7719 - val_loss: 0.5585 - val_accuracy: 0.7722\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5597 - accuracy: 0.7718 - val_loss: 0.5582 - val_accuracy: 0.7721\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5594 - accuracy: 0.7719 - val_loss: 0.5580 - val_accuracy: 0.7719\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5592 - accuracy: 0.7720 - val_loss: 0.5577 - val_accuracy: 0.7723\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5589 - accuracy: 0.7720 - val_loss: 0.5575 - val_accuracy: 0.7723\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5587 - accuracy: 0.7720 - val_loss: 0.5573 - val_accuracy: 0.7725\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5585 - accuracy: 0.7721 - val_loss: 0.5571 - val_accuracy: 0.7725\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5583 - accuracy: 0.7722 - val_loss: 0.5569 - val_accuracy: 0.7726\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5581 - accuracy: 0.7722 - val_loss: 0.5567 - val_accuracy: 0.7727\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5580 - accuracy: 0.7723 - val_loss: 0.5565 - val_accuracy: 0.7726\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5578 - accuracy: 0.7723 - val_loss: 0.5564 - val_accuracy: 0.7728\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5576 - accuracy: 0.7723 - val_loss: 0.5562 - val_accuracy: 0.7727\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5575 - accuracy: 0.7725 - val_loss: 0.5560 - val_accuracy: 0.7729\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5573 - accuracy: 0.7725 - val_loss: 0.5559 - val_accuracy: 0.7727\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5572 - accuracy: 0.7726 - val_loss: 0.5557 - val_accuracy: 0.7728\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5570 - accuracy: 0.7725 - val_loss: 0.5556 - val_accuracy: 0.7733\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5569 - accuracy: 0.7726 - val_loss: 0.5555 - val_accuracy: 0.7735\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5568 - accuracy: 0.7727 - val_loss: 0.5553 - val_accuracy: 0.7731\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5566 - accuracy: 0.7727 - val_loss: 0.5552 - val_accuracy: 0.7732\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.7728 - val_loss: 0.5551 - val_accuracy: 0.7735\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5564 - accuracy: 0.7728 - val_loss: 0.5550 - val_accuracy: 0.7734\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5563 - accuracy: 0.7730 - val_loss: 0.5548 - val_accuracy: 0.7735\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5562 - accuracy: 0.7729 - val_loss: 0.5547 - val_accuracy: 0.7735\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5560 - accuracy: 0.7729 - val_loss: 0.5546 - val_accuracy: 0.7739\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5559 - accuracy: 0.7729 - val_loss: 0.5545 - val_accuracy: 0.7739\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5558 - accuracy: 0.7729 - val_loss: 0.5544 - val_accuracy: 0.7738\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5557 - accuracy: 0.7731 - val_loss: 0.5543 - val_accuracy: 0.7739\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5556 - accuracy: 0.7732 - val_loss: 0.5542 - val_accuracy: 0.7737\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5555 - accuracy: 0.7731 - val_loss: 0.5541 - val_accuracy: 0.7743\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5554 - accuracy: 0.7732 - val_loss: 0.5540 - val_accuracy: 0.7741\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5554 - accuracy: 0.7733 - val_loss: 0.5539 - val_accuracy: 0.7738\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5553 - accuracy: 0.7731 - val_loss: 0.5539 - val_accuracy: 0.7743\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5552 - accuracy: 0.7733 - val_loss: 0.5537 - val_accuracy: 0.7741\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5551 - accuracy: 0.7735 - val_loss: 0.5536 - val_accuracy: 0.7742\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7733 - val_loss: 0.5536 - val_accuracy: 0.7742\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7734 - val_loss: 0.5536 - val_accuracy: 0.7743\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7735 - val_loss: 0.5534 - val_accuracy: 0.7743\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5548 - accuracy: 0.7735 - val_loss: 0.5534 - val_accuracy: 0.7743\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5547 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7741\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5546 - accuracy: 0.7736 - val_loss: 0.5532 - val_accuracy: 0.7745\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5546 - accuracy: 0.7735 - val_loss: 0.5531 - val_accuracy: 0.7744\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7736 - val_loss: 0.5530 - val_accuracy: 0.7744\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7736 - val_loss: 0.5529 - val_accuracy: 0.7743\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7735 - val_loss: 0.5529 - val_accuracy: 0.7746\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5543 - accuracy: 0.7736 - val_loss: 0.5528 - val_accuracy: 0.7744\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7736 - val_loss: 0.5528 - val_accuracy: 0.7744\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7738 - val_loss: 0.5527 - val_accuracy: 0.7745\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5541 - accuracy: 0.7738 - val_loss: 0.5526 - val_accuracy: 0.7744\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5540 - accuracy: 0.7737 - val_loss: 0.5526 - val_accuracy: 0.7743\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5540 - accuracy: 0.7736 - val_loss: 0.5525 - val_accuracy: 0.7744\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5539 - accuracy: 0.7737 - val_loss: 0.5524 - val_accuracy: 0.7744\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5539 - accuracy: 0.7739 - val_loss: 0.5524 - val_accuracy: 0.7746\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5538 - accuracy: 0.7738 - val_loss: 0.5523 - val_accuracy: 0.7745\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5537 - accuracy: 0.7738 - val_loss: 0.5523 - val_accuracy: 0.7744\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5537 - accuracy: 0.7738 - val_loss: 0.5522 - val_accuracy: 0.7746\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5536 - accuracy: 0.7738 - val_loss: 0.5522 - val_accuracy: 0.7744\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5536 - accuracy: 0.7739 - val_loss: 0.5523 - val_accuracy: 0.7752\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5535 - accuracy: 0.7738 - val_loss: 0.5521 - val_accuracy: 0.7744\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5535 - accuracy: 0.7738 - val_loss: 0.5520 - val_accuracy: 0.7748\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5534 - accuracy: 0.7739 - val_loss: 0.5519 - val_accuracy: 0.7746\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5533 - accuracy: 0.7740 - val_loss: 0.5519 - val_accuracy: 0.7745\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5533 - accuracy: 0.7738 - val_loss: 0.5518 - val_accuracy: 0.7748\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5533 - accuracy: 0.7740 - val_loss: 0.5518 - val_accuracy: 0.7748\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5532 - accuracy: 0.7740 - val_loss: 0.5517 - val_accuracy: 0.7746\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5531 - accuracy: 0.7739 - val_loss: 0.5517 - val_accuracy: 0.7749\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5531 - accuracy: 0.7740 - val_loss: 0.5517 - val_accuracy: 0.7748\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5531 - accuracy: 0.7741 - val_loss: 0.5516 - val_accuracy: 0.7747\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5530 - accuracy: 0.7741 - val_loss: 0.5515 - val_accuracy: 0.7747\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5530 - accuracy: 0.7741 - val_loss: 0.5515 - val_accuracy: 0.7748\n",
      "2858/2858 [==============================] - 2s 830us/step\n",
      "2858/2858 [==============================] - 3s 984us/step - loss: 0.5541 - accuracy: 0.7734\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 1.3117 - accuracy: 0.4495 - val_loss: 1.2754 - val_accuracy: 0.5196\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.2393 - accuracy: 0.5878 - val_loss: 1.1915 - val_accuracy: 0.6409\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.1409 - accuracy: 0.6617 - val_loss: 1.0830 - val_accuracy: 0.6785\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.0319 - accuracy: 0.6878 - val_loss: 0.9785 - val_accuracy: 0.6960\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.9352 - accuracy: 0.7045 - val_loss: 0.8907 - val_accuracy: 0.7122\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.8554 - accuracy: 0.7187 - val_loss: 0.8188 - val_accuracy: 0.7261\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7905 - accuracy: 0.7303 - val_loss: 0.7602 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7375 - accuracy: 0.7389 - val_loss: 0.7126 - val_accuracy: 0.7446\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6952 - accuracy: 0.7454 - val_loss: 0.6752 - val_accuracy: 0.7496\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6625 - accuracy: 0.7503 - val_loss: 0.6470 - val_accuracy: 0.7535\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6381 - accuracy: 0.7535 - val_loss: 0.6261 - val_accuracy: 0.7565\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6201 - accuracy: 0.7563 - val_loss: 0.6109 - val_accuracy: 0.7592\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6068 - accuracy: 0.7603 - val_loss: 0.5996 - val_accuracy: 0.7649\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5968 - accuracy: 0.7651 - val_loss: 0.5909 - val_accuracy: 0.7675\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5892 - accuracy: 0.7678 - val_loss: 0.5844 - val_accuracy: 0.7694\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5834 - accuracy: 0.7689 - val_loss: 0.5793 - val_accuracy: 0.7705\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5788 - accuracy: 0.7695 - val_loss: 0.5752 - val_accuracy: 0.7705\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5752 - accuracy: 0.7699 - val_loss: 0.5721 - val_accuracy: 0.7711\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5723 - accuracy: 0.7702 - val_loss: 0.5696 - val_accuracy: 0.7717\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5700 - accuracy: 0.7704 - val_loss: 0.5676 - val_accuracy: 0.7717\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5682 - accuracy: 0.7706 - val_loss: 0.5659 - val_accuracy: 0.7718\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5667 - accuracy: 0.7709 - val_loss: 0.5646 - val_accuracy: 0.7718\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5654 - accuracy: 0.7710 - val_loss: 0.5635 - val_accuracy: 0.7720\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5644 - accuracy: 0.7712 - val_loss: 0.5625 - val_accuracy: 0.7721\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5635 - accuracy: 0.7714 - val_loss: 0.5617 - val_accuracy: 0.7721\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5628 - accuracy: 0.7714 - val_loss: 0.5610 - val_accuracy: 0.7721\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5621 - accuracy: 0.7715 - val_loss: 0.5605 - val_accuracy: 0.7721\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5616 - accuracy: 0.7716 - val_loss: 0.5600 - val_accuracy: 0.7721\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5611 - accuracy: 0.7717 - val_loss: 0.5595 - val_accuracy: 0.7723\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5607 - accuracy: 0.7718 - val_loss: 0.5591 - val_accuracy: 0.7721\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5603 - accuracy: 0.7719 - val_loss: 0.5588 - val_accuracy: 0.7722\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5599 - accuracy: 0.7719 - val_loss: 0.5585 - val_accuracy: 0.7723\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5596 - accuracy: 0.7720 - val_loss: 0.5581 - val_accuracy: 0.7724\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5593 - accuracy: 0.7720 - val_loss: 0.5578 - val_accuracy: 0.7725\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5590 - accuracy: 0.7721 - val_loss: 0.5577 - val_accuracy: 0.7723\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5588 - accuracy: 0.7722 - val_loss: 0.5574 - val_accuracy: 0.7728\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5585 - accuracy: 0.7723 - val_loss: 0.5571 - val_accuracy: 0.7727\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5583 - accuracy: 0.7724 - val_loss: 0.5569 - val_accuracy: 0.7726\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5581 - accuracy: 0.7725 - val_loss: 0.5567 - val_accuracy: 0.7726\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5579 - accuracy: 0.7725 - val_loss: 0.5565 - val_accuracy: 0.7727\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5577 - accuracy: 0.7725 - val_loss: 0.5563 - val_accuracy: 0.7726\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5575 - accuracy: 0.7726 - val_loss: 0.5561 - val_accuracy: 0.7731\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5574 - accuracy: 0.7726 - val_loss: 0.5560 - val_accuracy: 0.7733\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5572 - accuracy: 0.7726 - val_loss: 0.5558 - val_accuracy: 0.7734\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5570 - accuracy: 0.7727 - val_loss: 0.5557 - val_accuracy: 0.7734\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5569 - accuracy: 0.7728 - val_loss: 0.5555 - val_accuracy: 0.7736\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5568 - accuracy: 0.7727 - val_loss: 0.5554 - val_accuracy: 0.7737\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5566 - accuracy: 0.7730 - val_loss: 0.5553 - val_accuracy: 0.7743\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.7729 - val_loss: 0.5551 - val_accuracy: 0.7735\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5564 - accuracy: 0.7730 - val_loss: 0.5550 - val_accuracy: 0.7743\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5562 - accuracy: 0.7730 - val_loss: 0.5548 - val_accuracy: 0.7737\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5561 - accuracy: 0.7731 - val_loss: 0.5548 - val_accuracy: 0.7746\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5560 - accuracy: 0.7731 - val_loss: 0.5546 - val_accuracy: 0.7739\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5559 - accuracy: 0.7732 - val_loss: 0.5546 - val_accuracy: 0.7736\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5558 - accuracy: 0.7731 - val_loss: 0.5544 - val_accuracy: 0.7747\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5557 - accuracy: 0.7732 - val_loss: 0.5543 - val_accuracy: 0.7740\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5556 - accuracy: 0.7734 - val_loss: 0.5543 - val_accuracy: 0.7735\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5555 - accuracy: 0.7734 - val_loss: 0.5541 - val_accuracy: 0.7740\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5554 - accuracy: 0.7733 - val_loss: 0.5540 - val_accuracy: 0.7744\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5553 - accuracy: 0.7733 - val_loss: 0.5539 - val_accuracy: 0.7744\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5552 - accuracy: 0.7735 - val_loss: 0.5538 - val_accuracy: 0.7742\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5551 - accuracy: 0.7734 - val_loss: 0.5537 - val_accuracy: 0.7743\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7736 - val_loss: 0.5536 - val_accuracy: 0.7742\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7735 - val_loss: 0.5535 - val_accuracy: 0.7742\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7736 - val_loss: 0.5534 - val_accuracy: 0.7742\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5548 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7742\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5547 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7748\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5546 - accuracy: 0.7737 - val_loss: 0.5532 - val_accuracy: 0.7741\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7736 - val_loss: 0.5531 - val_accuracy: 0.7742\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7737 - val_loss: 0.5530 - val_accuracy: 0.7741\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7736 - val_loss: 0.5530 - val_accuracy: 0.7743\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5543 - accuracy: 0.7737 - val_loss: 0.5529 - val_accuracy: 0.7744\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7737 - val_loss: 0.5528 - val_accuracy: 0.7745\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7737 - val_loss: 0.5527 - val_accuracy: 0.7741\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5541 - accuracy: 0.7738 - val_loss: 0.5526 - val_accuracy: 0.7743\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5541 - accuracy: 0.7738 - val_loss: 0.5526 - val_accuracy: 0.7748\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5540 - accuracy: 0.7739 - val_loss: 0.5525 - val_accuracy: 0.7743\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5539 - accuracy: 0.7738 - val_loss: 0.5524 - val_accuracy: 0.7744\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5538 - accuracy: 0.7739 - val_loss: 0.5524 - val_accuracy: 0.7741\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5538 - accuracy: 0.7738 - val_loss: 0.5523 - val_accuracy: 0.7743\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5537 - accuracy: 0.7739 - val_loss: 0.5522 - val_accuracy: 0.7745\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5536 - accuracy: 0.7740 - val_loss: 0.5522 - val_accuracy: 0.7746\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5536 - accuracy: 0.7740 - val_loss: 0.5521 - val_accuracy: 0.7747\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5535 - accuracy: 0.7739 - val_loss: 0.5520 - val_accuracy: 0.7747\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5535 - accuracy: 0.7739 - val_loss: 0.5520 - val_accuracy: 0.7749\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5534 - accuracy: 0.7740 - val_loss: 0.5519 - val_accuracy: 0.7747\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5533 - accuracy: 0.7739 - val_loss: 0.5519 - val_accuracy: 0.7748\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5533 - accuracy: 0.7739 - val_loss: 0.5518 - val_accuracy: 0.7750\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5532 - accuracy: 0.7739 - val_loss: 0.5517 - val_accuracy: 0.7747\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5532 - accuracy: 0.7739 - val_loss: 0.5517 - val_accuracy: 0.7749\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5531 - accuracy: 0.7739 - val_loss: 0.5516 - val_accuracy: 0.7750\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5530 - accuracy: 0.7740 - val_loss: 0.5515 - val_accuracy: 0.7749\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5530 - accuracy: 0.7740 - val_loss: 0.5516 - val_accuracy: 0.7753\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5529 - accuracy: 0.7740 - val_loss: 0.5514 - val_accuracy: 0.7750\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5528 - accuracy: 0.7740 - val_loss: 0.5514 - val_accuracy: 0.7750\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5528 - accuracy: 0.7742 - val_loss: 0.5513 - val_accuracy: 0.7753\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5527 - accuracy: 0.7741 - val_loss: 0.5512 - val_accuracy: 0.7752\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5527 - accuracy: 0.7742 - val_loss: 0.5512 - val_accuracy: 0.7753\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5526 - accuracy: 0.7741 - val_loss: 0.5511 - val_accuracy: 0.7751\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5525 - accuracy: 0.7742 - val_loss: 0.5510 - val_accuracy: 0.7752\n",
      "2858/2858 [==============================] - 3s 818us/step\n",
      "2858/2858 [==============================] - 3s 1ms/step - loss: 0.5536 - accuracy: 0.7735\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 1.4083 - accuracy: 0.3942 - val_loss: 1.3135 - val_accuracy: 0.4192\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.2896 - accuracy: 0.4831 - val_loss: 1.2556 - val_accuracy: 0.5821\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.2161 - accuracy: 0.6282 - val_loss: 1.1657 - val_accuracy: 0.6617\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.1145 - accuracy: 0.6773 - val_loss: 1.0579 - val_accuracy: 0.6900\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.0077 - accuracy: 0.6982 - val_loss: 0.9558 - val_accuracy: 0.7069\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.9132 - accuracy: 0.7142 - val_loss: 0.8700 - val_accuracy: 0.7224\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.8355 - accuracy: 0.7274 - val_loss: 0.8002 - val_accuracy: 0.7345\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7730 - accuracy: 0.7372 - val_loss: 0.7442 - val_accuracy: 0.7427\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7233 - accuracy: 0.7434 - val_loss: 0.7003 - val_accuracy: 0.7469\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6848 - accuracy: 0.7467 - val_loss: 0.6669 - val_accuracy: 0.7501\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6560 - accuracy: 0.7495 - val_loss: 0.6422 - val_accuracy: 0.7522\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6345 - accuracy: 0.7521 - val_loss: 0.6238 - val_accuracy: 0.7549\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6186 - accuracy: 0.7547 - val_loss: 0.6102 - val_accuracy: 0.7583\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6065 - accuracy: 0.7574 - val_loss: 0.5998 - val_accuracy: 0.7609\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5972 - accuracy: 0.7611 - val_loss: 0.5917 - val_accuracy: 0.7648\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5900 - accuracy: 0.7654 - val_loss: 0.5854 - val_accuracy: 0.7678\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5843 - accuracy: 0.7679 - val_loss: 0.5804 - val_accuracy: 0.7695\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5798 - accuracy: 0.7688 - val_loss: 0.5764 - val_accuracy: 0.7702\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5761 - accuracy: 0.7694 - val_loss: 0.5732 - val_accuracy: 0.7705\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5732 - accuracy: 0.7697 - val_loss: 0.5706 - val_accuracy: 0.7707\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5709 - accuracy: 0.7700 - val_loss: 0.5686 - val_accuracy: 0.7710\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5690 - accuracy: 0.7703 - val_loss: 0.5669 - val_accuracy: 0.7714\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5675 - accuracy: 0.7706 - val_loss: 0.5656 - val_accuracy: 0.7717\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5663 - accuracy: 0.7708 - val_loss: 0.5644 - val_accuracy: 0.7718\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5652 - accuracy: 0.7709 - val_loss: 0.5635 - val_accuracy: 0.7718\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5643 - accuracy: 0.7709 - val_loss: 0.5627 - val_accuracy: 0.7719\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5636 - accuracy: 0.7710 - val_loss: 0.5620 - val_accuracy: 0.7721\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5629 - accuracy: 0.7713 - val_loss: 0.5614 - val_accuracy: 0.7721\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5624 - accuracy: 0.7713 - val_loss: 0.5609 - val_accuracy: 0.7722\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5619 - accuracy: 0.7716 - val_loss: 0.5605 - val_accuracy: 0.7720\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5615 - accuracy: 0.7715 - val_loss: 0.5600 - val_accuracy: 0.7720\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5611 - accuracy: 0.7716 - val_loss: 0.5597 - val_accuracy: 0.7722\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5607 - accuracy: 0.7717 - val_loss: 0.5594 - val_accuracy: 0.7722\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5604 - accuracy: 0.7716 - val_loss: 0.5590 - val_accuracy: 0.7723\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5601 - accuracy: 0.7717 - val_loss: 0.5588 - val_accuracy: 0.7721\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5599 - accuracy: 0.7719 - val_loss: 0.5586 - val_accuracy: 0.7723\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5596 - accuracy: 0.7718 - val_loss: 0.5582 - val_accuracy: 0.7721\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5593 - accuracy: 0.7721 - val_loss: 0.5581 - val_accuracy: 0.7722\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5591 - accuracy: 0.7720 - val_loss: 0.5578 - val_accuracy: 0.7723\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5589 - accuracy: 0.7720 - val_loss: 0.5576 - val_accuracy: 0.7721\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5587 - accuracy: 0.7720 - val_loss: 0.5574 - val_accuracy: 0.7723\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5585 - accuracy: 0.7720 - val_loss: 0.5572 - val_accuracy: 0.7722\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5583 - accuracy: 0.7720 - val_loss: 0.5570 - val_accuracy: 0.7723\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5582 - accuracy: 0.7721 - val_loss: 0.5568 - val_accuracy: 0.7725\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5580 - accuracy: 0.7722 - val_loss: 0.5566 - val_accuracy: 0.7724\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5578 - accuracy: 0.7722 - val_loss: 0.5565 - val_accuracy: 0.7725\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5577 - accuracy: 0.7721 - val_loss: 0.5564 - val_accuracy: 0.7734\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5575 - accuracy: 0.7722 - val_loss: 0.5562 - val_accuracy: 0.7730\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5574 - accuracy: 0.7723 - val_loss: 0.5561 - val_accuracy: 0.7734\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5572 - accuracy: 0.7724 - val_loss: 0.5559 - val_accuracy: 0.7728\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5571 - accuracy: 0.7723 - val_loss: 0.5557 - val_accuracy: 0.7728\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5570 - accuracy: 0.7726 - val_loss: 0.5556 - val_accuracy: 0.7735\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5568 - accuracy: 0.7726 - val_loss: 0.5554 - val_accuracy: 0.7730\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5567 - accuracy: 0.7725 - val_loss: 0.5555 - val_accuracy: 0.7740\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5566 - accuracy: 0.7727 - val_loss: 0.5552 - val_accuracy: 0.7736\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.7728 - val_loss: 0.5551 - val_accuracy: 0.7739\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5563 - accuracy: 0.7729 - val_loss: 0.5549 - val_accuracy: 0.7735\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5562 - accuracy: 0.7728 - val_loss: 0.5548 - val_accuracy: 0.7739\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5561 - accuracy: 0.7729 - val_loss: 0.5547 - val_accuracy: 0.7739\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5560 - accuracy: 0.7728 - val_loss: 0.5546 - val_accuracy: 0.7737\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5559 - accuracy: 0.7730 - val_loss: 0.5545 - val_accuracy: 0.7742\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5558 - accuracy: 0.7731 - val_loss: 0.5544 - val_accuracy: 0.7741\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5557 - accuracy: 0.7731 - val_loss: 0.5543 - val_accuracy: 0.7742\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5556 - accuracy: 0.7729 - val_loss: 0.5542 - val_accuracy: 0.7739\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5555 - accuracy: 0.7732 - val_loss: 0.5541 - val_accuracy: 0.7742\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5554 - accuracy: 0.7733 - val_loss: 0.5540 - val_accuracy: 0.7744\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5553 - accuracy: 0.7733 - val_loss: 0.5539 - val_accuracy: 0.7743\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5552 - accuracy: 0.7733 - val_loss: 0.5538 - val_accuracy: 0.7744\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5551 - accuracy: 0.7733 - val_loss: 0.5537 - val_accuracy: 0.7744\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7735 - val_loss: 0.5538 - val_accuracy: 0.7747\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7735 - val_loss: 0.5535 - val_accuracy: 0.7746\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7735 - val_loss: 0.5535 - val_accuracy: 0.7745\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5548 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7747\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5547 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7745\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5546 - accuracy: 0.7737 - val_loss: 0.5532 - val_accuracy: 0.7748\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7737 - val_loss: 0.5532 - val_accuracy: 0.7750\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7737 - val_loss: 0.5530 - val_accuracy: 0.7747\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7738 - val_loss: 0.5529 - val_accuracy: 0.7745\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5543 - accuracy: 0.7738 - val_loss: 0.5528 - val_accuracy: 0.7749\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7738 - val_loss: 0.5528 - val_accuracy: 0.7749\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7739 - val_loss: 0.5527 - val_accuracy: 0.7749\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5541 - accuracy: 0.7739 - val_loss: 0.5526 - val_accuracy: 0.7749\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5540 - accuracy: 0.7740 - val_loss: 0.5525 - val_accuracy: 0.7748\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5539 - accuracy: 0.7739 - val_loss: 0.5525 - val_accuracy: 0.7751\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5539 - accuracy: 0.7739 - val_loss: 0.5524 - val_accuracy: 0.7751\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5538 - accuracy: 0.7739 - val_loss: 0.5523 - val_accuracy: 0.7748\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5537 - accuracy: 0.7739 - val_loss: 0.5523 - val_accuracy: 0.7750\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5537 - accuracy: 0.7740 - val_loss: 0.5522 - val_accuracy: 0.7748\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5536 - accuracy: 0.7739 - val_loss: 0.5521 - val_accuracy: 0.7747\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5535 - accuracy: 0.7739 - val_loss: 0.5521 - val_accuracy: 0.7749\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5535 - accuracy: 0.7739 - val_loss: 0.5520 - val_accuracy: 0.7750\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5534 - accuracy: 0.7741 - val_loss: 0.5519 - val_accuracy: 0.7752\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5534 - accuracy: 0.7741 - val_loss: 0.5519 - val_accuracy: 0.7750\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5533 - accuracy: 0.7740 - val_loss: 0.5518 - val_accuracy: 0.7749\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5532 - accuracy: 0.7741 - val_loss: 0.5517 - val_accuracy: 0.7750\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5532 - accuracy: 0.7741 - val_loss: 0.5517 - val_accuracy: 0.7751\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5531 - accuracy: 0.7741 - val_loss: 0.5516 - val_accuracy: 0.7752\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5530 - accuracy: 0.7742 - val_loss: 0.5515 - val_accuracy: 0.7752\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5530 - accuracy: 0.7741 - val_loss: 0.5515 - val_accuracy: 0.7752\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5529 - accuracy: 0.7742 - val_loss: 0.5514 - val_accuracy: 0.7753\n",
      "2858/2858 [==============================] - 3s 867us/step\n",
      "2858/2858 [==============================] - 3s 1ms/step - loss: 0.5541 - accuracy: 0.7737\n",
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 1.3420 - accuracy: 0.4239 - val_loss: 1.2953 - val_accuracy: 0.4608\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.2673 - accuracy: 0.5505 - val_loss: 1.2279 - val_accuracy: 0.6212\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.1852 - accuracy: 0.6390 - val_loss: 1.1333 - val_accuracy: 0.6626\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 1.0845 - accuracy: 0.6736 - val_loss: 1.0315 - val_accuracy: 0.6861\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.9871 - accuracy: 0.6931 - val_loss: 0.9408 - val_accuracy: 0.7023\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.9033 - accuracy: 0.7093 - val_loss: 0.8645 - val_accuracy: 0.7178\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.8332 - accuracy: 0.7230 - val_loss: 0.8005 - val_accuracy: 0.7303\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.7750 - accuracy: 0.7336 - val_loss: 0.7478 - val_accuracy: 0.7397\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.7275 - accuracy: 0.7415 - val_loss: 0.7051 - val_accuracy: 0.7466\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6893 - accuracy: 0.7474 - val_loss: 0.6712 - val_accuracy: 0.7510\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6596 - accuracy: 0.7518 - val_loss: 0.6453 - val_accuracy: 0.7547\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.6370 - accuracy: 0.7548 - val_loss: 0.6260 - val_accuracy: 0.7577\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.6203 - accuracy: 0.7580 - val_loss: 0.6118 - val_accuracy: 0.7615\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.6079 - accuracy: 0.7619 - val_loss: 0.6012 - val_accuracy: 0.7648\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5985 - accuracy: 0.7656 - val_loss: 0.5930 - val_accuracy: 0.7683\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5912 - accuracy: 0.7681 - val_loss: 0.5867 - val_accuracy: 0.7698\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5856 - accuracy: 0.7690 - val_loss: 0.5817 - val_accuracy: 0.7705\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5811 - accuracy: 0.7695 - val_loss: 0.5778 - val_accuracy: 0.7712\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5775 - accuracy: 0.7698 - val_loss: 0.5747 - val_accuracy: 0.7715\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5746 - accuracy: 0.7702 - val_loss: 0.5721 - val_accuracy: 0.7717\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5723 - accuracy: 0.7704 - val_loss: 0.5700 - val_accuracy: 0.7712\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5704 - accuracy: 0.7706 - val_loss: 0.5683 - val_accuracy: 0.7714\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5689 - accuracy: 0.7708 - val_loss: 0.5669 - val_accuracy: 0.7713\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5675 - accuracy: 0.7708 - val_loss: 0.5657 - val_accuracy: 0.7715\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5664 - accuracy: 0.7711 - val_loss: 0.5647 - val_accuracy: 0.7716\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5655 - accuracy: 0.7712 - val_loss: 0.5639 - val_accuracy: 0.7718\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5647 - accuracy: 0.7712 - val_loss: 0.5631 - val_accuracy: 0.7719\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5640 - accuracy: 0.7712 - val_loss: 0.5625 - val_accuracy: 0.7719\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5634 - accuracy: 0.7714 - val_loss: 0.5619 - val_accuracy: 0.7718\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5629 - accuracy: 0.7714 - val_loss: 0.5614 - val_accuracy: 0.7718\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5624 - accuracy: 0.7715 - val_loss: 0.5610 - val_accuracy: 0.7719\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5619 - accuracy: 0.7715 - val_loss: 0.5606 - val_accuracy: 0.7718\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5616 - accuracy: 0.7716 - val_loss: 0.5602 - val_accuracy: 0.7718\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5612 - accuracy: 0.7717 - val_loss: 0.5598 - val_accuracy: 0.7721\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5609 - accuracy: 0.7717 - val_loss: 0.5595 - val_accuracy: 0.7721\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5605 - accuracy: 0.7717 - val_loss: 0.5592 - val_accuracy: 0.7721\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5603 - accuracy: 0.7717 - val_loss: 0.5590 - val_accuracy: 0.7721\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5600 - accuracy: 0.7718 - val_loss: 0.5587 - val_accuracy: 0.7721\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5597 - accuracy: 0.7720 - val_loss: 0.5586 - val_accuracy: 0.7725\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5595 - accuracy: 0.7720 - val_loss: 0.5582 - val_accuracy: 0.7724\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5593 - accuracy: 0.7719 - val_loss: 0.5580 - val_accuracy: 0.7723\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5591 - accuracy: 0.7721 - val_loss: 0.5578 - val_accuracy: 0.7727\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5589 - accuracy: 0.7721 - val_loss: 0.5576 - val_accuracy: 0.7724\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5587 - accuracy: 0.7721 - val_loss: 0.5574 - val_accuracy: 0.7727\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5585 - accuracy: 0.7722 - val_loss: 0.5572 - val_accuracy: 0.7728\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5583 - accuracy: 0.7723 - val_loss: 0.5570 - val_accuracy: 0.7728\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5581 - accuracy: 0.7723 - val_loss: 0.5569 - val_accuracy: 0.7730\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5580 - accuracy: 0.7724 - val_loss: 0.5567 - val_accuracy: 0.7729\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5578 - accuracy: 0.7725 - val_loss: 0.5565 - val_accuracy: 0.7731\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5577 - accuracy: 0.7725 - val_loss: 0.5564 - val_accuracy: 0.7731\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5575 - accuracy: 0.7726 - val_loss: 0.5562 - val_accuracy: 0.7735\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5574 - accuracy: 0.7726 - val_loss: 0.5561 - val_accuracy: 0.7736\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5572 - accuracy: 0.7725 - val_loss: 0.5560 - val_accuracy: 0.7744\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5571 - accuracy: 0.7728 - val_loss: 0.5558 - val_accuracy: 0.7737\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5570 - accuracy: 0.7727 - val_loss: 0.5557 - val_accuracy: 0.7735\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5569 - accuracy: 0.7729 - val_loss: 0.5555 - val_accuracy: 0.7735\n",
      "Epoch 57/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5567 - accuracy: 0.7727 - val_loss: 0.5554 - val_accuracy: 0.7737\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5566 - accuracy: 0.7730 - val_loss: 0.5553 - val_accuracy: 0.7743\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.7729 - val_loss: 0.5552 - val_accuracy: 0.7737\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5564 - accuracy: 0.7731 - val_loss: 0.5551 - val_accuracy: 0.7745\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5563 - accuracy: 0.7730 - val_loss: 0.5549 - val_accuracy: 0.7736\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 7s 2ms/step - loss: 0.5562 - accuracy: 0.7732 - val_loss: 0.5548 - val_accuracy: 0.7739\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5561 - accuracy: 0.7732 - val_loss: 0.5548 - val_accuracy: 0.7739\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5560 - accuracy: 0.7733 - val_loss: 0.5546 - val_accuracy: 0.7739\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5559 - accuracy: 0.7732 - val_loss: 0.5545 - val_accuracy: 0.7739\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5558 - accuracy: 0.7733 - val_loss: 0.5544 - val_accuracy: 0.7744\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5557 - accuracy: 0.7733 - val_loss: 0.5543 - val_accuracy: 0.7742\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5556 - accuracy: 0.7734 - val_loss: 0.5542 - val_accuracy: 0.7740\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5555 - accuracy: 0.7735 - val_loss: 0.5541 - val_accuracy: 0.7744\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5554 - accuracy: 0.7734 - val_loss: 0.5540 - val_accuracy: 0.7742\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5553 - accuracy: 0.7734 - val_loss: 0.5540 - val_accuracy: 0.7746\n",
      "Epoch 72/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5552 - accuracy: 0.7736 - val_loss: 0.5539 - val_accuracy: 0.7747\n",
      "Epoch 73/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5551 - accuracy: 0.7735 - val_loss: 0.5537 - val_accuracy: 0.7741\n",
      "Epoch 74/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5551 - accuracy: 0.7735 - val_loss: 0.5536 - val_accuracy: 0.7742\n",
      "Epoch 75/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5550 - accuracy: 0.7736 - val_loss: 0.5535 - val_accuracy: 0.7743\n",
      "Epoch 76/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5549 - accuracy: 0.7734 - val_loss: 0.5536 - val_accuracy: 0.7747\n",
      "Epoch 77/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5548 - accuracy: 0.7736 - val_loss: 0.5534 - val_accuracy: 0.7742\n",
      "Epoch 78/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5547 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7744\n",
      "Epoch 79/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5547 - accuracy: 0.7736 - val_loss: 0.5533 - val_accuracy: 0.7750\n",
      "Epoch 80/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5546 - accuracy: 0.7738 - val_loss: 0.5532 - val_accuracy: 0.7743\n",
      "Epoch 81/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5545 - accuracy: 0.7737 - val_loss: 0.5531 - val_accuracy: 0.7744\n",
      "Epoch 82/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5545 - accuracy: 0.7736 - val_loss: 0.5530 - val_accuracy: 0.7744\n",
      "Epoch 83/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5544 - accuracy: 0.7737 - val_loss: 0.5529 - val_accuracy: 0.7749\n",
      "Epoch 84/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5543 - accuracy: 0.7738 - val_loss: 0.5529 - val_accuracy: 0.7747\n",
      "Epoch 85/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5542 - accuracy: 0.7737 - val_loss: 0.5528 - val_accuracy: 0.7744\n",
      "Epoch 86/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5542 - accuracy: 0.7736 - val_loss: 0.5527 - val_accuracy: 0.7747\n",
      "Epoch 87/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5541 - accuracy: 0.7738 - val_loss: 0.5526 - val_accuracy: 0.7745\n",
      "Epoch 88/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5540 - accuracy: 0.7737 - val_loss: 0.5527 - val_accuracy: 0.7752\n",
      "Epoch 89/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5540 - accuracy: 0.7738 - val_loss: 0.5525 - val_accuracy: 0.7754\n",
      "Epoch 90/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5539 - accuracy: 0.7738 - val_loss: 0.5524 - val_accuracy: 0.7748\n",
      "Epoch 91/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5539 - accuracy: 0.7738 - val_loss: 0.5524 - val_accuracy: 0.7749\n",
      "Epoch 92/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5538 - accuracy: 0.7738 - val_loss: 0.5523 - val_accuracy: 0.7753\n",
      "Epoch 93/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5537 - accuracy: 0.7738 - val_loss: 0.5523 - val_accuracy: 0.7754\n",
      "Epoch 94/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5537 - accuracy: 0.7739 - val_loss: 0.5522 - val_accuracy: 0.7756\n",
      "Epoch 95/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5536 - accuracy: 0.7738 - val_loss: 0.5522 - val_accuracy: 0.7753\n",
      "Epoch 96/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5535 - accuracy: 0.7739 - val_loss: 0.5521 - val_accuracy: 0.7756\n",
      "Epoch 97/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5535 - accuracy: 0.7738 - val_loss: 0.5520 - val_accuracy: 0.7750\n",
      "Epoch 98/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5534 - accuracy: 0.7739 - val_loss: 0.5519 - val_accuracy: 0.7752\n",
      "Epoch 99/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5534 - accuracy: 0.7739 - val_loss: 0.5519 - val_accuracy: 0.7753\n",
      "Epoch 100/100\n",
      "3251/3251 [==============================] - 6s 2ms/step - loss: 0.5533 - accuracy: 0.7738 - val_loss: 0.5518 - val_accuracy: 0.7753\n",
      "2858/2858 [==============================] - 3s 1ms/step\n",
      "2858/2858 [==============================] - 4s 1ms/step - loss: 0.5545 - accuracy: 0.7735\n"
     ]
    }
   ],
   "source": [
    "#we will set to options of the activation functions which will be the relu and the sigmoid functions\n",
    "activation_function = ['relu', 'sigmoid']\n",
    "#the first hidden layer will have 100 or 80 neurons\n",
    "neurons_1 = [100, 80]\n",
    "#the second hidden layer will have 50 or 40 neurons\n",
    "neurons_2 = [50,40]\n",
    "#we initialize the array that will store the data of each neural network\n",
    "results = []\n",
    "\n",
    "#We will have 8 neural networks given that we have two options per loop (2^3)\n",
    "#Activations will take values 'relu' and generate 4 neural networks and then 'sigmoid'and generate 4 more neural networks\n",
    "for activations in activation_function:\n",
    "    #neuron_1 will take 100 and 80 values\n",
    "    for neuron_1 in neurons_1:\n",
    "        #neuron_2 will take 50 and 40 values\n",
    "        for neuron_2 in neurons_2:\n",
    "            # Train the model\n",
    "            #we set the optimizer as 'Adam',which is a stochastic gradient descent method that is based on adaptive \n",
    "            #estimation of first-order and second-order moments. We also set the learning rate \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "            #initialize the model as Sequential which groups different layers \n",
    "            model=models.Sequential()\n",
    "            #we add the input layer and 1 hidden layer with the neurons (100 or 80) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_1,activation=activations,input_shape=(x_train.shape[1],)))\n",
    "            #we add the second hidden layer with the neurons (50 or 40) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_2, activation=activations))\n",
    "            #we add the output layer with 5 neurons given that we have 5 cateogires with the softmax function which gives a probabilitie\n",
    "            #for every category or neuron\n",
    "            model.add(layers.Dense(5,activation='softmax'))\n",
    "            #we compile the model and set the loss function to 'sparse_categorical_corssentropy', that suits our case as our target variable\n",
    "            #is integer encoded. We set the optimizer as explained before and we choose the accuracy as the metric to evaluate the performance.\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "            #now we  fit the model with the data and train the model with 100 epochs and 90 as batch size. Epochs are the number of times that \n",
    "            #the model will go trough the entire training set in the training process whereas batches are the number of training samples to work through \n",
    "            #before updating the model. Finally, we set to 20% the proportion of data from the training set that will be part of the validation set used to\n",
    "            #control for over or under fitting and for the parameter tuning. Moreover, we store the whole trainning process in the varaible history\n",
    "            history=model.fit(x_train,y_train,epochs=100,batch_size=90,validation_split=0.20)\n",
    "            \n",
    "            #we also store the predictions that the model does with the features of the test set\n",
    "            predictions=model.predict(x_test)\n",
    "            y_predictions=[]\n",
    "            #this for will select the maxium from the array of 5 probabilities for each category that has been predicted and append it to the list of predictions\n",
    "            for i in range(predictions.shape[0]):\n",
    "                maximum = np.argmax(predictions[i])\n",
    "                y_predictions.append(maximum)\n",
    "            \n",
    "            \n",
    "            # Append the current results trough a dictionary\n",
    "            results.append({\n",
    "                #we round the accuracy and multiply it by 100\n",
    "                'Accuracy': round(model.evaluate(x_test, y_test)[1]*100, 2),\n",
    "                'activation_function': activations,\n",
    "                'neuron_1': neuron_1,\n",
    "                'neuron_2': neuron_2,\n",
    "                'loss':history.history['loss'],\n",
    "                'val_loss':history.history['val_loss'],\n",
    "                'accuracy':history.history['accuracy'],\n",
    "                'val_accuracy':history.history['val_accuracy'],\n",
    "                'predictions':y_predictions\n",
    "            })\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data sorted for each neural network, we can do a table to compare them and see their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>neuron_1</th>\n",
       "      <th>neuron_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>78.26</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>78.21</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>78.18</td>\n",
       "      <td>relu</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model 4</td>\n",
       "      <td>78.13</td>\n",
       "      <td>relu</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model 5</td>\n",
       "      <td>77.34</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model 6</td>\n",
       "      <td>77.35</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model 7</td>\n",
       "      <td>77.37</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model 8</td>\n",
       "      <td>77.35</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Accuracy activation_function  neuron_1  neuron_2\n",
       "0  Model 1     78.26                relu       100        50\n",
       "1  Model 2     78.21                relu       100        40\n",
       "2  Model 3     78.18                relu        80        50\n",
       "3  Model 4     78.13                relu        80        40\n",
       "4  Model 5     77.34             sigmoid       100        50\n",
       "5  Model 6     77.35             sigmoid       100        40\n",
       "6  Model 7     77.37             sigmoid        80        50\n",
       "7  Model 8     77.35             sigmoid        80        40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the array to Pandas DataFrame \n",
    "comparison = pd.DataFrame(results)\n",
    "#add the model column\n",
    "comparison['Model']=['Model 1','Model 2', 'Model 3','Model 4','Model 5', 'Model 6','Model 7','Model 8']\n",
    "#keep only the important variables\n",
    "result1=comparison[['Model','Accuracy','activation_function','neuron_1','neuron_2']]\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the characteristics of each model but we have to analyse each of them more in depth to see if there is over or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate the dictionary to know the index of each model\n",
    "dict_models={'Model 1':0,'Model 2':1, 'Model 3':2,'Model 4':3,'Model 5':4, 'Model 6':5,'Model 7':6,'Model 8':7}\n",
    "#generate a list with the name of each model\n",
    "model_num=list(dict_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>We are going to create three different functions to plot the results of each model iteractively.\n",
    "\n",
    "First, the **overfitting_plots function**, creates a figure with two subbplots. In the first sublot we can see the curves of the training loss and the validation-loss. This function takes the index of the model as argument and returns the figure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_plots(option):\n",
    "    #we create two subplots of a certain dimension each    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "    #we define the variable loss as the array generated by the loss function when iterating in the training process extracted form the comparison data frame.\n",
    "    loss =comparison.iloc[option]['loss']\n",
    "    #the same as before but with the validation set\n",
    "    val_loss = comparison.iloc[option]['val_loss']\n",
    "    #we set de epochs as the length of the list\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    #we plot the epochs in the x axis and the loss in the y axis in yellow\n",
    "    ax1.plot(epochs, loss, 'y', label='Training loss')\n",
    "    #same as before with the validation loss in red\n",
    "    ax1.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    #put the title of the subplot\n",
    "    ax1.title.set_text('Training and validation loss')\n",
    "    #set the x and the y label\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    #plot the legend\n",
    "    ax1.legend()\n",
    "    #we do the second plot as the first but changing the loss for the accuracy.\n",
    "    accuracy =comparison.iloc[option]['accuracy']\n",
    "    val_accuracy =comparison.iloc[option]['val_accuracy']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    ax2.plot(epochs, accuracy, 'y', label='Accuracy')\n",
    "    ax2.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "    ax2.title.set_text('Training and validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    Second, the <b>observed_against_predicted</b>  function, takes the index of the model in the comparison table as argument and returns a barplot with the observed value against the predicted value for the first 20 observations of the test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed_against_predicted(option):\n",
    "    #we first construct a dataframe with the column of the observed hours worked per week and the predicted ones(two columns)\n",
    "    observed_predicted=y_test\n",
    "    observed_predicted['Predicted']=comparison.iloc[option]['predictions']\n",
    "    observed_predicted.rename(columns = {'Hours worked per week':'Observed'},inplace = True)\n",
    "    #we reset the index to go from 0 onwards \n",
    "    observed_predicted.reset_index(drop=True,inplace=True)\n",
    "    #we sum one to all the values of the dataframe to be able to plot the vaalue 0\n",
    "    observed_predicted2=observed_predicted+1\n",
    "    #we plot only the 20 first values of the dataframe\n",
    "    table=observed_predicted2.head(20)\n",
    "    table.plot.bar()\n",
    "    #we switch the values of the y axis to make them the cateogries of the hours worked per week variable\n",
    "    plt.yticks([0,1,2,3,4,5],[\"\",\"Not working\",\"Part-time: 15 or less hours worked\",\"Part-time: 16 to 30 hours worked\",\"Full-time: 31 to 48 hours worked\",\"Full-time: 49 or more hours worked\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Last, the <b>plot_model_results</b>, takes the name of the model as argument and has the previous two functions nested so that it plots both the over or underfitting  plots and the comaprison of predicted against observed values of the model choosed. Moreover, it prints a string that indicates the accuracy and other characteristics of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model):\n",
    "    #using the previous dictionary  we find the index with the name of the model\n",
    "    option=dict_models[model]\n",
    "    #print the title\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+\"                                                           \" +model+ ' Results:'+'\\033[0m')\n",
    "    #plot the curves of over and underfitting\n",
    "    overfitting_plots(option).show()\n",
    "    #prints a title to separate both plots\n",
    "    print('--------------------------------------------------------')\n",
    "    print('  Hours worked per week: Observed against Predicted  ')\n",
    "    print('--------------------------------------------------------')\n",
    "    #plots the arplot of the values predicted and observed\n",
    "    observed_against_predicted(option)\n",
    "    #prints the string with the data from the model\n",
    "    print('For this model we have used '+ str(comparison.iloc[option]['activation_function']) +' as activation function and the first hidden layer had '+str(comparison.iloc[option]['neuron_1']) \n",
    "          + ' neurons while the second hidden layer had '+str(comparison.iloc[option]['neuron_2']) + ' neurons. The accuracy of this model measured with the test set has been: '+ str(comparison.iloc[option]['Accuracy']) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d12d1b4f62141d08fd9a773243b95fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('Model 1', 'Model 2', 'Model 3', 'Model 4', 'Mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we finally make the last function described interactively and we set all the model names as the options\n",
    "#to choose in the parameter of the function\n",
    "res=interactive(plot_model_results,model=model_num)\n",
    "display(res)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTSS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Multinomial Regression  <a id='logistic_regression'></a>\n",
    "\n",
    "The LogisticRegression class can be configured for multinomial logistic regression by setting the “multi_class” argument to “multinomial” and the “solver” argument to a solver that supports multinomial logistic regression, such as “lbfgs“.\n",
    "\n",
    "The multinomial logistic regression model will be fit using cross-entropy loss and will predict the integer value for each integer encoded class label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736177194421657"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[['Observed']],logistic_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_models = dict()\n",
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    # create name for model\n",
    "    key = '%.4f' % p\n",
    "    # turn off penalty in some cases\n",
    "    if p == 0.0:\n",
    "        # no penalty in this case\n",
    "        logistic_models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "    else:\n",
    "        logistic_models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=dict()\n",
    "for key,value in logistic_models.items():\n",
    "    value.fit(x_train,y_train)\n",
    "    predictions=value.predict(x_test)\n",
    "    accuracies['Model with pealty: '+key]=accuracy_score(y_test[['Observed']],predictions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model with pealty: 0.0000</td>\n",
       "      <td>0.773640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model with pealty: 0.0001</td>\n",
       "      <td>0.746207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model with pealty: 0.0010</td>\n",
       "      <td>0.771288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model with pealty: 0.0100</td>\n",
       "      <td>0.773268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model with pealty: 0.1000</td>\n",
       "      <td>0.773585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model with pealty: 1.0000</td>\n",
       "      <td>0.773618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Accuracies\n",
       "0  Model with pealty: 0.0000    0.773640\n",
       "1  Model with pealty: 0.0001    0.746207\n",
       "2  Model with pealty: 0.0010    0.771288\n",
       "3  Model with pealty: 0.0100    0.773268\n",
       "4  Model with pealty: 0.1000    0.773585\n",
       "5  Model with pealty: 1.0000    0.773618"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_comparison=pd.DataFrame({\"Model\":list(accuracies.keys()),\"Accuracies\":list(accuracies.values())})\n",
    "logit_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison of approaches: Bootstrapping <a id='bootstraping'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data change the random_state to make each split different\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=50,test_size=0.20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5280/2526181574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#neural model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mneural_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mneural_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mneral_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#neural model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "neural_model=models.Sequential()\n",
    "neural_model.add(layers.Dense(100,activation='relu',input_shape=(x_train.shape[1],)))\n",
    "neral_model.add(layers.Dense(50, activation='relu'))\n",
    "neural_model.add(layers.Dense(5,activation='softmax'))\n",
    "neural_model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "#to train the neural\n",
    "neural_model.fit(x_train,y_train,epochs=100,batch_size=90,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic model\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "#to train the logistic\n",
    "\n",
    "logistic_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get accuracy with neural model\n",
    "predictions=model.predict(x_test)\n",
    "y_predictions=[]\n",
    "            \n",
    "for i in range(predictions.shape[0]):\n",
    "        maximum = np.argmax(predictions[i])\n",
    "        y_predictions.append(maximum)\n",
    "        \n",
    "\n",
    "print('Accuracy:'+ str(round(model.evaluate(x_test, y_test)[1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'int' has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5280/2625967623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#to get the accuracy with logistic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlogistic_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Observed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'int' has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "#to get the accuracy with logistic\n",
    "\n",
    "logistic_predictions=value.predict(x_test)\n",
    "accuracy_score(y_test[['Observed']],predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness analysis (0.5 points)\n",
    "\n",
    "How robust are your results? Think of things like:\n",
    "\n",
    "* outliers in the data\n",
    "* different specification of regression, neural network etc.\n",
    "* split of data in train, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c76eccb6035c07aa27eec1492ed7ed3",
     "grade": true,
     "grade_id": "cell-365f4bd48b20ea77",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion (0.5 points)\n",
    "\n",
    "What did you find in the analysis above; what is the answer to the question you started out with.\n",
    "\n",
    "What are weaknesses of your approach that can be improved upon in future research (e.g. in your thesis).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e908b508af5c2244449588615f83a4d",
     "grade": true,
     "grade_id": "cell-f8f3ab0fc4655b00",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
