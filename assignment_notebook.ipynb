{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Predicting the number of hours worked: a neural network vs logistic model approach\"\n",
    "COLLABORATORS = \"Matteo Turrini and Manuel Espinosa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Datascience-for-Economics-Final-Assignment\" data-toc-modified-id=\"Datascience-for-Economics-Final-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Datascience for Economics Final Assignment</a></span></li><li><span><a href=\"#Research-question-(0.5-points)\" data-toc-modified-id=\"Research-question-(0.5-points)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Research question (0.5 points)</a></span></li><li><span><a href=\"#Motivation-(0.5-points)\" data-toc-modified-id=\"Motivation-(0.5-points)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Motivation (0.5 points)</a></span></li><li><span><a href=\"#Method-and-data-(1.0-point)\" data-toc-modified-id=\"Method-and-data-(1.0-point)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Method and data (1.0 point)</a></span></li><li><span><a href=\"#Preview-of-the-answers-(0.5-points)\" data-toc-modified-id=\"Preview-of-the-answers-(0.5-points)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preview of the answers (0.5 points)</a></span></li><li><span><a href=\"#Main-assumptions-(0.5-points)\" data-toc-modified-id=\"Main-assumptions-(0.5-points)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Main assumptions (0.5 points)</a></span></li><li><span><a href=\"#Python/R-code-(6-points)\" data-toc-modified-id=\"Python/R-code-(6-points)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Python/R code (6 points)</a></span></li><li><span><a href=\"#Robustness-analysis-(0.5-points)\" data-toc-modified-id=\"Robustness-analysis-(0.5-points)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Robustness analysis (0.5 points)</a></span></li><li><span><a href=\"#Discussion-and-conclusion-(0.5-points)\" data-toc-modified-id=\"Discussion-and-conclusion-(0.5-points)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Discussion and conclusion (0.5 points)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535c21960d4663d5edac398cb445d087",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascience for Economics Final Assignment\n",
    "\n",
    "\n",
    "\n",
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Matteo Turrini|2084906|u207794|\n",
    "|Manuel Espinosa|2067814|u112479|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question (0.5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a9d08ea38644e3ac0b85955731a7e3",
     "grade": true,
     "grade_id": "cell-44d71caea99ce92b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "How does personal characteristics help predicting the level of hours worked per week in the UK? Which method performs better in doing so: Neural networks or Logistic Multinomial Regressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation (0.5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74e1c61d4c263e5dcedc9c4f7565432",
     "grade": true,
     "grade_id": "cell-a00025e68181b6f6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Predicting the number of hours worked per week could be a helpful tool in order to classify workers on the basis of this parameter, and then use this information to promote and implement Active Labor Market Programmes (ALMPs). These are government programmes aimed at increasing the rate of employment by bringing unemployed people into employment, preventing already employed people to lose their jobs, and increasing productivity and earnings: this way, governments are able to improve the functioning of labor markets. Therefore, predicting the hours worked per week per type of worker can provide some \"profiles\" of workers that ALMPs can use to improve the efficiency of a working sector or to maintain an already efficient one.\n",
    "\n",
    "**Moreover, we think that it would be interesting to compare two methods widely used in classification problems like ours: on one hand, we have  Neural networks, which are capable of disentangling different patterns in the data by using non-linear functions like relu and sigmoid [(Dreiseitl  et al., 2012)](https://doi.org/10.1016/S1532-0464(03)00034-0); on the other hand, we have Logistic Multinomial Regression, which is another classification method used to analyse the relationship between the outcome and multiple independent variables. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and data (1.0 point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b91e9ed0432af5f7bd6e33a6a51515",
     "grade": true,
     "grade_id": "cell-a2a13b6c938ec4c8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The data we have used to conduct the study comes from the English Office for National Statistics, here's the link to the [website](https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/datasets/2011censusteachingfile). \n",
    "    This is a sample of 2011 providing microdata with 569741 observations. People analysed in this sample are classified under 16 cateorical variables:the region, the residence type, family composition, population base, sex, age, marital status, student, country of birth, health, ethnic group, religion, economic activity, occupation, industry, hours worked per week, and approximated social grade. For a more in depth explanation of each variable and the different values it can take you can access this [file](Microdatateachingvariablels.pdf). This dataset is going to be [cleaned](#data_cleaning) and [transformed](#data_transformation) using pandas library. Once the data is ready there will be a brief descriptive analysis of our variables to see any relevant distribution or relationship.\n",
    "    \n",
    "   Regarding the methodology of our study it will consist in predicting, following a classification problem, the hours worked per week of our individuals. In order to do so, we first have to prepare our features and our target variable and have them separate. Having done this, we will first apply a [Neural network approach](#neural_network), in which we will compare several neural networks by implementing a simple parameter tunning technique; Then, we will implement a [logistic multinomial regression](#logistic_regression) comparing a few of them by introducing the penalty term. Finally, we will choose the best model for each approach based on its performance and compare them by implementing a [bootstrapping](#bootstrapping) tool that will serve as a robustness analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers (0.5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a675684538158f19ff2a70fa98e7fff2",
     "grade": true,
     "grade_id": "cell-c2854f0b8b034fae",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Since the research question is divided in two we considerthat the preview of the answer should be divided as well. There for the main results of our study have been:\n",
    "* Our best model has a prediction accuracy of the level of hours worked per week that goes from 78% to 78,5% in the 95% of times.\n",
    "* The Neural Network approach that a slightly better job than the Logistic Nominal Regression in predicting the level of hours worked per week. This slightly difference can be quantified as a 0.5% in the prediction accuracy.\n",
    "\n",
    "However, we suggest you to read our discussion to understand that our results are not robust enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions (0.5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43d4f76461858e4d6140ccbf5347ab6",
     "grade": true,
     "grade_id": "cell-03cb6a4826b945c9",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In order to answer to the research question with our analysis, we need to make two main assumptions: first of all, we assume the survey to be randomized in order to minimize data corruption, avoiding bias, and improve data collection. Our other main assumption is that we assume, in order for this model to be valid, that there are no significant changes in the economic environment, the industry or the markets: for this reason, we can't guarantee that this model will hold in the future if there will be significant changes in the aforementioned fields. \n",
    "Given that we don't establish any causal relationship in the model, there's no need to make additional assumptions regarding the distribution of variables and we do not have to worry about facing any problem such as missing counfounders, multicolinearity between variables or omitted variable bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/R code (6 points)\n",
    "\n",
    "\n",
    "\n",
    "## Package importation\n",
    "\n",
    "First of all we need to import the libraries for the functions we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0736d3fd2fd323ad705ff2d7f5f0b8",
     "grade": true,
     "grade_id": "cell-c8076f3091bfd885",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#we import the numpy library  for many uses such as generate new data or manipulate arrays\n",
    "import numpy as np\n",
    "#the pandas library to manipulate and transform the data\n",
    "import pandas as pd\n",
    "#the seaborn library which is based on matplotlib and makes the graphs more attractive\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "\n",
    "#matplotlib and concretely pyplot will be useful to plot the different graphs that we need to show our results\n",
    "import matplotlib.pyplot as plt\n",
    "#is a library designed for machine learning and it contains keras which we will use for the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#we will need the LogisticRegression model to implement the second approach\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#we will use it to suppress warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "#we import scikit learn to access different statistical tools and functions such as methods to split the data in training and testing set, \n",
    "#to standardize the data if needed and to compute the accuracy between predicted and true values\n",
    "#from sklearn.preprocessing import scale\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#We finally import the ipywidgets package to make our functions interactive and display them\n",
    "from ipywidgets import interact, interactive, fixed, Layout\n",
    "import ipywidgets as widgets \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "We will have to check thoroughly the structure of the data as well as the column types to see how to transform the data in a way that can be used afterwards, taking into account that all of the variables are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation\n",
    "\n",
    "We first import the data from a csv that you will find in the [repository](/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>E12000001</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569736</th>\n",
       "      <td>7946020</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569737</th>\n",
       "      <td>7944310</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569738</th>\n",
       "      <td>7945374</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569739</th>\n",
       "      <td>7944768</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569740</th>\n",
       "      <td>7944959</td>\n",
       "      <td>W92000004</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569741 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Person ID     Region Residence Type Family Composition Population Base  \\\n",
       "0        7394816  E12000001              H                  2               1   \n",
       "1        7394745  E12000001              H                  5               1   \n",
       "2        7395066  E12000001              H                  3               1   \n",
       "3        7395329  E12000001              H                  3               1   \n",
       "4        7394712  E12000001              H                  3               1   \n",
       "...          ...        ...            ...                ...             ...   \n",
       "569736   7946020  W92000004              H                  1               1   \n",
       "569737   7944310  W92000004              H                  3               1   \n",
       "569738   7945374  W92000004              H                  3               1   \n",
       "569739   7944768  W92000004              H                  1               1   \n",
       "569740   7944959  W92000004              H                  2               1   \n",
       "\n",
       "       Sex Age Marital Status Student Country of Birth Health Ethnic Group  \\\n",
       "0        2   6              2       2                1      2            1   \n",
       "1        1   4              1       2                1      1            1   \n",
       "2        2   4              1       2                1      1            1   \n",
       "3        2   2              1       2                1      2            1   \n",
       "4        1   5              4       2                1      1            1   \n",
       "...     ..  ..            ...     ...              ...    ...          ...   \n",
       "569736   1   5              1       2                1      4            1   \n",
       "569737   1   3              1       2                1      2            1   \n",
       "569738   1   1              1       1                1      1            1   \n",
       "569739   2   8              5       2                1      3            1   \n",
       "569740   2   2              2       2                1      2            1   \n",
       "\n",
       "       Religion Economic Activity Occupation Industry Hours worked per week  \\\n",
       "0             2                 5          8        2                    -9   \n",
       "1             2                 1          8        6                     4   \n",
       "2             1                 1          6       11                     3   \n",
       "3             2                 1          7        7                     3   \n",
       "4             2                 1          1        4                     3   \n",
       "...         ...               ...        ...      ...                   ...   \n",
       "569736        9                 1          8        8                     3   \n",
       "569737        1                 1          7        4                     3   \n",
       "569738        2                -9         -9       -9                    -9   \n",
       "569739        9                 5          9        2                    -9   \n",
       "569740        1                 1          7        4                     1   \n",
       "\n",
       "       Approximated Social Grade  \n",
       "0                              4  \n",
       "1                              3  \n",
       "2                              4  \n",
       "3                              2  \n",
       "4                              2  \n",
       "...                          ...  \n",
       "569736                         3  \n",
       "569737                         4  \n",
       "569738                        -9  \n",
       "569739                         4  \n",
       "569740                         4  \n",
       "\n",
       "[569741 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data importation\n",
    "#set the file directory path\n",
    "file=('2011 Census Microdata Teaching File.csv')\n",
    "#use the pandas methos read_csv to create a dataframe from the csv\n",
    "df= pd.read_csv(file, header=None)\n",
    "#we set the column names\n",
    "df.columns = df.iloc[1,:].values\n",
    "#we drop the first two raws of the file as there is no interesting information in these.\n",
    "df.drop([0,1], axis=0, inplace=True)\n",
    "#after manipulating the dataframe, we reset the index of it\n",
    "df_small=df.reset_index(drop=True)\n",
    "df_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation  <a id='data_transformation'></a>\n",
    "\n",
    "Given that all of the variables are integer encoded except from region and Residence type, we preferred to change these two to integer encoded as well. Moreover, all the columns were of the type object (Strings) and we would like them to be integers so we also changed the type of variables of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    object\n",
       "Region                        int64\n",
       "Residence Type                int64\n",
       "Family Composition           object\n",
       "Population Base              object\n",
       "Sex                          object\n",
       "Age                          object\n",
       "Marital Status               object\n",
       "Student                      object\n",
       "Country of Birth             object\n",
       "Health                       object\n",
       "Ethnic Group                 object\n",
       "Religion                     object\n",
       "Economic Activity            object\n",
       "Occupation                   object\n",
       "Industry                     object\n",
       "Hours worked per week        object\n",
       "Approximated Social Grade    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Region and Residence Type to integers:\n",
    "dictionary_region={\"E12000001\":1,\"E12000002\":2,\"E12000003\":3, \"E12000004\":4,\"E12000005\":5,\"E12000006\":6,\"E12000007\":7,\"E12000008\":8,\"E12000009\":9,\"W92000004\":10}\n",
    "dictionary_residence_type={\"C\":1,\"H\":2}\n",
    "#we use the two previous dictionaries created to replace the keys for the values in each column\n",
    "df_small=df_small.replace({\"Region\": dictionary_region,\"Residence Type\":dictionary_residence_type})\n",
    "\n",
    "#We show the types of data of the columns which is mostly objects\n",
    "df_small.dtypes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person ID                    int32\n",
       "Region                       int32\n",
       "Residence Type               int32\n",
       "Family Composition           int32\n",
       "Population Base              int32\n",
       "Sex                          int32\n",
       "Age                          int32\n",
       "Marital Status               int32\n",
       "Student                      int32\n",
       "Country of Birth             int32\n",
       "Health                       int32\n",
       "Ethnic Group                 int32\n",
       "Religion                     int32\n",
       "Economic Activity            int32\n",
       "Occupation                   int32\n",
       "Industry                     int32\n",
       "Hours worked per week        int32\n",
       "Approximated Social Grade    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create a list with the column names\n",
    "column_names=list(df_small.columns)\n",
    "#create a dictionary\n",
    "dictionary = dict()\n",
    "#this for will set the column names as the key of the dictionary and the int class as the value\n",
    "for item in column_names:\n",
    "    key = item\n",
    "    value=int\n",
    "    \n",
    "    dictionary[key] = value\n",
    "#we change the data type of each column using the dictionary created\n",
    "df_Small = df_small.astype(dictionary)\n",
    "\n",
    "#show the new data types for the columns\n",
    "df_Small.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clenaing <a id='data_cleaning'></a>\n",
    "\n",
    "Given the scope of our project we don't need data for people under 16 neither students living away during term-time. Therefore, we delete the rows corresponding to these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Residence Type</th>\n",
       "      <th>Family Composition</th>\n",
       "      <th>Population Base</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Student</th>\n",
       "      <th>Country of Birth</th>\n",
       "      <th>Health</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Religion</th>\n",
       "      <th>Economic Activity</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Hours worked per week</th>\n",
       "      <th>Approximated Social Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394816</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394745</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7395066</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395329</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394712</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457118</th>\n",
       "      <td>7944868</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457119</th>\n",
       "      <td>7946020</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457120</th>\n",
       "      <td>7944310</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457121</th>\n",
       "      <td>7944768</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457122</th>\n",
       "      <td>7944959</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457123 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Person ID  Region  Residence Type  Family Composition  \\\n",
       "0         7394816       1               2                   2   \n",
       "1         7394745       1               2                   5   \n",
       "2         7395066       1               2                   3   \n",
       "3         7395329       1               2                   3   \n",
       "4         7394712       1               2                   3   \n",
       "...           ...     ...             ...                 ...   \n",
       "457118    7944868      10               2                   2   \n",
       "457119    7946020      10               2                   1   \n",
       "457120    7944310      10               2                   3   \n",
       "457121    7944768      10               2                   1   \n",
       "457122    7944959      10               2                   2   \n",
       "\n",
       "        Population Base  Sex  Age  Marital Status  Student  Country of Birth  \\\n",
       "0                     1    2    6               2        2                 1   \n",
       "1                     1    1    4               1        2                 1   \n",
       "2                     1    2    4               1        2                 1   \n",
       "3                     1    2    2               1        2                 1   \n",
       "4                     1    1    5               4        2                 1   \n",
       "...                 ...  ...  ...             ...      ...               ...   \n",
       "457118                1    1    2               1        2                 1   \n",
       "457119                1    1    5               1        2                 1   \n",
       "457120                1    1    3               1        2                 1   \n",
       "457121                1    2    8               5        2                 1   \n",
       "457122                1    2    2               2        2                 1   \n",
       "\n",
       "        Health  Ethnic Group  Religion  Economic Activity  Occupation  \\\n",
       "0            2             1         2                  5           8   \n",
       "1            1             1         2                  1           8   \n",
       "2            1             1         1                  1           6   \n",
       "3            2             1         2                  1           7   \n",
       "4            1             1         2                  1           1   \n",
       "...        ...           ...       ...                ...         ...   \n",
       "457118       1             1         2                  1           4   \n",
       "457119       4             1         9                  1           8   \n",
       "457120       2             1         1                  1           7   \n",
       "457121       3             1         9                  5           9   \n",
       "457122       2             1         1                  1           7   \n",
       "\n",
       "        Industry  Hours worked per week  Approximated Social Grade  \n",
       "0              2                      0                          4  \n",
       "1              6                      4                          3  \n",
       "2             11                      3                          4  \n",
       "3              7                      3                          2  \n",
       "4              4                      3                          2  \n",
       "...          ...                    ...                        ...  \n",
       "457118         9                      3                          2  \n",
       "457119         8                      3                          3  \n",
       "457120         4                      3                          4  \n",
       "457121         2                      0                          4  \n",
       "457122         4                      1                          4  \n",
       "\n",
       "[457123 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We replace the -9 values that refer to no code required to the 0 as it will make it easier to plot the graphs\n",
    "df_Small=df_Small.replace(-9,0)\n",
    "#delete the rows in which age is under 16, which corresponds to value 1\n",
    "df_Small = df_Small[~(df_Small[\"Age\"] == 1)]\n",
    "#delete the rows in wich population base is students living away during term-time, which corresponds to value 2\n",
    "df_Small = df_Small[~(df_Small[\"Population Base\"] == 2)]\n",
    "\n",
    "\n",
    "#we define the list variables with all the column names of the dataframe\n",
    "variables=column_names[1:]\n",
    "#we reset the index of the dataframe after all these changes\n",
    "df_Small.reset_index(drop=True,inplace=True)\n",
    "df_Small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels\n",
    "\n",
    "Given that the data is integer encoded we will need dictionaries that associate the label to the value to set the labels in the plots or other implementations in the different functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "#Dictionary for every variable label and its corresponding value\n",
    "dict_family_composition={'Resident of a comunal establishment or short-term resident':0,'Not in a family':1, 'Married':2, 'Cohabiting couple':3, 'Lone parent family (male)':4, 'Lone parent family (female)':5, 'Others':6}\n",
    "dict_residence_type={'Resident in a communal establishment':1,'Not resident in a communal establishment':2}\n",
    "dict_population_base={'Usual resident':1, 'Short-term resident':3}\n",
    "dict_sex={'Male':1, 'Female':2}\n",
    "dict_age={'16 to 24':2, '25 to 34':3, '35 to 44':4, '45 to 54':5, '55 to 64':6, '65 to 74':7, '75 and over':8}\n",
    "dict_marital_status={'Single':1, 'Married':2, 'Separated':3, 'Divorced':4, 'Widowed':5}\n",
    "dict_student={'Yes':1, 'No':2}\n",
    "dict_country_of_birth={'UK':1, 'Non UK':2}\n",
    "dict_health={'Very good health':1, 'Good health':2, 'Fair health':3, 'Bad health':4, 'Very bad health':5}\n",
    "dict_ethnic_group={'White':1, 'Mixed':2, 'Asian and Asian British':3, 'Black or Black British':4, 'Chinese or Other ethnic group':5}\n",
    "dict_religion={'No religion':1, 'Christian':2, 'Buddhist':3, 'Hindu':4, 'Jewish':5, 'Muslim':6, 'Sikh':7, 'Other religion':8, 'Not stated':9}\n",
    "dict_economic_activity={'Employee':1, 'Self-employed':2, 'Unemployed':3, 'Full-time student':4, 'Retired':5, 'Student':6, 'Looking after home or family':7, 'Long-term sick or disabled':8, 'Others':9}\n",
    "dict_occupation={'People that never worked':0,'Managers, Directors and Senior Officials':1, 'Professional Occupations':2, 'Associate Professional and Technical Occupations':3, 'Administrative and Secretarial Occupations':4, 'Skilled Trades Occupations':5, 'Caring, Leisure and Other Service Occupations':6, 'Sales and Customer Service Occupations':7, 'Process, Plant and Machine Operatives':8, 'Elementary Occupations':9}\n",
    "dict_industry={'People never worked':0,'Agriculture, forestry and fishing':1, 'Mining and quarrying; Manufacturing; Electricity, gas, steam and air conditioning system; Water supply':2, 'Construction':3, 'Wholesale and retail trade; Repair of motor vehicles and motorcycles':4, 'Accommodation and food service activities':5, 'Transport and storage; Information and communication':6, 'Financial and insurance activities; Intermediation':7, 'Real estate activities; Professional, scientific and technical activities; Administrative and support service activities':8, 'Public administration and defence; compulsory social security':9, 'Education':10, 'Human health and social work activities':11, 'Other community, social and personal service activities; Private households employing domestic staff; Extra-territorial organisations and bodies':12}\n",
    "dict_hours_worked_per_week={ 'Not working':0,'Part-time: 15 or less hours worked':1, 'Part-time: 16 to 30 hours worked':2, 'Full-time: 31 to 48 hours worked':3, 'Full-time: 49 or more hours worked':4}\n",
    "dict_approximated_social_grade={'Resident in a comunal establishment':0,'AB':1, 'C1':2, 'C2':3, 'DE':4}\n",
    "dict_region={\"North East\":1,\"North West\":2,\"Yorkshire and the Humber\":3,\"East Midlands\":4,\"West Midlands\":5,\"East of England\":6,\"London\":7,\"South East\":8,\"South West\":9,\"Wales\":10}\n",
    "#list with all the dictionaries for each variable\n",
    "dict_variables=[dict_region,dict_residence_type,dict_family_composition, dict_population_base,dict_sex,dict_age,dict_marital_status,dict_student, dict_country_of_birth,dict_health,dict_ethnic_group,dict_religion, dict_economic_activity,dict_occupation,dict_industry,dict_hours_worked_per_week,dict_approximated_social_grade]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Data visualization and descrpitive analysis <a id='descriptive_analysis'></a>\n",
    "\n",
    "<p style='text-align: justify;'>Finally, we have defined four different functions that will be used to plot the main descriptive plots of each variable interactively.\n",
    "    \n",
    "First, the **generate_pie_chart**, will take the name of a variable and return the pie chart showing the distribution of this variable.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pie_chart(Variable):\n",
    "    \n",
    "        #we plot the pie chart using the value_count which computes the frequency of each value. We also use sort_index \n",
    "        #to sort them in order that it will be easier to match the labels in the legend\n",
    "        plt.pie(df_Small[Variable].value_counts().sort_index(), labels=None,autopct='%1.1f%%')\n",
    "        #the title of the plot in bold\n",
    "        plt.title(Variable,weight='bold', size=14)\n",
    "        #the legend of the plot using the function match_varaible_lables to have the labels of the variable\n",
    "        plt.legend( match_variable_labels(Variable),loc='center left', bbox_to_anchor=(1.5, 0.5));\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Then, the <b>generate_relation_plot</b>, creates a barplot that relates the variable passed as argument and the Hours worked per week variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_plot(Variable):\n",
    "        #a stores a crosstable of two factors which are the variable passed as an argument and Hours worked per week\n",
    "        a=pd.crosstab(df_Small[\"Hours worked per week\"],df_Small[Variable])\n",
    "        #we plot the crosstable\n",
    "        a.plot.bar()\n",
    "        #plot the legend using the same technique as in generate_pie function \n",
    "        plt.legend(match_variable_labels(Variable).keys(),loc='center left', bbox_to_anchor=(1.5, 0.5))\n",
    "        #we configure the values of the x axis to see the different labels of the variable Hours worked per week\n",
    "        plt.xticks(list(dict_hours_worked_per_week.values()), list(dict_hours_worked_per_week.keys()),rotation=\"vertical\")\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    The <b>match_variable_labels</b>, is an auxiliar function that will be useful in different parts of the project as it matches the variable with the labels corresponding to the values of the variable. Specifically, it return the list of the labels .</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_variable_labels(Variable):\n",
    "    #we implement a for loop that will have a boolean condition to see whether the variable passed is equal to the variable of \n",
    "    #the variable list passed in the for loop. If that's the case, we will have the position of the dictionary of the current variable in \n",
    "    #the list of dictionaries. This dictionary will be returned as label. In the other function we will use the method .keys() \n",
    "    #to get the keys of the dictionaries as lables.\n",
    "    for i in range(len(variables)):\n",
    "        if Variable == variables[i]:\n",
    "            label=dict_variables[i]\n",
    "            break\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    Finally, the <b>descriptive</b> function groups together the other three functions to plot the pie chart of the variable and the bar plot relating the variable with Hours worked per week.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive(Variable,Execute):\n",
    "      \n",
    "    #if the execute variable is true it will execute the function(enter the if)        \n",
    "    if Execute==True:\n",
    "        #print the title        \n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Pie Chart          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "        #show the plot created in the function generate_pie_chart\n",
    "        generate_pie_chart(Variable).show()\n",
    "        #print title\n",
    "        print(\"----------------------------------------------------------------\\n\"+\"                 Relation with Hours worked per week          \\n\"+\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "        #show the plot created in the function generate_relation_plot\n",
    "        generate_relation_plot(Variable).show()\n",
    "    else: \n",
    "        #if the execute variable is not pressed this message will be displayed\n",
    "        print(\"In order to display the plots you need to press the execute button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13068d8078b94f71bbfe6f4c33366d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('Region', 'Residence Type', 'Family Compositio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#in this cell we use the interactive function from ipywidgets to make the descriptive functions interactive. We can choose \n",
    "#whether to execute or not the function by clicking the number and the variable to be plotted from all the variables\n",
    "w=interactive(descriptive,Variable=variables,Execute=False)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* As we can see from the pie charts, people in the sample are almost equally distributed to what regards regions of origin, gender, age, industry sector, occupation, and approximated social grade.\n",
    "\n",
    "* London, South East and North West are the regions with most of the full-time job workers: this is probably because of the high cost of life in London and South East, while a lot of steel industries, mines and factories were built in the North-West, and that's why plenty of workers went living there.\n",
    "\n",
    "* Quite logically, given that feeding and supporting a family requires more money and, most of the time, more hours worked, married people, which corresponds to more than half of the sample, are mostly involved in full time jobs.\n",
    "\n",
    "* To what regards gender differences, we can see that males and females are equally distributed, but at the same time males are more involved in full time jobs while females are more involved with part-time jobs: this could be related to the fact that women are generally seen as responsible for houseworks and other chores not necessarily related with their jobs.\n",
    "\n",
    "* People in the sample are also equally split along all the age ranges present in the analysis, with people from 25 to 54 more involved with full-time jobs and people under 25 involved with part-time jobs, given that most people under 25 are probably student, and therefore they don't have time for a full-time job.\n",
    "\n",
    "* 87.5% of the sample are white people, with the biggest minority being asian people.\n",
    "\n",
    "* It's interesting to note that \"professional occupations\" and \"associate professional and technical occupations\" are the two industry sectors mostly present in full-time jobs in the 31 to 48 hours range, while \"Managers, directors and seniors officials\" are the majority in 49 or more hours range.\n",
    "\n",
    "* At the same time, industry types with more hours worked are \"Real Estate activities; Professional, Scientific and Technical activities; Administrative and support service activities\", \"Wholesale and Retail Trade; Repair of motor vehicles and motorcycles\", and \"Transport and Storage; Information and Communication\".\n",
    "\n",
    "* As a final remark, AB approximated social grade, which corresponds to \"higher and intermediate managerial, administrative and professional occupations\", is the one with most hours worked; C1 grade follows, corresponding to \"supervisory, clerical and junior managerial, administrative, professional occupations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting hours worked : Neural Network approach <a id='neural_network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>As a brief introduction, a Neural Network is a trainable tool that is formed by connecting the different\n",
    "layers containing neural units, which are the processing elements, that enable the Neural\n",
    "Network to learn and recognise patterns. Data propagates from\n",
    "the input layer to the output layer by connecting neurons in that single direction. Within\n",
    "the process, each neuron receives inputs from previous neurons and transfers outputs to\n",
    "next neurons. Depending on the strength of each connection, a weight is assigned to it,\n",
    "turning each neuron into a function of weighted inputs that in turn generate an output\n",
    "that is used as input to the next neuron. Once we have the\n",
    "output layer arising from forward feeding, the error of this is calculated by comparing\n",
    "the outputs generated with the true values of the dependent variable. Then, the error\n",
    "propagates backwards through the hidden layers until it reaches the input layer to readjust\n",
    "the weights seeking to minimise the error (”learning”) and that is why it is called back\n",
    "propagation. The aforementioned adjustments are produced by iteration with the objective of converging to an optimum. Therefore we have to set all the parameters mentioned (number of hidden layers and nuerons, loss-function, optimizer, the activation function and the learning rate) and train our model to get the best results.\n",
    "\n",
    "First of all we will prepare our data to train and evaluate our model. In order to do so we will one-hot encode all the nominal categorical variables to make each of the values of the variables binary which makes it easier for the model to detect paterns as it helps reduce the bias that the different variable scales may produce.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the list of variables we are going to one-hot encode\n",
    "dummy_variables=list(column_names[1:])\n",
    "#We remove the ordinal categorical variables from the list\n",
    "dummy_variables.remove('Age')\n",
    "dummy_variables.remove('Health')\n",
    "#We remove the variable that will be the target\n",
    "dummy_variables.remove('Hours worked per week')\n",
    "#We create the dataset that will be used to feed the neural network which will contain the ordinal variables and the one-hot encoded\n",
    "#nominal variables together with the target\n",
    "df_nn=df_Small[['Age','Health','Hours worked per week']]\n",
    "\n",
    "#We implement a for loop that will one-hot encode each of the nominal variables and put them in the new database\n",
    "for column in dummy_variables:\n",
    "    #it will store the arrays of 1s and 0s for each row of the column and add them in the new dataset. The prefix argument is the\n",
    "    #first part of the new column name.\n",
    "    dummies = pd.get_dummies(df_Small[column],prefix=column)\n",
    "    #create the new columns (as many as values could take the variable when was integer encoded)in the new dataset\n",
    "    df_nn[dummies.columns] = dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Now that we have the dataset prepared, we have to separate the data in the features that will be used to predict the hours worked per week and also in the target variable itself. After this we are going to split the features and the target variables between the training set and the test set. Therefore, we are going to have the train set of features (x_train), the train set  of variables(y_train), the test set of features(x_test) and the test set of targets (y_test).</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the list of features excluding the target variable\n",
    "features=list(df_nn.columns)\n",
    "features.remove('Hours worked per week')\n",
    "\n",
    "#We set the features data to x and the target variable to y\n",
    "x=df_nn[features]\n",
    "y=df_nn[[\"Hours worked per week\"]]\n",
    "\n",
    "#We randomly split the data putting 20% of the observations into the test set and 80% in the training set \n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=50,test_size=0.20)\n",
    "\n",
    "#After claning and transforming the data we think there is no need to standarize our cateogrical variables. Moreover, \n",
    "#we have tried standarizing and there were no relevant changes in prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Once the data is prepared we have to configure the neural network model. To do so we are goig to use the keras package from tensorflow. Concretely, we have decided to nest three different for loops that will produce 8 different neural networks. This is a way of doing a little bit of parameter tunning. We could have choosen more options in each for and implement more for loops in order to improve our tuning but it will have taken too much time. Nonetheless, we have followed a trial and error method before doing this parameter tuning in order to choose the other parameters. Each of these neural networks will be stored in an array. Concretely, we will store the accruacy metric for the validation set of each neural network, the list of the validation loss, the trainning loss, the validation accuracy and the trainning accuracy of each iteration when training the different neural networks to be able to plot the curves and analyse if there is over or underfitting. <b>BE AWARE THAT NEXT CELL IS GOING TO TRAIN 8 NEURAl NETWORKS WHICH CAN TAKE UP TO 50 MINUTES</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 1.2826 - accuracy: 0.5332 - val_loss: 1.0478 - val_accuracy: 0.6830\n",
      "Epoch 2/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.8845 - accuracy: 0.7162 - val_loss: 0.7497 - val_accuracy: 0.7450\n",
      "Epoch 3/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.6802 - accuracy: 0.7568 - val_loss: 0.6270 - val_accuracy: 0.7668\n",
      "Epoch 4/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.6034 - accuracy: 0.7682 - val_loss: 0.5828 - val_accuracy: 0.7715\n",
      "Epoch 5/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5752 - accuracy: 0.7713 - val_loss: 0.5662 - val_accuracy: 0.7731\n",
      "Epoch 6/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5639 - accuracy: 0.7722 - val_loss: 0.5587 - val_accuracy: 0.7741\n",
      "Epoch 7/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5583 - accuracy: 0.7727 - val_loss: 0.5546 - val_accuracy: 0.7746\n",
      "Epoch 8/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5551 - accuracy: 0.7731 - val_loss: 0.5521 - val_accuracy: 0.7751\n",
      "Epoch 9/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5529 - accuracy: 0.7734 - val_loss: 0.5502 - val_accuracy: 0.7754\n",
      "Epoch 10/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5512 - accuracy: 0.7737 - val_loss: 0.5489 - val_accuracy: 0.7758\n",
      "Epoch 11/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5499 - accuracy: 0.7741 - val_loss: 0.5479 - val_accuracy: 0.7756\n",
      "Epoch 12/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5488 - accuracy: 0.7745 - val_loss: 0.5467 - val_accuracy: 0.7764\n",
      "Epoch 13/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5478 - accuracy: 0.7750 - val_loss: 0.5460 - val_accuracy: 0.7766\n",
      "Epoch 14/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5470 - accuracy: 0.7752 - val_loss: 0.5451 - val_accuracy: 0.7771\n",
      "Epoch 15/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5461 - accuracy: 0.7755 - val_loss: 0.5444 - val_accuracy: 0.7771\n",
      "Epoch 16/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5454 - accuracy: 0.7758 - val_loss: 0.5437 - val_accuracy: 0.7773\n",
      "Epoch 17/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5447 - accuracy: 0.7761 - val_loss: 0.5431 - val_accuracy: 0.7773\n",
      "Epoch 18/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5441 - accuracy: 0.7764 - val_loss: 0.5426 - val_accuracy: 0.7777\n",
      "Epoch 19/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5435 - accuracy: 0.7767 - val_loss: 0.5421 - val_accuracy: 0.7779\n",
      "Epoch 20/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5429 - accuracy: 0.7769 - val_loss: 0.5415 - val_accuracy: 0.7782\n",
      "Epoch 21/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5424 - accuracy: 0.7771 - val_loss: 0.5411 - val_accuracy: 0.7781\n",
      "Epoch 22/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5419 - accuracy: 0.7772 - val_loss: 0.5406 - val_accuracy: 0.7785\n",
      "Epoch 23/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5413 - accuracy: 0.7774 - val_loss: 0.5401 - val_accuracy: 0.7788\n",
      "Epoch 24/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5408 - accuracy: 0.7778 - val_loss: 0.5396 - val_accuracy: 0.7794\n",
      "Epoch 25/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5403 - accuracy: 0.7780 - val_loss: 0.5392 - val_accuracy: 0.7797\n",
      "Epoch 26/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5399 - accuracy: 0.7782 - val_loss: 0.5387 - val_accuracy: 0.7792\n",
      "Epoch 27/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5394 - accuracy: 0.7784 - val_loss: 0.5383 - val_accuracy: 0.7796\n",
      "Epoch 28/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5389 - accuracy: 0.7787 - val_loss: 0.5379 - val_accuracy: 0.7798\n",
      "Epoch 29/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5385 - accuracy: 0.7788 - val_loss: 0.5375 - val_accuracy: 0.7798\n",
      "Epoch 30/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5381 - accuracy: 0.7792 - val_loss: 0.5371 - val_accuracy: 0.7800\n",
      "Epoch 31/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5376 - accuracy: 0.7794 - val_loss: 0.5367 - val_accuracy: 0.7804\n",
      "Epoch 32/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5372 - accuracy: 0.7796 - val_loss: 0.5363 - val_accuracy: 0.7804\n",
      "Epoch 33/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5368 - accuracy: 0.7798 - val_loss: 0.5359 - val_accuracy: 0.7809\n",
      "Epoch 34/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5364 - accuracy: 0.7797 - val_loss: 0.5356 - val_accuracy: 0.7807\n",
      "Epoch 35/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5360 - accuracy: 0.7799 - val_loss: 0.5351 - val_accuracy: 0.7814\n",
      "Epoch 36/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5356 - accuracy: 0.7801 - val_loss: 0.5349 - val_accuracy: 0.7809\n",
      "Epoch 37/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5352 - accuracy: 0.7803 - val_loss: 0.5345 - val_accuracy: 0.7811\n",
      "Epoch 38/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5348 - accuracy: 0.7802 - val_loss: 0.5341 - val_accuracy: 0.7817\n",
      "Epoch 39/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5344 - accuracy: 0.7805 - val_loss: 0.5338 - val_accuracy: 0.7817\n",
      "Epoch 40/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5341 - accuracy: 0.7806 - val_loss: 0.5335 - val_accuracy: 0.7817\n",
      "Epoch 41/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5337 - accuracy: 0.7806 - val_loss: 0.5330 - val_accuracy: 0.7818\n",
      "Epoch 42/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5333 - accuracy: 0.7807 - val_loss: 0.5327 - val_accuracy: 0.7822\n",
      "Epoch 43/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5329 - accuracy: 0.7808 - val_loss: 0.5323 - val_accuracy: 0.7823\n",
      "Epoch 44/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5326 - accuracy: 0.7809 - val_loss: 0.5321 - val_accuracy: 0.7824\n",
      "Epoch 45/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5323 - accuracy: 0.7809 - val_loss: 0.5318 - val_accuracy: 0.7825\n",
      "Epoch 46/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5319 - accuracy: 0.7811 - val_loss: 0.5317 - val_accuracy: 0.7827\n",
      "Epoch 47/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5316 - accuracy: 0.7813 - val_loss: 0.5311 - val_accuracy: 0.7827\n",
      "Epoch 48/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5312 - accuracy: 0.7813 - val_loss: 0.5308 - val_accuracy: 0.7830\n",
      "Epoch 49/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5309 - accuracy: 0.7814 - val_loss: 0.5305 - val_accuracy: 0.7831\n",
      "Epoch 50/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5306 - accuracy: 0.7815 - val_loss: 0.5303 - val_accuracy: 0.7831\n",
      "Epoch 51/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5303 - accuracy: 0.7817 - val_loss: 0.5302 - val_accuracy: 0.7831\n",
      "Epoch 52/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5300 - accuracy: 0.7817 - val_loss: 0.5296 - val_accuracy: 0.7832\n",
      "Epoch 53/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5297 - accuracy: 0.7816 - val_loss: 0.5295 - val_accuracy: 0.7836\n",
      "Epoch 54/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5294 - accuracy: 0.7819 - val_loss: 0.5292 - val_accuracy: 0.7833\n",
      "Epoch 55/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5291 - accuracy: 0.7820 - val_loss: 0.5289 - val_accuracy: 0.7834\n",
      "Epoch 56/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5288 - accuracy: 0.7821 - val_loss: 0.5286 - val_accuracy: 0.7834\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5285 - accuracy: 0.7821 - val_loss: 0.5284 - val_accuracy: 0.7837\n",
      "Epoch 58/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5282 - accuracy: 0.7821 - val_loss: 0.5281 - val_accuracy: 0.7836\n",
      "Epoch 59/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5279 - accuracy: 0.7822 - val_loss: 0.5279 - val_accuracy: 0.7837\n",
      "Epoch 60/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5277 - accuracy: 0.7824 - val_loss: 0.5277 - val_accuracy: 0.7841\n",
      "Epoch 61/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5274 - accuracy: 0.7823 - val_loss: 0.5275 - val_accuracy: 0.7839\n",
      "Epoch 62/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5271 - accuracy: 0.7824 - val_loss: 0.5273 - val_accuracy: 0.7840\n",
      "Epoch 63/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5269 - accuracy: 0.7825 - val_loss: 0.5272 - val_accuracy: 0.7841\n",
      "Epoch 64/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5266 - accuracy: 0.7826 - val_loss: 0.5270 - val_accuracy: 0.7843\n",
      "Epoch 65/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5264 - accuracy: 0.7827 - val_loss: 0.5270 - val_accuracy: 0.7841\n",
      "Epoch 66/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5261 - accuracy: 0.7827 - val_loss: 0.5267 - val_accuracy: 0.7841\n",
      "Epoch 67/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5259 - accuracy: 0.7829 - val_loss: 0.5262 - val_accuracy: 0.7844\n",
      "Epoch 68/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5257 - accuracy: 0.7828 - val_loss: 0.5259 - val_accuracy: 0.7841\n",
      "Epoch 69/100\n",
      "3251/3251 [==============================] - 5s 2ms/step - loss: 0.5254 - accuracy: 0.7831 - val_loss: 0.5259 - val_accuracy: 0.7842\n",
      "Epoch 70/100\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 0.5252 - accuracy: 0.7830 - val_loss: 0.5256 - val_accuracy: 0.7845\n",
      "Epoch 71/100\n",
      "3251/3251 [==============================] - 5s 1ms/step - loss: 0.5250 - accuracy: 0.7831 - val_loss: 0.5254 - val_accuracy: 0.7843\n",
      "Epoch 72/100\n",
      "3248/3251 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7832"
     ]
    }
   ],
   "source": [
    "#we will set to options of the activation functions which will be the relu and the sigmoid functions\n",
    "activation_function = ['relu', 'sigmoid']\n",
    "#the first hidden layer will have 100 or 80 neurons\n",
    "neurons_1 = [100, 80]\n",
    "#the second hidden layer will have 50 or 40 neurons\n",
    "neurons_2 = [50,40]\n",
    "#we initialize the array that will store the data of each neural network\n",
    "results = []\n",
    "\n",
    "#We will have 8 neural networks given that we have two options per loop (2^3)\n",
    "#Activations will take values 'relu' and generate 4 neural networks and then 'sigmoid'and generate 4 more neural networks\n",
    "for activations in activation_function:\n",
    "    #neuron_1 will take 100 and 80 values\n",
    "    for neuron_1 in neurons_1:\n",
    "        #neuron_2 will take 50 and 40 values\n",
    "        for neuron_2 in neurons_2:\n",
    "            # Train the model\n",
    "            #we set the optimizer as 'Adam',which is a stochastic gradient descent method that is based on adaptive \n",
    "            #estimation of first-order and second-order moments. We also set the learning rate \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "            #initialize the model as Sequential which groups different layers \n",
    "            model=models.Sequential()\n",
    "            #we add the input layer and 1 hidden layer with the neurons (100 or 80) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_1,activation=activations,input_shape=(x_train.shape[1],)))\n",
    "            #we add the second hidden layer with the neurons (50 or 40) and the actvation function(relu or sigmoid)\n",
    "            model.add(layers.Dense(neuron_2, activation=activations))\n",
    "            #we add the output layer with 5 neurons given that we have 5 cateogires with the softmax function which gives a probabilitie\n",
    "            #for every category or neuron\n",
    "            model.add(layers.Dense(5,activation='softmax'))\n",
    "            #we compile the model and set the loss function to 'sparse_categorical_corssentropy', that suits our case as our target variable\n",
    "            #is integer encoded. We set the optimizer as explained before and we choose the accuracy as the metric to evaluate the performance.\n",
    "            model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "            #now we  fit the model with the data and train the model with 100 epochs and 90 as batch size. Epochs are the number of times that \n",
    "            #the model will go trough the entire training set in the training process whereas batches are the number of training samples to work through \n",
    "            #before updating the model. Finally, we set to 20% the proportion of data from the training set that will be part of the validation set used to\n",
    "            #control for over or under fitting and for the parameter tuning. Moreover, we store the whole trainning process in the varaible history\n",
    "            history=model.fit(x_train,y_train,epochs=100,batch_size=90,validation_split=0.20)\n",
    "            #we also store the predictions that the model does with the features of the training set\n",
    "            predictions=model.predict(x_train)\n",
    "            y_predictions=[]\n",
    "            #this for will select the maxium from the array of 5 probabilities for each category that has been predicted and append it to the list of predictions\n",
    "            for i in range(predictions.shape[0]):\n",
    "                maximum = np.argmax(predictions[i])\n",
    "                y_predictions.append(maximum)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Append the current results trough a dictionary\n",
    "            results.append({\n",
    "                #we round the accuracy and multiply it by 100\n",
    "                'Validation accuracy': round(history.history['val_accuracy'][-1]*100, 2),\n",
    "                'activation_function': activations,\n",
    "                'neuron_1': neuron_1,\n",
    "                'neuron_2': neuron_2,\n",
    "                'loss':history.history['loss'],\n",
    "                'val_loss':history.history['val_loss'],\n",
    "                'accuracy':history.history['accuracy'],\n",
    "                'val_accuracy':history.history['val_accuracy'],\n",
    "                'predictions':y_predictions\n",
    "            })\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data sorted for each neural network, we can do a table to compare them based on the accuracy of the validation set that is what has to be used in the hyperparameter tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array to Pandas DataFrame \n",
    "comparison = pd.DataFrame(results)\n",
    "#add the model column\n",
    "comparison['Model']=['Model 1','Model 2', 'Model 3','Model 4','Model 5', 'Model 6','Model 7','Model 8']\n",
    "#keep only the important variables\n",
    "result1=comparison[['Model','Validation accuracy','activation_function','neuron_1','neuron_2']]\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the characteristics of each model but we have to analyse each of them more in depth to see if there is over or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate the dictionary to know the index of each model\n",
    "dict_models={'Model 1':0,'Model 2':1, 'Model 3':2,'Model 4':3,'Model 5':4, 'Model 6':5,'Model 7':6,'Model 8':7}\n",
    "#generate a list with the name of each model\n",
    "model_num=list(dict_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>We are going to create three different functions to plot the results of each model iteractively.\n",
    "\n",
    "First, the **overfitting_plots function**, creates a figure with two subbplots. In the first sublot we can see the curves of the training loss and the validation-loss. This function takes the index of the model as argument and returns the figure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_plots(option):\n",
    "    #we create two subplots of a certain dimension each    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "    #we define the variable loss as the array generated by the loss function when iterating in the training process extracted form the comparison data frame.\n",
    "    loss =comparison.iloc[option]['loss']\n",
    "    #the same as before but with the validation set\n",
    "    val_loss = comparison.iloc[option]['val_loss']\n",
    "    #we set de epochs as the length of the list\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    #we plot the epochs in the x axis and the loss in the y axis in yellow\n",
    "    ax1.plot(epochs, loss, 'y', label='Training loss')\n",
    "    #same as before with the validation loss in red\n",
    "    ax1.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    #put the title of the subplot\n",
    "    ax1.title.set_text('Training and validation loss')\n",
    "    #set the x and the y label\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    #plot the legend\n",
    "    ax1.legend()\n",
    "    #we do the second plot as the first but changing the loss for the accuracy.\n",
    "    accuracy =comparison.iloc[option]['accuracy']\n",
    "    val_accuracy =comparison.iloc[option]['val_accuracy']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    ax2.plot(epochs, accuracy, 'y', label='Accuracy')\n",
    "    ax2.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "    ax2.title.set_text('Training and validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    Second, the <b>observed_against_predicted</b>  function, takes the index of the model in the comparison table as argument and returns a barplot with the observed value against the predicted value for the first 20 observations of the training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed_against_predicted(option):\n",
    "    #we first construct a dataframe with the column of the observed hours worked per week and the predicted ones(two columns)\n",
    "    observed_predicted=y_train\n",
    "    observed_predicted['Predicted']=comparison.iloc[option]['predictions']\n",
    "    observed_predicted.rename(columns = {'Hours worked per week':'Observed'},inplace = True)\n",
    "    #we reset the index to go from 0 onwards \n",
    "    observed_predicted.reset_index(drop=True,inplace=True)\n",
    "    #we sum one to all the values of the dataframe to be able to plot the vaalue 0\n",
    "    observed_predicted2=observed_predicted+1\n",
    "    #we plot only the 20 first values of the dataframe\n",
    "    table=observed_predicted2.head(20)\n",
    "    table.plot.bar()\n",
    "    #we switch the values of the y axis to make them the cateogries of the hours worked per week variable\n",
    "    plt.yticks([0,1,2,3,4,5],[\"\",\"Not working\",\"Part-time: 15 or less hours worked\",\"Part-time: 16 to 30 hours worked\",\"Full-time: 31 to 48 hours worked\",\"Full-time: 49 or more hours worked\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Last, the <b>plot_model_results</b>, takes the name of the model as argument and has the previous two functions nested so that it plots both the over or underfitting  plots and the comaprison of predicted against observed values of the model choosed. Moreover, it prints a string that indicates the accuracy and other characteristics of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model):\n",
    "    #using the previous dictionary  we find the index with the name of the model\n",
    "    option=dict_models[model]\n",
    "    #print the title\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+\"                                                           \" +model+ ' Results:'+'\\033[0m')\n",
    "    #plot the curves of over and underfitting\n",
    "    overfitting_plots(option).show()\n",
    "    #prints a title to separate both plots\n",
    "    print('--------------------------------------------------------')\n",
    "    print('  Hours worked per week: Observed against Predicted  ')\n",
    "    print('--------------------------------------------------------')\n",
    "    #plots the arplot of the values predicted and observed\n",
    "    observed_against_predicted(option)\n",
    "    #prints the string with the data from the model\n",
    "    print('For this model we have used '+ str(comparison.iloc[option]['activation_function']) +' as activation function and the first hidden layer had '+str(comparison.iloc[option]['neuron_1']) \n",
    "          + ' neurons while the second hidden layer had '+str(comparison.iloc[option]['neuron_2']) + ' neurons. The accuracy of this model measured with the validation set has been: '+ str(comparison.iloc[option]['Validation accuracy']) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we finally make the last function described interactively and we set all the model names as the options\n",
    "#to choose in the parameter of the function\n",
    "res=interactive(plot_model_results,model=model_num)\n",
    "display(res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the next variable we store which is the model with the best accuracy in the validation set\n",
    "chosen=comparison.loc[comparison['Validation accuracy'].idxmax()][\"Model\"]\n",
    "#we also store the parameters of the best model for the bootstraping section\n",
    "chosen_neuron_1=comparison.loc[comparison['Validation accuracy'].idxmax()][\"neuron_1\"]\n",
    "chosen_neuron_2=comparison.loc[comparison['Validation accuracy'].idxmax()][\"neuron_2\"]\n",
    "chosen_activation_function=comparison.loc[comparison['Validation accuracy'].idxmax()][\"activation_function\"]\n",
    "print(\"Given that none of the previous models has overfitting, we are going to choose the best model based on the validation set accuracy. In this case this is \"+chosen+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model chosen will be compared to the best model of the second approach, which will be carried in the next section,  by the accuracy of each model when using out of sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Multinomial Regression  <a id='logistic_regression'></a>\n",
    "\n",
    "<p style='text-align: justify;'>Multinomial logistic regression is a simple extension of binary logistic regression that allows for more than two categories of the dependent or outcome variable. Like binary logistic regression, multinomial logistic regression uses maximum likelihood estimation to evaluate the probability of categorical membership.\n",
    "\n",
    "The LogisticRegression class can be used for a multinomial logistic regression if we specify the argument multi_class as multinomial. In order to be able to model multinomial target the solver of the model has to be set as lbfgs as well.\n",
    "\n",
    "The multinomial logistic regression model will be fit using cross-entropy loss and will predict the integer value for each integer encoded class label. This loss function is the same as the one used in the neural network model.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we first initilize the model with the previosuly mentioned parameters to adapt the binary logistic regression to a multiple logistic regression.\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now train the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make predictions using the test set \n",
    "logistic_pred=model.predict(x_test)\n",
    "#we use the previously predictions to get the accuracy of the model when using data out of sample\n",
    "accuracy_score(y_test[['Hours worked per week']],logistic_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to repeat the same process but including different levels of penalty by implementing a for loop.Specifically, penalty imposes pressure on the model to seek smaller model weights. There are three types of penalty but we have chosen the L2 penalty that adds the (weighted) sum of the squared coefficients to the loss function. A weighting of the coefficients(C) can be used that reduces the strength of the penalty from full penalty to a very slight penalty. This will allow us to compare them and choose the best one based on the test set accuracy. The choosen one will be the one used to be compared with the neural network model choosen in the [previous section](#neural_network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initialize the dictionary to store each alternative model\n",
    "logistic_models = dict()\n",
    "#we implement a for loop to create a model for each penalty alternative. As clarification, penalty = 1 – C. That is why we add\n",
    "#the parameter C.\n",
    "for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    # create name for model\n",
    "    key = '%.4f' % p\n",
    "    #the for loop will enter thos if when the penalty of the model equals 0\n",
    "    if p == 0.0:\n",
    "        # no penalty in this case\n",
    "        logistic_models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "    else:\n",
    "        #penalty equal to p in this case\n",
    "        logistic_models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initialize the dictionary in which we will store the test set accuracy for each of the models previously created\n",
    "accuracies=dict()\n",
    "#we implement a for loop that will compute and store the test accuracy for each model\n",
    "for key,value in logistic_models.items():\n",
    "    #we train each model\n",
    "    value.fit(x_train,y_train)\n",
    "    #once the model is trained we make the predictions based on the test set\n",
    "    predictions=value.predict(x_test)\n",
    "    #we compute the test accuracy for each model and store it in the dictionary\n",
    "    accuracies['Model with pealty: '+key]=accuracy_score(y_test[['Hours worked per week']],predictions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we create the data frame to have  a table with each model and its accuracy by using the ditionary accuracies previously created\n",
    "logit_comparison=pd.DataFrame({\"Model\":list(accuracies.keys()),\"Accuracies\":list(accuracies.values())})\n",
    "logit_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Given the table shown above we consider the \"+logit_comparison.iloc[logit_comparison[\"Accuracies\"].idxmax()][\"Model\"] +\" to be the best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison of approaches: Bootstrapping <a id='bootstrapping'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a function to work with the same neural network every loop. THis neural network is the chosen in the neural network \n",
    "#section. The parameters of this neural network are \"chosen_activation_function\",\"chosen_neuron_1\",chosen_neuron_2.\n",
    "def compile_neural_model():\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    neural_model=models.Sequential()\n",
    "    neural_model.add(layers.Dense(chosen_neuron_1,activation=chosen_activation_function,input_shape=(x_train.shape[1],)))\n",
    "    neural_model.add(layers.Dense(chosen_neuron_2, activation=chosen_activation_function))\n",
    "    neural_model.add(layers.Dense(5,activation='softmax'))\n",
    "    neural_model.compile(loss='sparse_categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])\n",
    "    return neural_model\n",
    "#we define the next function to work with the same logistic multinomial regression every loop\n",
    "def compile_logistic_model():\n",
    "    logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='none')\n",
    "    return logistic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data change the random_state to make each split different\n",
    "#initialize the array to store the accuracies for every neural and for every logistic multinomial regression\n",
    "neural_accuracies=[]\n",
    "logistic_accuracies=[]\n",
    "#set 10 different random_state which values are irrelevant as long as they are different\n",
    "random_states=[10,20,30,40,50,60,70,80,90,100]\n",
    "#one loop for every random state to make our analysis more robust by training and predicting with different splits which\n",
    "#gives consistency\n",
    "for i in random_states:\n",
    "    #we first split the data that will be used in both models. Each round the split will be different so the train and test set\n",
    "    #are different every time\n",
    "    x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=i,test_size=0.20)\n",
    "    \n",
    "    #we compile each model as it was never trained\n",
    "    neural_network=compile_neural_model()\n",
    "    logistic_model=compile_logistic_model()\n",
    "    #we train each model\n",
    "    neural_network.fit(x_train,y_train,epochs=100,batch_size=90,validation_split=0.20)\n",
    "    logistic_model.fit(x_train, y_train)\n",
    "    #we evaluate the neural network model\n",
    "    #we also store the predictions that the model does with the features of the test set\n",
    "    predictions=neural_network.predict(x_test)\n",
    "    y_predictions=[]\n",
    "    #this for will select the maxium from the array of 5 probabilities for each category that has been predicted and append it \n",
    "    #to the list of predictions        \n",
    "    for i in range(predictions.shape[0]):\n",
    "            maximum = np.argmax(predictions[i])\n",
    "            y_predictions.append(maximum)\n",
    "    \n",
    "    \n",
    "    \n",
    "    neural_stat =accuracy_score(y_test[['Hours worked per week']],y_predictions) \n",
    "    neural_accuracies.append(neural_stat)\n",
    "    #we evaluate the logistic model\n",
    "    logistic_predictions=value.predict(x_test)\n",
    "    logistic_stat=accuracy_score(y_test[['Hours worked per week']],logistic_predictions)\n",
    "    logistic_accuracies.append(logistic_stat)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>In the next cell we will use each of the accuracies arrays created using the bootstraping method to find the confidence interval of the neural network predictions accuracies and the logistic multinomial regression prediction accuracies. To do so we are going to plot the boxplot of the accuracies distributions as well as the confidence intervals which can be done by finding the \n",
    "correspondent percentiles of the accuracies array. We use the numpy percentile method. Firstwe chose the confidence level which we choose to be 95, thus we find the 2.5 and the 97.5 percentiles.</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_confidence_interval_95=np.percentile(neural_accuracies,[100*(1-0.95)/2,100*(1-(1-0.95)/2)])\n",
    "logistic_confidence_interval_95=np.percentile(logistic_accuracies,[100*(1-0.95)/2,100*(1-(1-0.95)/2)])\n",
    "models_accuracies_comparison=pd.DataFrame({\"Neural Network Accuracies\":neural_accuracies,\"Logistic Regression Accuracies\":logistic_accuracies})\n",
    "sn.boxplot(data=models_accuracies_comparison, orient=\"h\", palette=\"Set2\")\n",
    "plt.xlabel('accuracy')\n",
    "plt.title('Accuracies distribution per model with 10 samples')\n",
    "plt.show()\n",
    "print(\"The accuracy of the Neural Network model predictions goes from \"+str(round(neural_confidence_interval_95[0]*100,2)) +\"% to \"+str(round(neural_confidence_interval_95[1]*100,2)) +\"% in 95% of the times. Regarding the Logistic Multinomial Regression model, the accuracy of its predictions goes from \"+str(round(logistic_confidence_interval_95[0]*100,2))+\"% to \"+str(round(logistic_confidence_interval_95[1]*100,2)) +\"% in 95% of the times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this results we can say that the **Neural Network model does a slightly better job** in predicting the hours worked per week which can be seen in the difference between the predictions accuracy confidence interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness analysis (0.5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c76eccb6035c07aa27eec1492ed7ed3",
     "grade": true,
     "grade_id": "cell-365f4bd48b20ea77",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<p style='text-align: justify;'>We consider that the procedure of our analysis is quite robust given that we have performed diferent comparisons at an intra model stage and between approaches. We first performed a comparison between neural networks when doing the hyperparameter tunning by looking at the validation set accuracy among 8 different models. Secondly, we have compared the different Logistic Multinomial Regression models by setting different penalty degrees and looking at the accuracy when using data out of sample. Lastly, we have used a bootstraping method to take samples of the same neural network and the same Logistic Multinomial Regression accuracy when being trained with different data and also tested with different test sets which has allowed us to give a confidence interval of the predictions accuracy for each model. Nonetheless, each of this steps has been performed with few samples which weakness our study robustness. This las point will be extended in the discussion.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion (0.5 points)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e908b508af5c2244449588615f83a4d",
     "grade": true,
     "grade_id": "cell-f8f3ab0fc4655b00",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<p style='text-align: justify;'>All things considered I thing that our study can give an answer to our research question. The answer to the first part of our research question, which consisted in estimating to what degreee can personal characteristics help predicting the level of hours worked per week in the UK, is that the personal characteristic can help to reach an accuracy in the predictions between 78% and 78.5% in 95% of times based on the best model we could configure.  Regarding the second part of the research question, which was to detemine which method does a better job in predicting the hours worked per week; the Neural Network or the Logistic Multinomial Regression, we have found that the Neural network does a slightly better job as the confidence interval is better.\n",
    "\n",
    "Nonetheless, we consider our study to have some weaknesses that make our results not robust enough. This particular weaknesses can be divided into two groups. First of all, the time and memory constraint leaded to a not consistent enough hyperparameter tunning that avoid us from finding a better neural network model as with more time and memory we could have increased the alternatives for each parameter. The same happened with the bootstraping section to compare both model as we just did a sample of ten models for alternatives which is too low. The second group of weaknesses correspond to the data that we have got which is only categorical leaving out some numerial features like income, number of people responsible of and others that may increase the predictive performance. \n",
    "\n",
    "To conclude, we can say that we have conducted quite a complete analysis that can be further developed by imporving some of the weaknesses previously mentioned.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
